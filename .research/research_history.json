{
  "research_topic": "# Structuring Knowledge Graphs from Document Collections  Develop a method to extract entities, relations, and triples from Japanese document collections (such as Wikipedia) and construct knowledge graphs. Assign a class name to each entity and relation. During extraction, periodically reorganize the knowledge graph schema after extracting a sufficient number of entities.",
  "queries": [
    "Japanese relation extraction",
    "knowledge graph schema induction",
    "ontology learning Wikipedia"
  ],
  "research_study_list": [
    {
      "title": "Prototypical Representation Learning for Relation Extraction",
      "full_text": "Published as a conference paper at ICLR 2021 PROTOTYPICAL REPRESENTATION LEARNING FOR RE- LATION EXTRACTION Ning Ding1∗, Xiaobin Wang2∗, Yao Fu3, Guangwei Xu2, Rui Wang2, Pengjun Xie2, Ying Shen4, Fei Huang2, Hai-Tao Zheng1†, Rui Zhang‡ 1Tsinghua University, 2Alibaba Group, 3The University of Edinburgh, 4Sun Yat-sen University {dingn18}@mails.tsinghua.edu.cn, {yao.fu}@ed.ac.uk {xuanjie.wxb, kunka.xgw, chengchen.xjp, f.huang}@alibaba-inc.com {zheng.haitao}@sz.tsinghua.edu.cn, {rui.zhang}@ieee.org ABSTRACT Recognizing relations between entities is a pivotal task of relational learning. Learn- ing relation representations from distantly-labeled datasets is difﬁcult because of the abundant label noise and complicated expressions in human language. This paper aims to learn predictive, interpretable, and robust relation representations from distantly-labeled data that are effective in different settings, including super- vised, distantly supervised, and few-shot learning. Instead of solely relying on the supervision from noisy labels, we propose to learn prototypes for each relation from contextual information to best explore the intrinsic semantics of relations. Pro- totypes are representations in the feature space abstracting the essential semantics of relations between entities in sentences. We learn prototypes based on objectives with clear geometric interpretation, where the prototypes are unit vectors uniformly dispersed in a unit ball, and statement embeddings are centered at the end of their corresponding prototype vectors on the surface of the ball. This approach allows us to learn meaningful, interpretable prototypes for the ﬁnal classiﬁcation. Results on several relation learning tasks show that our model signiﬁcantly outperforms the previous state-of-the-art models. We further demonstrate the robustness of the encoder and the interpretability of prototypes with extensive experiments. 1 I NTRODUCTION Relation extraction aims to predict relations between entities in sentences, which is crucial for under- standing the structure of human knowledge and automatically extending knowledge bases (Cohen & Hirsh, 1994; Bordes et al., 2013; Zeng et al., 2015; Schlichtkrull et al., 2018; Shen et al., 2020). Learning representations for relation extraction is challenging due to the rich forms of expressions in human language, which usually contains ﬁne-grained, complicated correlations between marked entities. Although many works are proposed to learn representations for relations from well-structured knowledge (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), when we extend the learning source to be unstructured distantly-labeled text (Mintz et al., 2009), this task becomes particularly challenging due to spurious correlations and label noise (Riedel et al., 2010). This paper aims to learn predictive, interpretable, and robust relation representations from large-scale distantly labeled data. We propose a prototype learning approach, where we impose a prototype for each relation and learn the representations from the semantics of each statement, rather than solely from the noisy distant labels. Statements are deﬁned as sentences expressing relations between two marked entities. As shown in Figure 1, a prototype is an embedding in the representation space capturing the most essential semantics of different statements for a given relation. These prototypes essentially serve as the center of data representation clusters for different relations and are surrounded by statements expressing the same relation. We learn the relation and prototype representations based on objective functions with clear geometric interpretations. Speciﬁcally, our approach assumes prototypes are unit vectors uniformly dispersed in a unit ball, and statement embeddings are centered ∗Equal contribution. †Corresponding author. ‡http://ruizhang.info 1 arXiv:2103.11647v1  [cs.CL]  22 Mar 2021Published as a conference paper at ICLR 2021 -70 -44 -18 8 34 60 -40 -23 -6 11 28 Cause-Effect Component-Whole Whole-Component Contant-Container Collection-Member  -8.703243 38.913616 -11.877513 26.187891 -10.24494 31.439125 -11.185886 28.53963 -6.7643037 31.873375 -9.293796 30.790508 -7.6870537 36.014236 -11.216029 31.59263 -7.4329624 35.181496 -10.611754 35.345486 -12.039416 22.71971 -11.616351 26.774305 -8.657383 35.28101 -13.320659 29.52109 -11.843529 26.295746 -10.6841545 28.803246 -7.6381073 36.70645 -9.564307 31.78187 -7.315561 39.579956 -4.533672 34.883198 -8.275788 33.035698 -7.4689636 35.644993 -11.624744 30.369122 -9.997218 38.49715 -7.7783055 37.59266 -9.765418 38.476517 -9.501085 32.574696 -6.3372855 37.55847 -11.9366 27.271059 -12.867382 36.685673 -10.081516 36.11969 -11.364351 34.99714 -13.114578 37.7766 -12.3460045 36.313652 -10.533733 29.321291 -6.8389997 37.80524 -10.056862 37.98478 -11.23021 29.466564 -8.006758 36.853874 -9.294378 29.327047 -10.339967 35.70266 -10.410015 31.533749 -13.232315 35.18631 -13.210859 29.454363 -7.7240295 37.255066 -10.900574 32.073906 -7.5629783 36.904022 -10.104565 31.449959 -8.409077 33.879704 -4.9887133 35.23039 -9.434023 33.882618 -10.956362 38.222942 -12.509354 31.459927 -6.1397 37.212097 -8.249299 36.092 -8.1636095 38.060524 -13.816341 26.780077 -12.954381 37.67792 -8.914301 38.879467 -11.33116 32.713974 15.663114 12.8621025 17.03712 18.486742 17.551886 14.484679 18.931808 18.930895 15.572551 13.700756 10.558792 17.574535 17.613794 13.805676 17.844099 18.073603 10.654739 18.235968 16.538021 10.523316 17.641438 13.43363 14.079281 12.15079 18.864927 14.856975 15.856341 10.3096 14.077647 16.088354 8.66363 17.863062 13.279723 15.911679 19.156492 14.567751 16.535969 16.739618 17.02585 13.34684 18.596682 11.846535 12.980082 18.898485 16.939898 17.028992 11.612106 16.237764 16.605167 17.61748 17.126232 9.916799 18.854584 16.075922 10.663816 15.945908 15.273596 11.521369 13.061167 16.711267 17.905909 10.257096 12.815981 17.505522 15.421573 16.05703 17.434866 14.227275 14.178516 17.211475 16.826227 12.7907715 8.85155 18.680946 16.42301 15.614987 16.469488 15.520174 17.752037 11.194748 13.268249 11.668395 19.25654 19.09535 17.51284 8.887941 10.873803 14.245057 15.503976 14.112884 12.484748 12.027752 12.8171425 9.299756 10.16367 10.303493 19.28029 17.801203 6.9740515 14.092293 16.590027 14.853376 15.17265 18.155851 18.5615 18.52006 17.618689 14.498615 16.670238 16.880407 18.137753 16.441898 14.629623 19.16981 17.200861 18.588112 17.524498 11.210086 17.630442 18.84577 11.128795 -11.828977 -43.852882 -18.523153 -48.35973 -9.013127 -49.965767 -12.775725 -38.4372 -11.386365 -45.235485 -12.341138 -46.811172 -18.695421 -47.257236 -17.548359 -47.813652 -16.86908 -47.66306 -9.285677 -48.134 -15.553001 -47.495358 -14.435563 -47.35719 -15.938199 -44.728043 -10.96651 -50.96271 -9.333371 -47.840088 -8.612561 -50.579475 -9.66584 -45.757843 -14.620562 -36.031902 -13.302847 -47.38101 -12.612855 -44.99101 -11.566471 -48.70189 -14.438774 -45.744644 -13.297511 -51.752514 -12.276336 -48.390713 -8.143216 -49.363167 -14.794755 -45.36748 -9.096069 -49.784893 -9.096887 -48.303566 -11.601617 -40.474926 -8.760579 -48.474392 -8.8828 -51.211826 -14.739173 -13.053048 -14.749782 -44.379173 -13.720553 -47.30534 -13.980023 -38.663837 -16.943655 -45.568115 -7.284974 -47.792404 -13.98673 -38.59544 -15.000847 -42.772976 -11.206605 -39.627068 -14.621026 -37.741215 -5.989932 -49.660408 -7.066158 -50.164024 -13.359581 -41.030807 -11.044324 -48.147877 -10.257005 -41.0706 -8.1137085 -49.74866 -11.536943 -44.25597 -10.309574 -49.382477 -16.535332 -48.246834 -16.57508 -44.80348 -14.411162 -48.468327 -14.510945 -36.572926 -14.163412 -45.338158 -19.041832 -47.55801 -14.49797 -46.043823 -16.699863 -49.536724 -15.524808 -47.48149 -8.445343 -48.45605 -11.151114 -46.65739 -14.092925 -49.13657 -8.212766 -49.940895 -11.3532095 -46.101093 -11.26969 -45.414417 -7.291685 -49.634018 -15.727305 -43.962696 -14.725657 -36.18008 -8.530978 -48.039 -12.263783 -46.4457 -13.111706 -45.93163 -16.514885 -47.25092 -17.156609 -46.77544 10.73023 -60.08112 -8.636485 -48.777756 -15.849547 -46.368256 -14.301521 -36.94852 -13.472748 -47.12356 9.924656 -54.517498 -12.016547 -49.535435 -12.705193 -43.919346 -13.171877 -39.083767 -13.118616 -42.864147 -14.06309 -49.1256 -6.9750905 -51.342743 -10.250488 -47.86121 -6.3860655 -50.226997 -13.797769 -46.049805 -9.539331 -43.63843 -7.4702673 -50.618774 -14.247917 -38.567375 -15.689215 -35.869644 -11.770379 -46.01179 -7.974457 -48.025314 -15.115464 -41.50516 -17.751368 -47.27474 -11.07022 -47.56136 -12.364049 -45.718647 -7.192023 -51.772503 -16.014822 -36.732063 -11.9974165 -42.06392 -9.965427 -51.854122 -13.210664 -38.62058 -8.270923 -49.581097 -11.441111 -49.41309 -8.452865 -13.024228 -14.365504 -41.821922 -15.149126 -40.793488 -13.114061 -43.343094 -15.477967 -42.245445 -16.184212 -41.677425 -15.117426 -47.537777 -11.991396 -41.70241 -12.122491 -44.353127 -14.115153 -36.14243 -13.586221 -45.992226 -13.601769 -44.940388 -10.696865 -45.62354 -9.2235775 -48.957237 -8.406905 -50.76499 -18.666452 -47.53441 -7.9554815 -46.99664 -10.015536 -52.052444 -14.992561 -37.97665 -14.920621 -48.478596 Prototypes for the relation s1s2 s3 s4 s5s6 : The subject of imply\" is the source of an implication  while the subject of \"infer\" is the recipient…  (Cause-Effect) s1 : Lopez and Espinoza took plea agreements that resulted in  diversion -- the completion of requirements… (Cause-Effect) s2 : That is one of the many reasons that a CEO's team includes  an experienced general counsel.              (Collection-Member) s5 : The cells inside a nested table are isolated from changes  made to the outer table.                            (Component-Whole) s4 : As well as the wildlife, India has a vast treasure trove of  palaces, forts, temples, museums and…  (Collection-Member) s6 : The small photoreceptors of the retina (the inner surface  at the back of the eye) sense…                 (Component-Whole) s3 1 Figure 1: The t-SNE visualization of relation representations and corresponding prototypes learned by our model. In the right part, s1:6 are examples of input statements, where red and blue represent the head and tail entities, and the italics in parenthesis represents the relation between them. at the end of their corresponding prototype vectors on the surface of the ball. We propose statement- statement and prototype-statement objectives to ensure the intra-class compactnessand inter-class separability. Unlike conventional cross-entropy loss that only uses instance-level supervision (which could be noisy), our objectives exploit the interactions among all statements, leading to a predictively powerful encoder with more interpretable and robust representations. We further explore these properties of learned representations with extensive experiments. We apply our approach using a pretraining ﬁne-tuning paradigm. We ﬁrst pretrain a relation encoder with prototypes from a large-scale distantly labeled dataset, then ﬁne-tune it on the target dataset with different relational learning settings. We further propose a probing dataset, FuzzyRED dataset (Section 4.3), to verify if our method can capture the underlying semantics of statements. Experiments demonstrate the predictive performance, robustness, and interpretability of our method. For predictive performance, we show that our model outperforms existing state-of-the-art methods on supervised, few-shot, and zero-shot settings (Section 4.4 and 4.2). For robustness, we show how the model generalize to zero-shot setting (Section 4.2) and how the prototypes regularize the decision boundary (Section 4.5). For interpretability, we visualize the learned embeddings and their corresponding prototypes (Section 4.4) and show the cluster clearly follow the geometric structure that the objective functions impose. The source code of the paper will be released at https://github.com/ Alibaba-NLP/ProtoRE. 2 R ELATED WORK Relation learning could be mainly divided into three categories. The logic-based methods reason relations via symbolic logic rules, with adopting probabilistic graphical models or inductive logic systems to learn and infer logic rules orienting to relations (Cohen & Hirsh, 1994; Wang et al., 2015; Yang et al., 2017; Chen et al., 2018; Kazemi & Poole, 2018; Qu & Tang, 2019). The graph-based methods encode entities and relations into low-dimensional continues spaces to capture structure features of KBs. (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2015; Wang et al., 2014; Nickel et al., 2016; Ji et al., 2015; Lin et al., 2015; Trouillon et al., 2016; Sun et al., 2019; Balaˇzevi´c et al., 2019). The text-based methods have been widely explored recently, focusing on extracting semantic features from text to learn relations. The conventional methods to learn relations from the text are mainly supervised models, like statistical supervised models (Zelenko et al., 2003; GuoDong et al., 2005; Mooney & Bunescu, 2006). As deep neural networks have gained much attention then, a series of neural supervised models have been proposed (Liu et al., 2013; Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang & Wang, 2015; Verga et al., 2016; Verga & McCallum, 2016; Li et al., 2019; Distiawan et al., 2019; Ding et al., 2019). To address the issue of the insufﬁciency of annotated data, distant supervision has been applied to automatically generate a dataset with heuristic rules (Mintz et al., 2009). Accompanying with auto-labeled data, massive noise will be introduced into models. Accordingly, various denoising methods have been explored to reduce noise effects for distant supervision (Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Han et al., 2018a; Wu et al., 2017; Qin et al., 2018a;b; Feng et al., 2Published as a conference paper at ICLR 2021 2018). Further efforts pay attention to learning relations from the text in various speciﬁc scenarios, such as zero-few-shot scenarios (Levy et al., 2017; Han et al., 2018b; Gao et al., 2019; Soares et al., 2019; Ye & Ling, 2019) and open-domain scenarios (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro & Gemulla, 2013; Angeli et al., 2015; Stanovsky & Dagan, 2016; Mausam, 2016; Cui et al., 2018; Shinyama & Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019). However, up to now, there are few strategies of relation learning that could be adapted to any relation learning tasks. Soares et al. (2019) introduces a preliminary method that learns relation representations from unstructured corpus. And the assumption that two same entities must express the same relation is imprecise, because it does not consider the semantics of contextual information. This paper abstracts the task from the text as a metric and prototype learning problem and proposes a interpretable method for relation representation learning. Apart from the differences in methods and task scenarios, row-less universal schema (Verga & McCallum, 2016) has a similar spirit with our method, where relation type embeddings guide clustering of statements for entity pair representations and embeddings of entity pairs are regarded as an aggregation of relations. Although the intuition that assigns a representative prototype for each class is similar to some previous studies like Prototypical Networks (Snell et al., 2017) (ProtoNet), there exist some essential distinctions between our approach and the ProtoNet. The ProtoNet computes the prototype of each class as the average of the embeddings of all the instance embeddings, that could be regarded as a non-linear version of the nearest class mean approach (Mensink et al., 2013). The idea of mean-of- class prototypes could also be traced to earlier studies in machine learning (Graf et al., 2009) and cognitive modeling and psychology (Reed, 1972; Rosch et al., 1976). In our method, prototypes and relation encoder are collaboratively and dynamically trained by three objectives, which divides the high-dimensional feature space into disjoint manifolds. The ProtoNet performs instance-level classiﬁcation to update the parameters, which is not robust for noisy labels. Our method carries out a novel prototype-level classiﬁcation to effectively regularize the semantic information. The prototype-level classiﬁcation reduces the distortion caused by noisy labels of the decision boundary ﬁtted by NNs. Moreover, the ProtoNet is designed for few-zero-shot learning and our methods are more like a semi-supervised pre-training approach that could be applied to supervised, few-shot, and transfer learning scenarios. Furthermore, the ProtoNet does not contain the metric learning among instances, and our method simultaneously optimizes the prototype-statement and statement-statement metrics. We also make a geometry explanation of our method (Section 3.1). 3 M ETHOD Our method follows a pretraining-ﬁnetuning paradigm. We ﬁrst pretrain an encoder with distantly labeled data, then ﬁne-tune it to downstream learning settings. We start with the pretraining phase. Given a large-scale, distantly labeled dataset containing training pairs (w,r) where w= [w1,...,w n] is a relation statement, i.e., a sequence of words with a pair of entities [h,t] marked, hbeing the head and tbeing the tail, ∃i1,i2,h = wi1:i2 ,∃j1,j2,t = wj1:j2 , ris the relation between hand t. ris a discrete label and is probably noisy. We aim to learn an encoder Enc φ(·) parameterized by φthat encodes winto a representation s∈Rm, mbeing the dimension: s= Encφ(w). (1) A prototype zfor relation ris an embedding in the same metric space withsthat abstracts the essential semantics of r. We use [(z1,r1),..., (zK,rK)],zk ∈Rm to denote the set of prototype-relation pairs. Kis the number of different relations. zk is the prototype for relation rk and superscripts denote the index of relation type. Given a batch B= [(s1,r1),..., (sN,rN)], N is the batch size and subscripts denote batch index, the similarity metric between two statement embeddings d(si,sj), and between a statement embedding and a prototype d(s,z) are deﬁned as: d(si,sj) = 1/(1 + exp(si ||si||· sj ||sj||)), d (s,z) = 1/(1 + exp(s ||s||· z ||z||)). (2) Geometrically, this metric is based on the angles of the normalized embeddings restricted in a unit ball. We will explain the geometric implication of this metric in the next section. Inspired by Soares et al. (2019), we deﬁne a contrastive objective function between statements: LS2S = − 1 N2 ∑ i,j exp(δ(si,sj)d(si,sj))∑ j′exp((1 −δ(si,sj′))d(si,sj′)), (3) 3Published as a conference paper at ICLR 2021 where δ(si,sj) denotes if si and sj corresponds to the same relations, i.e., given (si,ri),(sj,rj), δ(si,sj) = 1if ri = rj else 0. The computation of the numerator term forces that statements with same relations to be close, and the denominator forces that those with different relations are dispersed. When constructing the batch B, we equally sample all relation types to make sure the summation in the denominator contains all relations. Essentially, equation 3 ensures intra-class compactnessand inter-class separabilityin the representation space. Additionally, equation 3 contrast one positive sample to all the negative samples in the batch, which puts more weights on the negative pairs. We ﬁnd it empirically more effective than previous objectives like Soares et al. (2019). 3.1 L EARNING PROTOTYPES Now we discuss the objective functions for learning prototypes. Denote S= [s1,...,s N] as the set of all embeddings in the batch B, given a ﬁxed prototype zr for relation r, we denote Sr the subset of all statements si in Swith relation r, and S−r the set of the rest statements. Z−r as the set of prototypes z′for all other relations except r. We impose two key inductive biases between prototypes and statements: (a) for a speciﬁc relation r, the “distance” between zr and any statements with the same relation r should be less than the “distance” between zr and any statements with relations r′ ̸= r. (b) the “distance” between zr and any statements with relation rshould be less than the “distance” between any prototypes z′∈Z−r and statements with relation r. To realize these two properties, we deﬁne two objective functions: LS2Z = − 1 N2 ∑ si∈Sr,sj∈S−r [ log d(zr,si) + log(1−d(zr,sj)) ] , (4) LS2Z’ = − 1 N2 ∑ si∈Sr,z′∈Z−r [ log d(zr,si) + log(1−d(z′,si)) ] , (5) where equation 4 corresponds to (a) and equation 5 corresponds to (b). These objectives effectively splits the data representations intoKdisjoint manifolds centering at different prototypes. We further highlight the differences between equation 4 and 5 and a conventional cross-entropy loss: there is no interactions between different statements in the cross-entropy loss that solely relies on the instance level supervision, which is particularly noisy under a noisy-label setting. On the other hand, our loss functions consider distances between different statements and prototypes, which exploits the interactions between statements. This type of interaction would effectively serve as a regularization to the decision boundary, as we will empirically verify in section 4.5. zr s /uni2208Sr Figure 2: An illustration of the geometric explanation. Stars rep- resent prototypes and circles rep- resent statements. Combining equations 2, 3, 4 and 5, we further give a geometric explanation of the representation space: a prototype is a unit vector (because we normalize the vector length in equation 2) starting from the origin and ending at the surface of a unit ball, and statements for that prototypes are unit vectors with approx- imately same directions centering at the prototype (because our objective functions 3, 4, 5 push them to be close to each other). Under the optimal condition, different prototype vectors would be uniformly dispersed with the angles between them as large as possible (because the distance metric is based on the angle in equation 2 where the minimum is at π, and we maximize dis- tances between prototypes in equation 4 and 5). Consequently, the dataset is clustered with each cluster centered around the end of one prototype vectoron the surface of the unit ball. The illustration of the intuition is shown in Figure 2. To further regularize the semantics of the prototypes, we use a prototype-level classiﬁcation objective: LCLS = 1 K ∑ k log pγ(rk|zk), (6) where γ denotes the parameters of an auxiliary classiﬁer. Our prototype essentially serves as a regularization averaging the inﬂuence of label noises. We further validate this regularization in Section 4.5 by showing that it reduces the distortion of the decision boundary caused by noisy labels. Finally, with hyper-parameters λ1, λ2 and λ3, the full loss is: L= λ1LS2S + λ2(LS2Z + LS2Z) +λ3LCLS. (7) 4Published as a conference paper at ICLR 2021 3.2 F INE -TUNING ON DIFFERENT LEARNING SETTINGS We apply our learned encoder and prototypes to two typical relational learning settings: (a) supervised; (b) few-shot. Under the supervised learning setting, having the pretrained encoder at hand, we ﬁne- tune the model on a dataset of a target domain (which could again, be distantly labeled) containing new training pairs (w,r), we encode wto the representation and feed it to a feed-forward classiﬁer: p(r|w) =FF(Encφ(w)), (8) where FF(·) denotes a feed-forward layer, and we train it with the conventional cross-entropy loss. Under the few-shot learning setting, we assume a slightly different input data format. Speciﬁcally, the training set consists of the relations [r1,...,r K] and a list of supporting statements: sk 1,...,s k L for all rk. Denote S⋆ = {sk l}k=1,...,K l=1,...,L all supporting statements for all relations. Given a query statement q, the task is to classify the relation r∗that qexpresses. During traing, we use the average distance to different prototypes as the logits of a feed-forward classiﬁer and train the model with cross-entropy loss. During testing, we classify qaccording to the minimum similarity metrics: sk∗ l∗ = arg min sk l ∈S⋆ d(sk l,Encφ(q)), r ∗= k∗. (9) Note this is equivalent to choosing the argmax of the logits because the logits are formed by the similarity scores. We note that model pre-trained by our approach performs surprisingly well even with no training data for ﬁne-tuning, which is reported in Figure 3 and 4 in Section 4.2. 4 E XPERIMENTS In order to evaluate the performance of prototypical metrics learning, we conduct extensive experi- ments on three tasks: supervised relation learning, few-shot learning and our proposed fuzzy relation learning evaluation. We make a comprehensive evaluation and analysis of our method, as well as full-scale comparisons between our work with previous state-of-the-art methods. 4.1 E XPRIMENTAL DETAILS Pretraining dataset We prepare the weakly-supervised data by aligning relation tuples from the Wikidata database to Wikipedia articles. Speciﬁcally, all entities in wikipedia sentences are identiﬁed by a named entity recognition system. For an entity pair (h,t) in a sentence w, if it also exists in the knowledge base with a relation r, then wwill be distantly annotated as r. After processing, we collect more than 0.86 million relation statements covering over 700 relations. Baseline ModelsWe primarily compare our model with MTB because it is a previous SOTA model. For fair comparison, we re-implement MTB with BERTbase (Devlin et al., 2018) and pretrain it on our collected distant data as same as our methods. We note under our setting, the reported number of MTB is smaller than its original paper because: (a) the original paper uses much larger dataset than ours (600 million vs 0.86 million); (b) they use BERTlarge encoder. In addition, multiple previous state-of-the-art approaches are picked up for comparison in all the three relation learning tasks (detailed later). For baseline models (PCNN, Meta Network, GNN, Prototypical Network, MLMAN) with publicly available source code, we run the source code manually, and if the produced results are close to those in the original paper, we would select and report the results in the original paper. For ones without open source code ( BERTEM, MTB), we re-implement the methods with BERTbase pre-trained on our distant data and report the results. We also implement a baseline that directly trains BERTbase on our distant data with cross-entropy loss (LCE), namely BERTCE in the section (the encoder is pre-trained by directly predicting the distant label for each statement). The IND baselineWe additionally design a baseline, where the prototypes are pre-computed by vectorizing the extracted relation patterns independently (IND). In this way, for each statement, we pre-compute a ﬁxed metric as its weight demonstrating the similarity with the corresponding prototype. The IND baseline is primarily for validating the effectiveness of trained prototypes over the rule-based prototypes. Given the distantly-labeled training set, we use the Snowball algorithm from Agichtein & Gravano (2000) to extract relation patterns. This algorithm takes a corpus of relational instances as inputs and outputs a list of patterns and their embeddings for each relation. After getting these patterns, we use patterns to match instances and calculate how many instances can each pattern match. For each relation label, we select the top-k patterns that match the most instances 5Published as a conference paper at ICLR 2021 and use the average of their embeddings as the prototype embedding. Note that these embeddings are not comparable to the sentence embeddings generated by our encoders, so the loss functions in our equation 4 and 5 are not usable for the prototypes generated here. To incorporate the information of the extracted prototype embeddings, we slightly modify equation 3 and use the similarity of instances to the embeddings to weight the instance embeddings. The modiﬁed loss function is: LIND = − 1 N2 ∑ i,j exp(δ(si,sj)d(s′ i,s′ j))∑ j′exp((1 −δ(si,sj′))d(s′ i,s′ j′)), (10) s′ i = sim(wi,z′(wi)) ·si, s ′ j = sim(wj,z′(wj)) ·sj, s ′ j = sim(w′ j,z′(w′ j)) ·s′ j, (11) where wi denote the corresponding sentence to si, z′(wi) denote the correspond prototype. Implementation Details For encoding relation statements into a feature space, we adopt deep Transformers (Vaswani et al., 2017), speciﬁcally BERT, as Eθ. We take the input and the output of the BERT encoder as follows: given a statement s, we add special entity markers to mark Mh and Mt before the entities, then input the statement and special markers to Encφ to compute ﬁnal representations. Note that our framework is designed independently to the encoder choice, and other neural architectures like CNN (Zeng et al., 2014) and RNN (Zhang & Wang, 2015) can be easily adopted as the encoder. We use PyTorch (Paszke et al., 2019) framework to implement our model. All the experiments run on NVIDIA Tesla V100 GPUs. The encoder is optimized with the combination of the prototype-statement and statement-statement metrics as well as the masked language model loss with the settings as follows. For the sake of saving resources, we choose BERTbase as the backbone instead of BERTlarge, although our method sacriﬁces a considerable part of the performance. We select AdamW (Loshchilov & Hutter, 2018) with the learning rate of 1e−5 for optimization. Meanwhile, Warmup (Popel & Bojar, 2018) mechanism is used during training. The number of layers is 12, and the number of heads is 12. The hidden size is set to 768 and the batch size is set to 60. We train the model for 5 epochs in each experiment. 4.2 F EW-SHOT RELATION LEARNING Dataset We use a large few-shot relation learning dataset FewRel (Han et al., 2018b) in this task. FewRel consists of 70000 sentences (about 25 tokens in each sentence) of 100 relations (700 sentences for each relation), on the basis of Wikipedia. We utilize the ofﬁcial evaluation setting which splits the 100 relations into 64, 16, 20 relations for training, validation, and testing respectively. Experimental settingsWe use accuracy as the evaluation metric in this task. The batch size for few-shot training is 4, the training step is 20000 and the learning rate is 2e-5 with AdamW. We follow the original meaning of N-way-K-shot in this paper, please refer to (Han et al., 2018b) for details. Results and analysisThe results for few-shot relation learning are illustrated in Table 1. It is worth noting that we report the average score rather than the best score of each method for the sake of fairness. One observation is that the task is challenging without the effectiveness of pre-trained language models. Prototypical network (CNN) only yields 69.20 and 56.44 accuracies (%) for 5 way 1 shot and 10 way 1 shot respectively. Methods that have achieved signiﬁcant performances on supervised RE (such as PCNN, GNN, and Prototypical Network) suffer grave declines. MTB learns the metrics between statements based on large pre-trained language models and yield surprising results on this task. Better performance gains by considering the semantic information by applying ﬁxed-prototypes to MTB (IND strategy). The collaborative prototype-based methods are proven to be effective as shown in the last four rows. The results of COL (LS2S′+ LZ2S + LZ2S′) indicate that the loss of MTB have little inﬂuence for the performance. Finally, our ﬁnal collaborative model achieve the best performance on all the N-way K-shot settings. Our method also outperforms human in 5 way 1 shot setting (92.41 vs 92.22) and 10 way 1 shot setting (86.39 vs 85.88). Effect of the amount of training dataIn real-world scenarios, it is costly and labor-consuming to construct a dataset like FewRel containing 70,000 positive instances. Some relations rarely appear in public articles and obtain candidate instances via distance supervision introduces much noise. Therefore, the amount of instances to annotate may much larger than 70, 000. To investigate the ability of few-shot classiﬁcation when lack of high-quality task-speciﬁc training data, we intentionally control the amount of instances in training data and run evaluations with the new training set. Figure 3 and Figure 4 respectively display the accuracy of the 5-way-1-shot task given a limited number 6Published as a conference paper at ICLR 2021 Table 1: Few-shot classiﬁcation accuracies (%) on FewRel dataset. The last block is the results of the series of our models with prototypes. We use LZ to brieﬂy indicate LZ2S + LZ2S′ . Results with †are reported as published, and other methods are implemented and evaluated by us. ↑denotes outperformance over the main baseline MTB and ↓denotes underperformance. Method 5 way 1 shot 5 way 5 shot 10 way 1 shot 10 way 5 shot Finetune (PCNN)†(Han et al., 2018b) 45.64 57.86 29.65 37.43 Meta Network (CNN)†(Han et al., 2018b) 64.46 80.57 53.96 69.23 GNN (CNN)†(Han et al., 2018b) 66.23 81.28 46.27 64.02 Prototypical Network†(Han et al., 2018b) 69.20 84.79 56.44 75.55 MLMAN†(Ye & Ling, 2019) 82.98 92.66 73.59 87.29 BERTEM(Soares et al., 2019) 88.70 95.01 81.93 90.05 MTB (LS2S′) (Soares et al., 2019) 89.09 95.32 82.17 91.73 BERTCE(LCE) 91.02 (↑) 95.40 ( ↑) 84.95 ( ↑) 91.43 ( ↓) IND (LIND) 89.90 ( ↑) 95.42 ( ↑) 82.47 ( ↑) 91.55 ( ↓) COL (LZ) 90.40 ( ↑) 94.73 ( ↓) 84.27 ( ↑) 91.58 ( ↓) COL (LZ +LCLS) 91.12 ( ↑) 95.45 ( ↑) 85.10 ( ↑) 91.75 ( ↑) COL (LS2S′+LZ +LCLS) 91.08 ( ↑) 95.52 ( ↑) 85.83 ( ↑) 92.18 ( ↑) COL Final (LS2S+LZ +LCLS) 92.51(↑) 95.88(↑) 86.39(↑) 92.76(↑) ᤒ໒ 1 #Rel BERT MTB Ours 0 71.9 81.32 89.86 10 80.54 81.9 85.6 20 85.98 87.7 88.6 40 87.91 89.83 90.41 64 88.70 89.09 91.08 Accuracy 70 75 80 85 90 95 Number of relation types in training 0 10 20 40 64 BERT MTB Ours 71.9 80.5 86.0 87.9 88.7 81.3 81.9 87.7 89.8 89.189.9 85.6 88.6 90.4 91.1 /uni0394= 17 . 96 ᤒ໒ 1-1 #Rel BERT MTB Ours 0 71.9 81.32 89.86 4 83.82 87.73 90.17 16 86.98 88.34 89.42 64 88.35 88.75 90.41 700 88.70 89.09 91.08 Accuracy 70 75 80 85 90 95 Number of instances per type in training 0 4 16 64 700 BERT MTB Ours 1 Figure 3: Impact of # of relation types ᤒ໒ 1 #Rel BERT MTB Ours 0 71.9 81.32 89.86 10 80.54 81.9 85.6 20 85.98 87.7 88.6 40 87.91 89.83 90.41 64 88.70 89.09 91.08 Accuracy 70 75 80 85 90 95 Number of relation types in training 0 10 20 40 64 BERT MTB Ours 71.9 80.5 86.0 87.9 88.7 81.3 81.9 87.7 89.8 89.189.9 85.6 88.6 90.4 91.1 /uni0394= 17 . 96 ᤒ໒ 1-1 #Rel BERT MTB Ours 0 71.9 81.32 89.86 4 83.82 87.73 90.17 16 86.98 88.34 89.42 64 88.35 88.75 90.41 700 88.70 89.09 91.08 Accuracy 70 75 80 85 90 95 Number of instances per type in training 0 4 16 64 700 BERT MTB Ours 1 Figure 4: Impact of # of instances per type of relation types or amount of instances of each relation in training data. As shown in the ﬁgures, pre-trained relation encoders outperform the basic BERTEM model when short of training data. Encoder learned with our proposed method shows large superiority over the others, when there is no training data (0), the absolute improvement of our framework is 17.96. Besides, the accuracy increase with the increment of training data regardless of providing more relations or more instances of each relation. However, the diversity of relations is more important, since the performance in Figure 4 outperforms those in Figure 3 while the former is trained on a much smaller training data. Lack of diversity even hurts the performance when applying relation encoder which is learned with prototype-statement metric. Similar conclusions have been mentioned in Soares et al. (2019). 4.3 F UZZY RELATION EVALUATION Dataset Aimed at evaluating relation representation learning on the noisy weakly supervised corpus, we propose a new evaluation dataset named FuzzyRED to quantitatively evaluate the capture of relational information. FuzzyRED is constructed aimed at the shortcoming of distant supervision, that is not all the statements contain the same entity pairs express the same relation. All the instances are from the weakly-supervised data and manually annotated binary labels to judge if the sentence semantically expresses the relation. For example, for “Jack was born in London, UK”, even if the relation Capital is correct between the entities in knowledge base, the sentence does not express the capital relationship and will be labeled as False in FuzzyRED. The dataset contains 1000 manually annotated instances covering 20 relations. For detailed information please refer to Appendix A. Experimental settingsThe pre-trained relation encoders are directly utilized on FuzzyRED and expected to separate the false positive instances from true positives. For each relation, we calculate the minimum of statement-statement distances among all true positives as the threshold T. Then for 7Published as a conference paper at ICLR 2021 each instance sto be classiﬁed, we sample 1 K(k-shot) true positive instances s′∈{s1,s2,...,s K} and compare the average score of a = p(l = 1|s,s′) with T. If a > T, the instance is classiﬁed as positive. FuzzyRED is considerably challenging since it is designed for the issue of distant supervision and directly reﬂects the gap between semantic relations and annotation information. Table 2: Accuracies (%) on FuzzyRED. Method 1-shot 3-shot 5-shot DS 46.3 46.3 46.3 MTB 51.3 50.7 50.7 IND 51.7 50.8 50.7 COL Final 53.2 52.1 52.1 Results The accuracies of FuzzyRED are reported in Table 2. The accuracy of labels annotated accord- ing to the DS assumption is denoted by DS, which indicates that the majority of the dataset are noise. From the experimental results, we could ﬁrstly ob- serve that all the relation encoders suffer from the noise issue and get lower results. The impact of noise increases with the kincreasing. But Table 2 still shows that pre-trained relation encoders do de- tect a few noise data thus they all yield better performance than DS. Taking consideration of noise by prototypes leads to better performances, and the ﬁxed prototypes play a small role to detect the com- plex semantics that express relations. Even though the COL model achieves remarkable improvement, the evaluation task is still very challenging because of the fuzziness of relation semantics. 4.4 S UPERVISED RELATION LEARNING Dataset We use classic benchmark dataset SemEval 2010 Task 8 (Hendrickx et al., 2010) as the dataset for supervised relation learning. SemEval 2010 Task 8 contains nine different pre-deﬁned bidirectional relations and a None relation, thus the task is a 19-way classiﬁcation. For model selection, we randomly sample 1500 instances from the ofﬁcial training data as the validation set. Table 3: Accuracies (%) on SemEval 2010 Task 8. Method P R F Bi-RNN (Zhang & Wang, 2015) - - 79.6 Self-attention (Bilan & Roth, 2018) - - 84.8 BERT (Soares et al., 2019) 86.0 89.1 87.4 MTB (Soares et al., 2019) 86.5 89.0 87.7 IND 86.5 89.7 88.0 COL Final 87.9 88.2 88.0 Experimental settingsFor evaluation, we use standard precision, recall, and F- measure for supervised RE task. Since the annotation schema of SemEval dataset dif- fers from the data for the encoder training, we ﬁne-tune the relation encoder in SemEval training data. The batch size is set to 20, the training epoch is 30, the learning rate is 1e-5 with Adam (Kingma & Ba, 2014). We note that the numbers for MTB are different from their paper because their original setting uses BERTlarge with 600 million instances and is trained on TPUs. Given the restricted resources at hand, to make fair comparison, we re-implemented their model with BERTbase and pretrain it in our setting (0.86 million instances). -32.03327 46.011948 -37.232216 40.208374 57.851215 14.5119915 -36.279755 51.350945 -32.948685 46.43371 -31.635967 47.787243 -36.134483 42.48665 -31.304863 42.825962 -32.55659 47.624786 -35.014057 48.92385 -36.379192 47.746483 34.449425 -16.171896 -32.72987 47.94211 -36.35051 48.344917 -32.323208 48.114315 -36.12216 47.569447 -31.248325 46.342117 -33.704372 49.20423 -36.37419 50.321262 -29.65793 47.09177 -31.451002 45.0103 -32.14381 49.78235 -29.975254 45.620026 -36.890347 48.410774 -36.03316 49.730835 -33.495018 48.77257 -32.30098 45.464066 -30.569134 47.351215 -35.562378 48.145374 -35.073498 49.593136 -34.47625 46.868164 -35.48577 40.106052 -35.557995 47.325172 -38.040337 47.136715 -31.228033 45.695377 -35.597275 43.520714 -31.506842 44.87331 -35.010452 42.522884 -34.73986 43.47125 -37.41552 43.61826 -35.69385 49.25468 -31.205017 42.460396 -33.285717 45.283794 -36.373367 45.761417 -29.974056 47.401974 -32.927696 45.678284 -33.653336 49.56394 -35.173725 46.738068 -34.5852 42.925407 -37.50132 47.716694 -38.138428 45.84838 -31.819334 49.608013 -33.862312 48.406174 -38.185066 47.90718 -36.436043 50.15913 -34.864468 48.428936 -35.16919 43.385315 -31.708015 48.926952 -30.569593 48.685593 -34.53277 47.761166 -36.307808 46.96454 -32.520645 48.23458 -38.003544 49.78163 -32.10012 46.83617 -31.726135 46.437572 -35.08728 48.052307 -35.350697 45.245758 -34.06626 50.767662 -32.926025 44.442913 -36.350803 50.315533 -34.40357 47.076015 57.75246 14.097505 -35.83675 49.368526 -35.12131 40.01342 -33.83321 45.92377 -36.011734 51.10648 -36.024174 43.70184 -37.989933 47.93833 -35.747288 48.638702 -38.807808 47.706417 -37.24076 44.838326 -34.48572 40.2217 -34.113678 50.226685 -33.383656 49.44177 -34.480286 50.100666 -31.52165 43.355156 -37.150234 45.605995 -36.815033 42.564392 -31.166628 45.97167 -37.12979 46.48814 -33.015057 46.838806 -34.655914 44.71694 -36.292625 40.777004 -35.716537 43.760395 -31.006073 44.627674 -30.176638 47.484 -34.868774 51.93568 -36.713535 43.436123 -8.855203 2.3545828 -34.847054 50.385387 -35.652664 48.003242 -38.131527 49.783115 -33.934677 43.65679 -33.24209 47.74869 -32.334812 50.05055 -36.299046 44.02077 -34.330074 49.573418 -36.422413 43.96753 -33.53799 47.246986 -35.346615 47.56133 -32.847996 47.483776 -33.8747 46.44791 -38.154247 46.58428 -33.08718 44.50684 -35.5292 45.896248 -32.879486 38.93397 -32.376263 48.008026 -34.14996 48.06252 -35.306133 43.764988 -32.10036 47.11178 -34.310684 49.74119 -34.202057 48.316734 -34.083035 45.261963 -32.96691 49.549446 -34.994102 45.641056 -36.779217 49.506325 -34.004677 46.21778 -37.055138 50.32413 -33.685005 47.60056 -30.295895 49.543644 -36.494568 48.82859 -34.787422 51.660244 -70 -35 0 35 70 -100 -66 -32 2 36 70 Cause-Effect Effect-Cause Component-Whole Whole-Component Content-Container Container-Content Entity-Destination Entity-Origin Origin-Entity Instrument-Agency Agency-Instrument Member-Collection Collection-Member Message-Topic Topic-Message Product-Producer Producer-Product Prototypes Figure 5: A t-SNE visualization of the relation repre- sentations and prototypes of SemEval data. Results The results for supervised RE are reported in Table 3. As a classic benchmark task, results on SemEval 2010 Task 8 are al- ready relatively high compared with other supervised RE tasks. The BERT based models outperform the traditional model signiﬁcantly. By ﬁne-tuning the relation encoder in Semeval training data, we gain 0.6 percent improvement over the origi- nal BERT model. Since the SemEval data contains rich semantical information, it is meaningful to visualize the relation rep- resentations produced by the trained re- lation encoders. In this part, we use t- SNE (Maaten & Hinton, 2008) method to project the 768-dimensional relation repre- sentations of the test set of SemEval data to 2-dimensional points. The visualization is illustrated in Figure 5, where each round marker represents a statement and a star marker represents a prototype. First, the visualization shows that statements 1For each instance, we sample 100 times and report the average accuracy. 8Published as a conference paper at ICLR 2021 are well clustered and the prototypes are effectively learned for all the relations. Another interesting observation is that the visualization shows visible directions and it is identical to our geometric explanation Section 3.1. 4.5 A TOY EXPERIMENT OF DECISION BOUNDARIES FOR PROTOTYPICAL LEARNING In section 3.1, we mention that the prototype-level classiﬁcation (equation 6) reduces the disortion of the decision boundary caused by noisy labels. In this section, we carry out a toy experiment to explore the point. We perform a simple binary classiﬁcation on the iris dataset (Blake, 1998). Here, we use the instance-level linear classiﬁer and the prototype-level classiﬁer. For the prototype-level classiﬁer, the decision function could be written as g(x) = sign∥(x−z−)∥2 −∥(x−z+)∥2, where xis a data point and z−and z+ represent the prototypes for two classes. This equation is also equivalent to a linear classiﬁer g(x) = sign(w⊤x+ b) with w= z+ −z−and b= ∥z−∥2−∥z+∥2 2 . Prototypes zare computed as the classical mean-of-class (Reed, 1972) method, thus the decision boundary could be easily depicted. We focus on the distortion of the decision boudaries, as shown in Figure 6, when the instances are wrongly labeled, the instance-level boundary is easily impacted and overﬁtted by noisy labels (Figure 6(b)). And the boundary of prototype-level classiﬁer is hardly affected because of the regularization, it is an interesting phenomenon and we believe it worth further theoretical and empirical study. The results also demonstrate the potential of our model for the generalization of other standard/noisy/few-shot classiﬁcation tasks. Sepal length Sepal width Instance-level Classification (Clean) setosa versicolor (a) Decision boundary on clean labels (ins-level). Sepal length Sepal width Instance-level Classification (Noisy) setosa versicolor (b) Decision boundary on noisy labels (ins-level). Sepal length Sepal width Prototype-level classification(Clean) setosa versicolor setosa-prototype versicolor-prototype (c) Decision boundary on clean labels (proto-level). Sepal length Sepal width Prototype-level classification(Noisy) setosa versicolor setosa-prototype versicolor-prototype (d) Decision boundary on noisy labels (proto-level). Figure 6: A set of toy experiments on iris dataset to illustrate the distortion of decision boundaries for instance-level and prototype-level learning. 5 C ONCLUSION In this paper, we re-consider the essence of relation representation learning and propose an effective method for relation learning directly from the unstructured text with the perspective of prototypical metrics. We contrastively optimize the metrics between statements and infer a prototype to abstract the core features of a relation class. With our method, the learned relation encoder could produce pre- dictive, interpretable, and robust representations over relations. Extensive experiments are conducted to support our claim, and the method also shows the good potential of generalization. ACKNOWLEDGMENT This research is supported by National Natural Science Foundation of China (Grant No. 61773229 and 6201101015), Alibaba Innovation Research (AIR) programme, the Shenzhen General Research Project (Grand No. JCYJ20190813165003837, No.JCYJ20190808182805919), and Overseas Cooper- ation Research Fund of Graduate School at Shenzhen, Tsinghua University (Grant No. HW2018002). Finally, we thank the valuable help of Xu Han and suggestions of all the anonymous reviewers. 9Published as a conference paper at ICLR 2021 REFERENCES Eugene Agichtein and Luis Gravano. Snowball: Extracting relations from large plain-text collections. In Proceedings of the ACM DL, pp. 85–94, 2000. Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. Leveraging linguistic structure for open domain information extraction. In Proceedings of ACL-IJCNLP, pp. 344–354, 2015. Ivana Balaˇzevi´c, Carl Allen, and Timothy M Hospedales. Hypernetwork knowledge graph embed- dings. In Proceedings of NIPS, pp. 553–565, 2019. Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. Open information extraction from the web. In Proceedings of IJCAI, volume 7, pp. 2670–2676, 2007. Ivan Bilan and Benjamin Roth. Position-aware self-attention with relative positional encodings for slot ﬁlling. In Proceedings of CoRR, 2018. Catherine Blake. Uci repository of machine learning databases. http://www. ics. uci. edu/˜ mlearn/MLRepository. html, 1998. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. InProceedings of NIPS, pp. 2787–2795, 2013. Wenhu Chen, Wenhan Xiong, Xifeng Yan, and William Yang Wang. Variational knowledge graph reasoning. In Proceedings of NAACL-HLT, pp. 1823–1832, 2018. William W Cohen and Haym Hirsh. Learning the classic description logic: Theoretical and experi- mental results. In Principles of KR, pp. 121–133, 1994. Lei Cui, Furu Wei, and Ming Zhou. Neural open information extraction. In Proceedings of ACL, pp. 407–413, 2018. Luciano Del Corro and Rainer Gemulla. Clausie: clause-based open information extraction. In Proceedings of WWW, pp. 355–366, 2013. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, pp. 4171–4186, 2018. Ning Ding, Ziran Li, Zhiyuan Liu, Haitao Zheng, and Zibo Lin. Event detection with trigger-aware lattice neural network. In Proceedings of EMNLP, pp. 347–356, 2019. Bayu Distiawan, Gerhard Weikum, Jianzhong Qi, and Rui Zhang. Neural relation extraction for knowledge base enrichment. In Proceedings of ACL, pp. 229–240, 2019. Hady Elsahar, Elena Demidova, Simon Gottschalk, Christophe Gravier, and Frederique Laforest. Unsupervised open relation extraction. In Proceedings of ESWC, pp. 12–16, 2017. Anthony Fader, Stephen Soderland, and Oren Etzioni. Identifying relations for open information extraction. In Proceedings of EMNLP, pp. 1535–1545, 2011. Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xiaoyan Zhu. Reinforcement learning for relation classiﬁcation from noisy data. In Proceedings of AAAI, pp. 5779–5786, 2018. Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fewrel 2.0: Towards more challenging few-shot relation classiﬁcation. In Proceedings of EMNLP, pp. 6249–6254, 2019. Arnulf BA Graf, Olivier Bousquet, Gunnar R¨atsch, and Bernhard Sch¨olkopf. Prototype classiﬁcation: Insights from machine learning. Neural computation, 21(1):272–300, 2009. 10Published as a conference paper at ICLR 2021 Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min. Exploring various knowledge in relation extraction. In Proceedings of ACL, pp. 427–434, 2005. Xu Han, Zhiyuan Liu, and Maosong Sun. Neural knowledge acquisition via mutual attention between knowledge graph and text. In Proceedings of AAAI, pp. 4832–4839, 2018a. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Fewrel: A large-scale supervised few-shot relation classiﬁcation dataset with state-of-the-art evaluation. In Proceedings of EMNLP, pp. 4803–4809, 2018b. Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid ´O S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. SemEval-2010 task 8: Multi- way classiﬁcation of semantic relations between pairs of nominals. In Proceedings of SemEval, pp. 33–38, 2010. Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding via dynamic mapping matrix. In Proceedings of ACL, pp. 687–696, 2015. Xiaotian Jiang, Quan Wang, Peng Li, and Bin Wang. Relation extraction with multi-instance multi-label convolutional neural networks. In Proceedings of COLING, pp. 1471–1480, 2016. Seyed Mehran Kazemi and David Poole. Relnn: A deep neural model for relational learning. In Proceedings of AAAI, pp. 6367–6375, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint, 2014. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Proceedings of CoNLL, pp. 333–342, 2017. Ziran Li, Ning Ding, Zhiyuan Liu, Haitao Zheng, and Ying Shen. Chinese relation extraction with multi-grained information and external linguistic knowledge. In Proceedings of ACL, pp. 4377–4386, 2019. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of AAAI, pp. 2181–2187, 2015. Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation extraction with selective attention over instances. In Proceedings of ACL, volume 1, pp. 2124–2133, 2016. ChunYang Liu, WenBo Sun, WenHan Chao, and Wanxiang Che. Convolution neural network for relation extraction. In Proceedings of ADMA, pp. 231–242, 2013. I Loshchilov and F Hutter. Fixing weight decay regularization in adam. In Proceedings of ICLR, 2018. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605, 2008. Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. Open language learning for information extraction. In Proceedings of EMNLP-CoNLL, pp. 523–534, 2012. Mausam Mausam. Open information extraction systems and downstream applications. InProceedings of IJCAI, pp. 4074–4077, 2016. Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image classiﬁcation: Generalizing to new classes at near-zero cost. IEEE transactions on pattern analysis and machine intelligence, 35(11):2624–2637, 2013. Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of ACL, pp. 1003–1011, 2009. Raymond J Mooney and Razvan C Bunescu. Subsequence kernels for relation extraction. In Proceedings of NIPS, pp. 171–178, 2006. 11Published as a conference paper at ICLR 2021 Maximilian Nickel, V olker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In Proceedings of ICML, pp. 809–816, 2011. Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of knowledge graphs. In Proceedings of AAAI, pp. 1955–1961, 2016. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Proceedings of NeuRIPS, pp. 8024–8035, 2019. Martin Popel and Ondˇrej Bojar. Training tips for the transformer model. The Prague Bulletin of Mathematical Linguistics, 110(1):43–70, 2018. Pengda Qin, Weiran Xu, and William Yang Wang. Dsgan: generative adversarial training for distant supervision relation extraction. In Proceedings of ACL, pp. 496–505, 2018a. Pengda Qin, Weiran Xu, and William Yang Wang. Robust distant supervision relation extraction via deep reinforcement learning. In Proceedings of ACL, pp. 2137–2147, 2018b. Meng Qu and Jian Tang. Probabilistic logic neural networks for reasoning. In Proceedings of NIPS, pp. 7710–7720, 2019. Stephen K Reed. Pattern recognition and categorization. Cognitive psychology, 3(3):382–407, 1972. Sebastian Riedel, Limin Yao, and Andrew McCallum. Modeling relations and their mentions without labeled text. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 148–163. Springer, 2010. Eleanor Rosch, Carolyn B Mervis, Wayne D Gray, David M Johnson, and Penny Boyes-Braem. Basic objects in natural categories. Cognitive psychology, 8(3):382–439, 1976. Cicero Nogueira dos Santos, Bing Xiang, and Bowen Zhou. Classifying relations by ranking with convolutional neural networks. In Proceedings of ACL-IJCNLP, pp. 626–634, 2015. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In Proceedings of ESWC, pp. 593–607, 2018. Ying Shen, Ning Ding, Hai-Tao Zheng, Yaliang Li, and Min Yang. Modeling relation paths for knowledge graph completion. IEEE Transactions on Knowledge and Data Engineering, 2020. Yusuke Shinyama and Satoshi Sekine. Preemptive information extraction using unrestricted relation discovery. In Proceedings of NAACL-HLT, pp. 304–311, 2006. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Proceedings of NIPS, pp. 4077–4087, 2017. Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the blanks: Distributional similarity for relation learning. In Proceedings of ACL, pp. 2895–2905, 2019. Gabriel Stanovsky and Ido Dagan. Creating a large benchmark for open information extraction. In Proceedings of EMNLP, pp. 2300–2305, 2016. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In Proceedings of ICLR, 2019. Th´eo Trouillon, Johannes Welbl, Sebastian Riedel, ´Eric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In Proceedings of ICML, pp. 2071–2080, 2016. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of NIPS, pp. 5998–6008, 2017. Patrick Verga and Andrew McCallum. Row-less universal schema. InProceedings of ACL, pp. 63–68, 2016. 12Published as a conference paper at ICLR 2021 Patrick Verga, David Belanger, Emma Strubell, Benjamin Roth, and Andrew McCallum. Multilingual relation extraction using compositional universal schema. In Proceedings of NAACL, pp. 886–896, 2016. William Yang Wang, Kathryn Mazaitis, Ni Lao, and William W Cohen. Efﬁcient inference and learning in a large knowledge base. Machine Learning, 100:101–126, 2015. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In Proceedings of AAAI, pp. 1112–1119, 2014. Ruidong Wu, Yuan Yao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen Lin, Leyu Lin, and Maosong Sun. Open relation extraction: Relational knowledge transfer from supervised data to unsupervised data. In Proceedings of EMNLP-IJCNLP, pp. 219–228, 2019. Yi Wu, David Bamman, and Stuart Russell. Adversarial training for relation extraction. InProceedings of EMNLP, pp. 1778–1783, 2017. Yan Xu, Lili Mou, Ge Lizhang2015relation, Yunchuan Chen, Hao Peng, and Zhi Jin. Classifying relations via long short term memory networks along shortest dependency paths. In Proceedings of EMNLP, pp. 1785–1794, 2015. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of ICLR, 2015. Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge base reasoning. In Proceedings of NIPS, pp. 2319–2328, 2017. Zhi-Xiu Ye and Zhen-Hua Ling. Multi-level matching and aggregation network for few-shot relation classiﬁcation. In Proceedings of ACL, pp. 2872–2881, 2019. Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. Kernel methods for relation extraction. Proceedings of JMLR, 3(Feb):1083–1106, 2003. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, Jun Zhao, et al. Relation classiﬁcation via convolutional deep neural network. In Proceedings of COLING, pp. 2335–2344, 2014. Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of EMNLP, pp. 1753–1762, 2015. Dongxu Zhang and Dong Wang. Relation classiﬁcation via recurrent neural network. In Proceedings of CoRR, 2015. 13Published as a conference paper at ICLR 2021 A F UZZY RELATION EVALUATION DATASET A.1 D ISTANT SUPERVISION Even though our framework is adaptive for a variety of data types, weakly (distantly) supervised data is the most suitable in terms of quantity and quality. Generally, weakly supervised data is constructed on the basis of a large unstructured textual data (e.g. Wikipedia) and a large structured knowledge base (e.g. Wikidata) via the method of distant supervision. First, all the entities are identiﬁed by a named entity recognition system which could tag persons, organizations and locations, etc. Those entities form an entity set Ethat is mentioned in Section 2. Then if for a sentence I, the entity pair (h,t) also exists in the knowledge base with a relation r, then Iwill be distantly annotated as r. After the distant supervision, all the relations form a relation set R. A.2 M OTIVATION OF FUZZY RED The distant supervision assumption demonstrates that if two entities participate in a relation, then all the sentences that mention these two entities express that relation. While obtaining plenty of weakly (distant) supervised data, this assumption has also become the primary drawback of distant supervision. Generally, one entity pair in the distant dataset may correspond to multiple relations in different textual environments. For example, the entity pair (Steve Jobs, California)has three relations, which are Place of Birth, Place of Work, and Place of Residence. Thus, instances of these corresponding relations prone to introducing false-positive noise. We named such relations as fuzzy relations. Intuitively, conducting extraction on fuzzy relations is more challenging. To empirically evaluate if encoder trained on distant supervised could really learn the correct semantic from the noisy distant supervised corpus and perform well on Fuzzy relations, we present a novel dataset, namely, Fuzzy Relation Extraction Dataset (FuzzyRED). In this section, We will ﬁrstly introduce the construction process for FuzzyRED and report some statistics. A.3 C ONSTRUCTION PROCESS OF FUZZY RED Broadly speaking, the dataset is selected from a distant supervised corpus and judged by humans to determine whether each instance really expresses the same relation as the distant supervision labels. For instance, sentence ”Jobs was born in California” really expresses the supervision label ”Place of Birth” while sentence ”Jobs lived in California” does not. The detailed process is as follows: 1. First, following the distant supervision assumption, we align the relation triples in Wikidata with Wikipedia articles to generate massive relation instances. 2. We count the number of corresponding relations for each entity pair, denote as CountR(pair). For example, the entity pair (Steve Jobs, California) has three relations, which are Place of Birth, Place of Work, and Place of Residence. Larger CountR(pair) implies a higher risk of introducing noise. Thus, we consider a pair whose CountR(pair) larger than two as an ambiguous seed of the relations that it is involved in. 3. Then, for each relation, we count the number of ambiguous seeds, denote as CountS(rel). The bigger CountS(rel) is, the more likely the relation will get the wrong data, which will lead to a worse learning effect of the relation. 4. We sort the relations according to CountS(rel) and select the top 20 relations as fuzzy relations of FuzzyRED. For each relation, 50 instances are randomly selected to annotate. For the speciﬁc annotation process, each annotator strictly judges whether the sentence could express the given relation according to the deﬁnition of the relation in Wikidata. A.4 S TATISTICS OF FUZZY RED In FuzzyRED, there are 20 different kinds of relations, and 50 instances for each relation. As mentioned above, every instance is manually classiﬁed as true positive (TP) or false positive (FP). Table 4 shows the statistics of FP rates, which are calculated as count(FP) count(TP)+count(FP). As shown in Table 4, the average FP rate is more than 50 percent, in other words, the majority of the data are FPs. Hence, it is challenging to learn a relation extraction model from FuzzyRED if it doesn’t take the 14Published as a conference paper at ICLR 2021 Table 4: Statistics of false positive (FP) rate of the raw data of FuzzyRED. An FP statement means that the statement does not express the relation but is distantly annotated the relation. Median Average Maximum Minimum False Positive Rate (%) 49.0 54.1 94.0 19.0 noise problem into consideration. In this paper, we empirically evaluate different relation encoders by conducting a binary classiﬁcation task. A better encoder is expected to distinguish TPs from FPs. Speciﬁcally, a model based on such relation encoder should predict a higher probability for TPs. B A LGORITHMS In Section 3.2, we introduce the adaptation of different downstream scenarios for our framework, which trains an encoder Encφ. In this section, we provide the algorithms for the three relation learning scenarios. Algorithm 1 reports the training of supervied relation learning. Algorithm 1Training for Supervised Relation Extraction Input: Supervised dataset Sr = {s1:n}, statement encoder Encφ while not converge do Sample mini-batches Sbatches from Sr. for Sbatch in Sbatches do for sin Sbatch do s = Encφ(s) p(y|s,θ) = Softmax(W s+ b) end for Update W,b and Encφ w.r.t. l(θ) = ∑ s∈Sbatch log p(y|s,θ) end for end while Similarly, Algorithm 2 illustrates the training algorithm for few-shot relation learning. Algorithm 2Training for Few-shot Relation Extraction Input: Few-shot dataset Sf, statement encoder Encφ repeat R′= SampleRelation(N,R). S = ∅, Q= ∅ for rin R′do {sr i}= SampleInstance(k,r),i ∈{1,2...k} qr = SampleInstanc(1,r) S⋃{sr i},Q ⋃{qr} end for for qr in Qdo for sr′ i in Sdo simr′ i = Encφ(qr) ·Encφ(sr′ i ) end for ˆr= argmaxr′,r′∈R′(simr′ i ) Update Encφ w.r.t. CrossEntropy(r,ˆr) end for until enough steps 15Published as a conference paper at ICLR 2021 Algorithm 3 reports the training of fuzzy relation learning. Note that FuzzyRED is designed for evaluation, so the algorithm shows the inference phase. Algorithm 3Training for Fuzzy Relation Evaluation Input: FuzzyRED Sz = {s1:n}, statement encoder Encφ while not converge do for sin Sz do {si}= SampleInstance(k),i ∈{1,2,...,k } Compute aw.r.t a= 1 k ∑p(l= 1|s,si) if a>T then sis true positive (TP) else sis false positive (FP) end if end for end while 16",
      "references": [
        "Snowball: Extracting relations from large plain-text collections.",
        "Leveraging linguistic structure for open domain information extraction.",
        "Hypernetwork knowledge graph embed- dings.",
        "Open information extraction from the web.",
        "Position-aware self-attention with relative positional encodings for slot ﬁlling.",
        "Uci repository of machine learning databases.",
        "Translating embeddings for modeling multi-relational data.",
        "Variational knowledge graph reasoning.",
        "Neural open information extraction.",
        "Clausie: clause-based open information extraction.",
        "Bert: Pre-training of deep bidirectional transformers for language understanding.",
        "Event detection with trigger-aware lattice neural network.",
        "Neural relation extraction for knowledge base enrichment.",
        "Unsupervised open relation extraction.",
        "Identifying relations for open information extraction.",
        "Reinforcement learning for relation classiﬁcation from noisy data.",
        "Fewrel 2.0: Towards more challenging few-shot relation classiﬁcation.",
        "Prototype classiﬁcation: Insights from machine learning.",
        "Exploring various knowledge in relation extraction.",
        "Neural knowledge acquisition via mutual attention between knowledge graph and text.",
        "Fewrel: A large-scale supervised few-shot relation classiﬁcation dataset with state-of-the-art evaluation.",
        "SemEval-2010 task 8: Multi- way classiﬁcation of semantic relations between pairs of nominals.",
        "Knowledge graph embedding via dynamic mapping matrix.",
        "Relation extraction with multi-instance multi-label convolutional neural networks.",
        "Relnn: A deep neural model for relational learning.",
        "Adam: A method for stochastic optimization.",
        "Zero-shot relation extraction via reading comprehension.",
        "Chinese relation extraction with multi-grained information and external linguistic knowledge.",
        "Learning entity and relation embeddings for knowledge graph completion.",
        "Neural relation extraction with selective attention over instances.",
        "Convolution neural network for relation extraction.",
        "Fixing weight decay regularization in adam.",
        "Visualizing data using t-sne.",
        "Open language learning for information extraction.",
        "Open information extraction systems and downstream applications.",
        "Distance-based image classiﬁcation: Generalizing to new classes at near-zero cost.",
        "Distant supervision for relation extraction without labeled data.",
        "Subsequence kernels for relation extraction.",
        "A three-way model for collective learning on multi-relational data.",
        "Holographic embeddings of knowledge graphs.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Training tips for the transformer model.",
        "Dsgan: generative adversarial training for distant supervision relation extraction.",
        "Robust distant supervision relation extraction via deep reinforcement learning.",
        "Probabilistic logic neural networks for reasoning.",
        "Pattern recognition and categorization.",
        "Modeling relations and their mentions without labeled text.",
        "Basic objects in natural categories.",
        "Classifying relations by ranking with convolutional neural networks.",
        "Modeling relational data with graph convolutional networks.",
        "Modeling relation paths for knowledge graph completion.",
        "Preemptive information extraction using unrestricted relation discovery.",
        "Prototypical networks for few-shot learning.",
        "Matching the blanks: Distributional similarity for relation learning.",
        "Creating a large benchmark for open information extraction.",
        "Rotate: Knowledge graph embedding by relational rotation in complex space.",
        "Complex embeddings for simple link prediction.",
        "Attention is all you need.",
        "Row-less universal schema.",
        "Multilingual relation extraction using compositional universal schema.",
        "Efﬁcient inference and learning in a large knowledge base.",
        "Knowledge graph embedding by translating on hyperplanes.",
        "Open relation extraction: Relational knowledge transfer from supervised data to unsupervised data.",
        "Adversarial training for relation extraction.",
        "Classifying relations via long short term memory networks along shortest dependency paths.",
        "Embedding entities and relations for learning and inference in knowledge bases.",
        "Differentiable learning of logical rules for knowledge base reasoning.",
        "Multi-level matching and aggregation network for few-shot relation classiﬁcation.",
        "Kernel methods for relation extraction.",
        "Relation classiﬁcation via convolutional deep neural network.",
        "Distant supervision for relation extraction via piecewise convolutional neural networks.",
        "Relation classiﬁcation via recurrent neural network.",
        "Learning the classic description logic: Theoretical and experi- mental results."
      ],
      "meta_data": {
        "arxiv_id": "2103.11647v1",
        "authors": [
          "Ning Ding",
          "Xiaobin Wang",
          "Yao Fu",
          "Guangwei Xu",
          "Rui Wang",
          "Pengjun Xie",
          "Ying Shen",
          "Fei Huang",
          "Hai-Tao Zheng",
          "Rui Zhang"
        ],
        "published_date": "2021-03-22T08:11:43Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes a prototypical representation learning framework (ProtoRE/COL) for relation extraction that learns one prototype vector per relation from distantly-labeled text to obtain predictive, robust-to-noise, and interpretable relation representations usable across supervised, distantly supervised, few-shot, and even zero-shot settings. Key contribution is replacing pure instance-level cross-entropy supervision with prototype/metric-based objectives that enforce intra-relation compactness and inter-relation separability in a geometrically interpretable unit-ball space; demonstrates state-of-the-art performance and improved robustness/interpretability, and introduces FuzzyRED to evaluate semantic fidelity under distant supervision noise.",
        "methodology": "Uses a Transformer encoder (BERT-base) to embed relation statements with entity markers into a metric space. Learns trainable relation prototypes jointly with the encoder using: (1) statement–statement contrastive loss (LS2S) encouraging same-relation statements to be close and different-relation statements apart; (2) prototype–statement losses (LS2Z, LS2Z′) enforcing that each prototype is closer to its relation’s statements than to other relations’ statements, and that a relation’s statements are closer to their own prototype than to other prototypes; (3) an auxiliary prototype-level classification loss (LCLS) to regularize against noisy labels. Similarity is based on normalized dot product (angle) mapped through a sigmoid-like function, giving a unit-ball geometric interpretation where prototypes are unit vectors dispersed on the sphere and statements cluster around them. Training follows pretrain-then-finetune: pretrain on large distant data; finetune with cross-entropy for supervised RE or nearest-prototype/nearest-support classification for few-shot; evaluates also with no finetuning (zero-shot) in ablations.",
        "experimental_setup": "Pretraining: 0.86M distantly-labeled Wikipedia statements aligned from Wikidata, >700 relations; entities detected via NER; BERT-base backbone; batch size 60; AdamW lr 1e-5 with warmup; 5 epochs; includes masked language modeling during optimization (as stated). Baselines: MTB (Matching the Blanks) reimplemented with BERT-base and same pretraining data; BERTCE (cross-entropy pretrain on distant labels); IND (rule/pattern-derived fixed prototypes via Snowball) and various few-shot baselines (ProtoNet, GNN, MLMAN, etc.). Downstream evaluations: (1) Few-shot relation classification on FewRel (100 relations; 64/16/20 train/val/test split) using N-way K-shot (5/10-way, 1/5-shot) accuracy; additionally studies data-scarce regimes by varying #train relations and #instances per relation, including 0-shot. (2) Supervised RE on SemEval-2010 Task 8 (19-way) with P/R/F1; finetune for 30 epochs, batch size 20, Adam lr 1e-5. (3) Fuzzy relation evaluation on new FuzzyRED: 1000 manually labeled instances over 20 “fuzzy” relations selected by entity-pair multi-relation ambiguity; evaluates k-shot (1/3/5) binary TP/FP detection via distance-thresholding derived from true positives. Additional analyses: t-SNE visualization of prototypes/embeddings; toy Iris experiment to illustrate prototype-level decision-boundary robustness to label noise.",
        "limitations": "Prototype learning still relies on distant labels to form positive/negative groupings; if labels are highly corrupted or relation taxonomy mismatched, metric objectives may be biased. Assumes a single prototype per relation (unimodal class) which may underfit highly polysemous/multi-pattern relations; clustering on a unit-sphere may not capture complex manifolds. Computational cost: requires large-batch contrastive interactions and maintaining prototypes across many relations; pretraining uses only 0.86M instances and BERT-base (authors note performance limited vs larger resources). FuzzyRED results remain modest (~53% accuracy), indicating difficulty separating semantic relation truth from DS noise. The similarity function and “uniformly dispersed prototypes” claim is heuristic; no strong theoretical guarantees or explicit uniform-dispersion regularizer beyond losses. Few-shot inference uses nearest-support/nearest-prototype with limited calibration; may be sensitive to sampling and threshold choices (e.g., FuzzyRED threshold T definition).",
        "future_research_directions": "Extend to multiple prototypes per relation (mixture/prototype sets) to model intra-relation diversity and polysemy; learn hierarchical or structured prototypes reflecting relation ontologies. Develop theory and explicit regularizers for prototype dispersion and robustness under label noise; connect to margin bounds or noise-tolerant contrastive learning. Improve distant-supervision denoising by jointly modeling latent true labels, instance reliability, or bag-level/multi-instance objectives alongside prototypes. Explore stronger encoders (BERT-large/modern LLMs), larger-scale pretraining corpora, and multilingual settings; test transfer to domain-specific corpora (biomedical/legal). Integrate entity typing/knowledge graph signals and relation descriptions for better zero-shot generalization. Refine evaluation: expand FuzzyRED, add graded/fuzzy labels, and evaluate calibration and interpretability via human-in-the-loop prototype inspection and counterfactual examples.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs",
      "full_text": "Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs Meng Qu 1 2 Tianyu Gao 3 Louis-Pascal A. C. Xhonneux 1 2 Jian Tang1 4 5 Abstract This paper studies few-shot relation extraction, which aims at predicting the relation for a pair of entities in a sentence by training with a few labeled examples in each relation. To more effec- tively generalize to new relations, in this paper we study the relationships between different relations and propose to leverage a global relation graph. We propose a novel Bayesian meta-learning ap- proach to effectively learn the posterior distribu- tion of the prototype vectors of relations, where the initial prior of the prototype vectors is pa- rameterized with a graph neural network on the global relation graph. Moreover, to effectively optimize the posterior distribution of the proto- type vectors, we propose to use the stochastic gradient Langevin dynamics, which is related to the MAML algorithm but is able to handle the uncertainty of the prototype vectors. The whole framework can be effectively and efﬁciently opti- mized in an end-to-end fashion. Experiments on two benchmark datasets prove the effectiveness of our proposed approach against competitive base- lines in both the few-shot and zero-shot settings. 1. Introduction A fundamental problem in natural language processing is relation extraction, which aims to identify the relations be- tween entities in sentences. The problem is usually studied as a supervised classiﬁcation task by training with labeled sentences. However, annotating a large set of sentences is time-consuming and expensive. As a result, the number of labeled data is very limited for this task. In practice, a com- mon solution to this challenge is distant supervision (Mintz et al., 2009), where a knowledge graph is utilized to au- *Equal contribution 1Mila - Quebec AI Institute, Montr ´eal, Canada 2University of Montr ´eal, Montr ´eal, Canada 3Tsinghua University, Beijing, China 4HEC Montr ´eal, Montr ´eal, Canada 5CIFAR AI Research Chair. Correspondence to: Meng Qu <meng.qu@umontreal.ca>, Jian Tang <jian.tang@hec.ca>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). tomatically generate training data. For example, given a triplet (Washington, Capital, US) in a knowledge graph, all the sentences containing the two entities Washington and US will be labeled with the relation Capital. In this way, a large quantity of training data can be generated for each relation, and such an approach has been extensively studied and has been proven very effective. However, a limitation of distant supervision is that the generated training data can be very noisy. This is because there could be multiple rela- tions between two entities, and it is hard to determine which relation the entity pair belongs to in a particular context, or whether there is any relation expressed by the sentence. An alternative approach for relation extraction, which is attracting growing interest, is meta-learning for relation ex- traction (Han et al., 2018; Gao et al., 2019). The idea of meta-learning is to train models with a large number of diverse tasks, each of which has a few examples for demon- stration, so that the learned model can quickly generalize to new tasks with only a few examples. For example, the Model-Agnostic Meta-Learning (MAML) algorithm (Finn et al., 2017) tries to ﬁnd a good initialization for the param- eters of a neural model, based on which the model can be quickly adapted to a new task with several steps of gradient descent. Another example is the prototypical network (Snell et al., 2017), which learns a prototype vector from a few ex- amples for each class, and further uses the prototype vectors for prediction. Based on these techniques, handful recent studies (Han et al., 2018; Gao et al., 2019) are able to train relation extraction models with only a few examples for each relation. Although these methods achieve encouraging improvements, the performance remains unsatisfactory as the amount of information in training data is still limited. To more effectively generalize to new relations and tasks, in this paper we study modeling the relationships between different relations, and propose to leverage a global graph between different relations. In practice, such a global graph can be obtained in different ways. For example, we can use the knowledge graph embedding algorithms (Bordes et al., 2013; Sun et al., 2019) to infer the relation embeddings and then construct a K-nearest neighbor graph based on the relation embeddings. The global relation graph provides the prior knowledge on the relationships between different relations, which allows us to transfer supervision between these relations and even generalize to these relations without arXiv:2007.02387v1  [cs.LG]  5 Jul 2020Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs leveraging any labeled sentences (i.e. zero-shot learning). Moreover, we propose a novel Bayesian meta-learning ap- proach for few-shot relation extraction, which solves the problem by learning the prototype vectors of relations based on the labeled sentences (a.k.a. support set) and the global relation graph. Instead of learning a point estimation as in MAML (Finn et al., 2017) or prototypical networks (Snell et al., 2017), we follow existing work on Bayesian meta- learning (Gordon et al., 2019; Kim et al., 2018) and aim to infer the posterior distribution of the prototype vectors, which is able to effectively handle the uncertainty of the vectors. The posterior can be naturally factorized into a like- lihood function on the support set, and a prior of prototype vectors obtained from the global graph. We parameterize the prior distribution of prototype vectors of relations by applying a graph neural network (Kipf & Welling, 2017) to the global graph, allowing us to effectively utilize the rela- tionships between different relations encoded in the graph. For the posterior distribution of prototype vectors, existing studies (Gordon et al., 2019; Ravi & Beatson, 2019) usually parameterize it as a Gaussian distribution, and amortized variational inference is generally used to learn the posterior distribution. However, the posterior distribution of prototype vectors can be much more complicated than the Gaussian distribution. In this paper, instead of assuming a speciﬁc dis- tribution for the posterior distribution of prototype vectors, we propose to directly sample from the posterior distribu- tion with the stochastic gradient Langevin dynamics tech- nique (Welling & Teh, 2011), which is very general and can be applied to different distributions. Our approach can be viewed as a stochastic version of MAML (Finn et al., 2017), where random noises are added at each step of the gradi- ent descent to model the uncertainty of prototype vectors. The sampled prototype vectors are further used to make predictions for query sentences in test sets, and the whole framework can be optimized in an end-to-end fashion. We conduct extensive experiments to evaluate the proposed approach on two benchmark datasets of few-shot relation extraction. Empirical results prove the effectiveness of our proposed approach over many competitive baselines in both the settings of few-shot and zero-shot relation extraction. 2. Related Work 2.1. Few-shot Learning and Meta-learning Our work is related to few-shot learning and meta-learning. The goal is to train deep learning models with diverse tasks, where each task is speciﬁed by a few examples for demon- stration, so that the model can be quickly adapted to new tasks. One type of representative methods is the metric- based method (Vinyals et al., 2016; Snell et al., 2017; Garcia & Bruna, 2018; Sung et al., 2018). The basic idea is to learn a prototype vector for each class based on the few exam- ples, and use the prototype vectors for prediction. Another type of representative methods is the optimization-based method (Finn et al., 2017; Ravi & Larochelle, 2017). Typ- ically, these methods formalize the problem as a bi-level optimization problem (Franceschi et al., 2018). The outer loop learns global parameters shared across different tasks, such as the initialization of model parameters. The inner loop adapts the shared parameters to each task by perform- ing several steps of gradient descent according to the few examples. Compared with these methods, which aim to learn a point estimation of prototype vectors or model pa- rameters, our approach treats them as random variables and models their posterior distributions, which can thus handle the uncertainty of these prototype vectors or parameters. In addition, there are several recent studies (Kim et al., 2018; Gordon et al., 2019; Ravi & Beatson, 2019) also us- ing Bayesian learning techniques for meta-learning, where the posterior distributions of prototype vectors or model parameters are inferred. However, these methods ignore the relationships of different classes, while we model their relationships by applying a graph neural network (Kipf & Welling, 2017; Gilmer et al., 2017; Veliˇckovi´c et al., 2018) to a global graph of classes, allowing our approach to better generalize to all different classes. Furthermore, we model the posterior distribution in a more effective way. For Gor- don et al. (2019) and Ravi & Beatson (2019), they use a sim- ple Gaussian distribution parameterized by an amortization network to approximate the true posterior distribution. How- ever, the true posterior distribution can be more complicated than the Gaussian distribution, and hence these methods are less precise. Another method from Kim et al. (2018) uses Stein Variational Gradient Descent (SVGD) (Liu & Wang, 2016) to draw samples from the posterior distribution for optimization, but SVGD relies on a properly-designed kernel function for different samples, which can be hard to choose. In contrast, our approach uses the stochastic gradient Langevin dynamics (Welling & Teh, 2011) to per- form Monte Carlo sampling for optimization, which is more ﬂexible and effective as we will show in the experiment. 2.2. Relation Extraction Relation extraction is a fundamental task in natural language processing. Given two entities in a sentence, the goal is to predict the relation expressed in the sentence. Most existing studies (Zeng et al., 2014; 2015; Zhang et al., 2017) focus on the supervised or semi-supervised settings of relation extraction, where they assume massive labeled sentences are available. However, the number of labeled sentences can be very limited in practice. Some studies try to solve the challenge of insufﬁcient labeled sentences by resorting to knowledge graphs, where existing facts are used to anno- tate unlabeled sentences through distant supervision (MintzFew-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs et al., 2009) or provide additional training signals (Shwartz et al., 2016; Qu et al., 2018; Zhu et al., 2019a). Nevertheless, the data or signals obtained in this way can be very noisy. Some more recent studies (Han et al., 2018; Gao et al., 2019; Soares et al., 2019) instead focus on few-shot relation extrac- tion, and the goal is to develop models which can be trained with only a few labeled sentences. By combining meta- learning methods with BERT encoders (Devlin et al., 2019), these methods achieve impressive results. However, they ignore the connections of different relations, which naturally exist in practice. In contrast, we treat a global graph of rela- tions as prior knowledge, and propose a principled Bayesian meta-learning approach to leverage the global graph, which is able to better generalize across different relations. 3. Problem Deﬁnition Relation extraction is an important task in many research areas, which aims at predicting the relation of two entities given a sentence. Most existing methods for relation extrac- tion require a large number of labeled sentences as training data, which are expensive to obtain. Therefore, more recent studies focus on few-shot relation extraction, where only a few examples for each relation are given as training data. However, the results are still far from satisfactory due to the limited information in these examples. To further improve the results, another data source should be considered. In this paper, we propose to study few-shot relation extrac- tion with a global graph of relations, where a global graph describing the relationships of all possible relations is as- sumed to be an additional data source. More formally, we denote the global relation graph as G= (R,L), where Ris the set of all the possible relations, and Lis a collection of links between different relations. The linked relations are likely to have more similar semantic meanings. In few-shot relation extraction, each time we only consider a subset of relations from the whole relation set, i.e., T ⊆ R. Given a few support sentences Sof these relations, where xS= {xs}s∈Srepresents the text of these sentences, and yS = {ys}s∈S represents the corresponding labels with each ys ∈T , our goal is to learn a neural classiﬁer for these relations by using the global graph and support sentences. Then given some unlabeled sentences as queries xQ= {xq}q∈Q, we will apply the classiﬁer to predict their labels, i.e., yQ= {yq}q∈Qwith each yq ∈T . 4. Model In this section, we introduce our approach for few-shot re- lation extraction with global relation graphs. Our approach represents each relation with a prototype vector, which can be used for classifying the query sentences. In contrast to most existing meta-learning methods, which learn a point estimation of the prototype vector, we treat the prototype vector as a random variable to model its posterior distri- bution. The posterior distribution is naturally composed of two terms, i.e., a prior of the prototype vector obtained from the global relation graph, and a likelihood function on the support sentences. Our approach parameterizes the prior distribution by applying a graph neural network to the global relation graph. By using such a graph-based prior, our approach can effectively generalize to different relations. However, optimization remains challenging in our approach, as the posterior distribution of prototype vectors has a com- plicated form. To address that, we approximate the posterior distribution through Monte Carlo sampling, where multiple samples of prototype vectors are drawn by using the stochas- tic gradient Langevin dynamics. By doing so, our approach can be effectively optimized in an end-to-end fashion. 4.1. A Probabilistic Formalization Our approach uses Bayesian learning techniques for few- shot relation extraction, where we formalize the problem in a probabilistic way. More speciﬁcally, recall that given a subset of relations T ⊆R, the goal is to predict the labels yQ of some query texts xQ based on a global relation graph Gand a few support sentences (xS,yS). Formally, our goal can be stated as computing the following log-probability: log p(yQ|xQ,xS,yS,G). (1) We compute the probability by representing each relation r ∈T with a prototype vector vr, which summarizes the semantic meaning of that relation. By introducing such prototype vectors, the log-probability can be factorized as: log p(yQ|xQ,xS,yS,G) = log ∫ p(yQ|xQ,vT)p(vT|xS,yS,G)dvT (2) where vT = {vr}r∈T is a collection of prototype vectors for all the target relations in T. These prototype vectors are characterized by both the labeled sentences in the sup- port set and global relation graph through the distribution p(vT|xS,yS,G). With such prototype vectors to represent target relations, the distribution of query sentence labels can then be deﬁned through a softmax function as follows: p(yQ|xQ,vT) = ∏ q∈Q p(yq|xq,vT), with each p(yq = r|xq,vT) = exp(E(xq) ·vr)∑ r′∈T exp(E(xq) ·vr′), (3) where Eis a sentence encoder, which encodes a query sen- tence xq into an encoding E(xq). Intuitively, we compute the dot product of the encoding and the prototype vector vr to estimate how likely the sentence expresses the relation.Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs Relation 𝛼 Relation 𝛽 Relation 𝛾 [---] - --- , -- [---] . --- [--] ---- --- [--] . -- -- [---] , [----- ] . 𝛼𝛽 𝛾 Global Relation Graph A Few Labeled Sentences p ( v T |G ) p ( y S | x S , v T ) Graph-based Prior Likelihood p ( v T | x S , y S , G ) Posterior BERT GNN Langevin  Dynamics ⇝ 𝛼 𝛽 𝛾 Prototype Vectors Figure 1.Framework overview. We consider a global relation graph and a few labeled sentences of each relation. Our approach aims at modeling the posterior distribution of prototype vectors for different relations. The prior distribution in the posterior is parameterized by applying a graph neural network to the global graph, and the likelihood is parameterized by using BERT to the labeled sentences. We use stochastic gradient Langevin dynamics to draw multiple samples from the posterior for optimization, which is in an end-to-end fashion. Under such a formalization, the key is how to parameterize p(vT|xS,yS,G), which is the posterior distribution of pro- totype vectors conditioned on the support sentences and the global relation graph. Next, we introduce how we parame- terize the posterior distribution in our proposed approach. 4.2. Parameterization of the Posterior Distribution To model the posterior distribution of prototype vectors, we notice that the posterior can be naturally factorized into a prior distribution conditioned on the relation graph, and a likelihood function on the few support sentences. Therefore, we can formally represent the posterior as follows: p(vT|xS,yS,G) ∝p(yS|xS,vT)p(vT|G), (4) where p(vT|G) is the prior for the prototype vectors and p(yS|xS,vT) is the likelihood on support sentences. To extract the knowledge from the global relation graph to characterize the prior distribution, we introduce a graph neural network (Kipf & Welling, 2017; Gilmer et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) in our approach. The graph neural network is denoted as F, which can learn a la- tent representation hr for each relation r, i.e., hr = F(G)r. More speciﬁcally, the graph neural network Finitializes the latent embedding hr of each relation as its initial feature vector. Then Fiteratively updates the latent embedding of each relation r according to the embeddings of r and r’s neighbors. Formally, Fupdates the embeddings as follows: hr ←U    ∑ r′∈NB(r) M(hr′),hr   , (5) where NB(r) is the neighbor of rin the global graph, and Mis a transformation function. Basically, for each relation r, we apply Mto the latent embeddings of r’s neighbors and then aggregate the transformed embeddings together. Finally, the latent embedding of ris updated based on its previous value and the aggregated embeddings through an update function U. After several rounds of such update, the relationships between different relations encoded in the global graph can be effectively preserved into the ﬁnal rela- tion embeddings, which can serve as regularization for the prototype vectors. Motivated by that, we parameterize the prior distribution of prototype vectors p(vT|G) as follows: p(vT|G) = ∏ r∈T p(vr|hr) = ∏ r∈T N(vr|hr,I), (6) where we model the prior distribution of each relationr∈T independently. For each relation, we deﬁne its prior as a Gaussian distribution, where the mean is set as the latent representation hr given by the graph neural network F. In this way, the knowledge from the relation graph can be effectively distilled into the prior distribution, allowing our approach to better generalize to a wide range of relations. Besides the graph-based prior, we also consider the likeli- hood on support sentences when parameterizing the poste- rior distribution of prototype vectors. Similar to the likeli- hood on the query sentences in Eq. (3), the likelihood on support sentences can be characterized as below: p(yS|xS,vT) = ∏ s∈S p(ys|xs,vT), with each p(ys = r|xs,vT) = exp(E(xs) ·vr)∑ r′∈T exp(E(xs) ·vr′), (7) where Eis the sentence encoder. By applying the likelihood on support sets to the prior distribution of prototype vectors, we can effectively adapt the prior distribution to the target relations with the few support sentences. In this way, the posterior distribution combines the knowledge from both the global relation graph and the support sentences, which can thus be used to effectively classify query sentences. 4.3. Optimization and Prediction In the above section, we have introduced how we parame- terize the posterior distribution of prototype vectors. Next, we explain the model optimization and prediction.Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs During both model optimization and prediction, we have to deal with the log-probability log p(yQ|xQ,xS,yS,G), where we either maximize or compute this value. However, according to Eq. (2), the log-probability relies on the integra- tion over prototype vectors, which is intractable. Therefore, we estimate the log-probability with Monte Carlo sampling, where several samples of prototype vectors are drawn from the posterior distribution for approximation: log p(yQ|xQ,xS,yS,G) = logEp(vT |xS,yS,G)[p(yQ|xQ,vT)] ≈log 1 L L∑ l=1 p(yQ|xQ,v(l) T ). (8) Here, v(l) T is a sample drawn from the posterior distribution, i.e., v(l) T ∼p(vT|xS,yS,G). However, as shown in Eq.(4), the posterior distribution combines both a graph-based prior and a likelihood function on support sentences. The graph- based prior is a Gaussian distribution while the likelihood function is speciﬁed by a softmax function. Therefore, the posterior distribution has a very complicated form, and sam- pling from the posterior is nontrivial. To address the prob- lem, in this paper we use the stochastic gradient Langevin dynamics (Welling & Teh, 2011), which performs sampling via multiple stochastic updates. Formally, to draw a sample ˆvT of the prototype vector, we can randomly initialize the sample ˆvT, and iteratively update the sample as follows: ˆvT ←ˆvT + ϵ 2∇ˆvT log p(yS|xS,ˆvT)p(ˆvT|G) + √ϵˆz, (9) where ˆz ∼N (0,I) is a sample from the standard Gaus- sian distribution. After a burn-in period, ˆvT can then be treated as a true sample from the posterior distribution of prototype vectors. In the above update rule, the term ∇ˆvT log p(yS|xS,ˆvT)p(ˆvT|hT) is highly related to the MAML algorithm (Finn et al., 2017), as they both aim at moving the sample towards maximizing the likelihood log p(yS|xS,ˆvT) on support sentences, and thereby adapt to the target relations in T. The difference is that we also leverage the graph-based prior p(ˆvT|G) to guide this pro- cess. Moreover, at each step, a random noise ˆz is added, allowing us to get different samples from the posterior dis- tribution p(vT|xS,yS,G), rather than a single sample with maximum posterior probability. In other words, our ap- proach is able to model the uncertainty of prototype vectors. However, the Langevin dynamics requires a burn-in period, which can take a long time. To accelerate this process, it is helpful to let the samples stay in the high-density regions of the posterior distribution, such that we can better explore around those regions (Gong et al., 2019). Therefore, we try to initialize the sample ˆvT at a point with high posterior Algorithm 1 Training Algorithm Given: A relation set R, a global relation graph G. while not converge do 1. Sample a subset of relations T ⊆Ras targets. 2. Sample support and query sets (xS,yS) (xQ,yQ). 3. Compute the summary vectors hT of relations by applying the GNN Fto the global relation graph G. 4. Initialize prototype vectors {v(l) T }L l=1 with Eq. (10). 5. Update prototype vectors for M steps with Eq. (9). 6. Compute and maximize the log-likelihood function log p(yQ|xQ,xS,yS,G) based on Eq. (8). end while probability. Towards this goal, we theoretically justify in appendix that a proper initialization can be given as below: ˆvT ←{ˆvr}r∈T, with each ˆvr ←mr + hr −m, (10) where hr is the latent embedding of relation rgiven by the graph neural network on the global relation graph,mr is the mean encoding of all the sentences under relation rin the support set, and m is the mean encoding of all the sentences in the support set. Intuitively, for each relation r, we add the latent embedding hr from the global relation graph and the mean encoding mr from the given examples of that relation. Also, we subtract the mean encoding m of all sentences in the support set, so that we can better distinguish sentences from different relations. In practice, we introduce two hyperparameters for hr and m to control their relative weights. With such an initialization, we can empirically guarantee that the Langevin dynamics will quickly converge. Once we obtain the samples of prototype vectors from their posterior distribution, we can then use the samples to op- timize and compute log p(yQ|xQ,xS,yS,G) according to Eq. (8). The whole optimization process is end-to-end, and we summarize the optimization algorithm in Alg. 1. 5. Experiment In this section, we empirically evaluate our proposed ap- proach on two benchmark datasets, and we consider both the few-shot and the zero-shot learning settings. 5.1. Datasets Table 1.Dataset statistics. Dataset FewRel NYT-20 # All Possible Relations 828 828 # Training Relations 64 10 # Validation Relations 16 5 # Test Relations 20 10 # Sentences per Relation 700 100Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs Table 2.Results of few-shot classiﬁcation on the FewRel test set (%). We rerun all the algorithms with the same sentence encoder BERTBASE. Our approach outperforms all the baseline methods. Algorithm 5-Way 1-Shot 5-Way 5-Shot 10-Way 1-Shot 10-Way 5-Shot GNN 75.66 89.06 70.08 76.93 SNAIL 67.75 86.58 54.29 77.54 Proto 80.68 89.60 71.48 82.89 Pair 88.32 93.22 80.63 87.02 MTB 89.80 93.59 83.37 88.64 MAML 89.70 93.55 83.17 88.51 Versa 88.52 93.15 81.62 87.73 BMAML 89.65 93.40 82.94 88.20 REGRAB 90.30 94.25 84.09 89.93 We use two benchmark datasets for evaluation. One dataset is the FewRel dataset (Han et al., 2018; Gao et al., 2019), which has been recently proposed for few-shot relation ex- traction. Note that only the training set and validation set of FewRel are released, and the test set is not public, so researchers have to evaluate models on the remote cluster provided by the FewRel team. Because of that, we conduct most of the performance analysis on the validation set of FewRel, and for the test set we only report a ﬁnal number. The other dataset is NYT-25. The raw data of NYT-25 is from the ofﬁcial website of FewRel 1 (Han et al., 2018; Gao et al., 2019), where labeled sentences under 25 relations are provided by annotating the New York Times data. However, the dataset has not been splitted into training, validation and test sets. Therefore, we randomly sample 10 relations for training, 5 for validation and the remaining 10 for test. For both datasets, the relations are from a knowledge graph named Wikidata 2, which has 828 unique relations in total. To construct the global graph of all the relations, we ﬁrst employ GraphVite (Zhu et al., 2019b) to run the TransE algorithm (Bordes et al., 2013) on Wikidata to learn a 512- dimensional embedding vector for each relation. Then we use the relation embeddings to construct a 10-nearest neigh- bor graph as the global relation graph, and the learned rela- tion embeddings are treated as the initial relation features in GNN F. The detailed statistics are summarized in Tab. 1. 5.2. Compared Algorithms We choose the following few-shot relation extraction meth- ods and meta-learning methods for comparison. (1) Pair (Gao et al., 2019): A few-shot relation extrac- tion method, which performs prediction by measuring the similarity of a pair of sentences. (2) MTB (Soares et al., 2019): A few-shot relation extraction method, which could 1 https://github.com/thunlp/FewRel 2https://www.wikidata.org/wiki/Wikidata: Main_Page be viewed as a variant of the prototypical network (Snell et al., 2017), where dot product is used to measure vector similarity rather than Euclidean distance. (3) GNN (Garcia & Bruna, 2018): A meta-learning method which uses graph neural networks for prediction. (4) SNAIL (Mishra et al., 2018): An algorithm which uses temporal convolutional neu- ral networks and attention mechanisms for meta-learning. (5) Proto (Snell et al., 2017): The algorithm of prototypi- cal networks. (6) MAML (Finn et al., 2017): The model- agnostic meta-learning algorithm. (7) Versa (Gordon et al., 2019): A Bayesian meta-learning method which uses amor- tization networks to model the posterior of prototype vectors. (8) BMAML (Kim et al., 2018): A Bayesian meta-learning method which uses SVGD to learn the posterior distribu- tion. (9) REGRAB: Our proposed approach for relation extraction with graph-based Bayesian meta-learning. For all the meta-learning algorithms, we use BERTBASE (De- vlin et al., 2019) as encoder to project sentences into en- codings, and apply a linear softmax classiﬁer on top of the encoding for classiﬁcation, where the meta-learning algo- rithms are used to learn the parameters in the classiﬁer, or in other words the prototype vectors of different relations. 5.3. Parameter Settings In our approach, we use BERTBASE (Devlin et al., 2019) as encoder to encode all the tokens in a sentence. Then we follow Soares et al. (2019) and combine the token encodings of entity mentions in a sentence as the sentence encoding. We do the same thing for the advanced meta-learning algo- rithms, i.e., MTB, Proto, MAML, Versa, BMAML, in order to make fair comparison. The details of how we compute sentence encodings are explained in the appendix. For the softmax function of the likelihood on support and query sentences, we apply an annealing temperature of 10. For the Gaussian prior of prototype vectors, we apply a one-layer graph convolutional network (Kipf & Welling, 2017) to the global relation graph to compute the mean. We have also tried more layers, but only obtain very marginal improve- ment. For the stochastic gradient Langevin dynamics, the number of samples to draw is set as 10 by default, which is the same as used by other Bayesian meta-learning methods, and we perform 5 steps of update for these samples with the initial step size (i.e., ϵin Eq. (9)) as 0.1 by default. The graph encoder and sentence encoder are tuned by SGD with learning rate as 0.1. For other hyperparameters, they are selected on the FewRel validation set through grid search. 5.4. Results 5.4.1. Comparison with Baseline Methods The main results on FewRel test set, FewRel validation set and NYT-25 test set are presented in Tab. 2, Tab. 3 andFew-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs Table 3.Results of few-shot classiﬁcation on FewRel validation set (%). All the methods use the same encoder for fair comparison. Category Algorithm 5-Way 1-Shot 5-Way 5-Shot 10-Way 1-Shot 10-Way 5-Shot Meta-learning Pair 85.66 89.48 76.84 81.76 MTB 84.61 88.76 75.22 80.15 Proto 82.92 91.32 73.24 83.68 MAML 82.93 86.21 73.20 76.06 Bayesian Meta-learning Versa 84.47 88.44 74.70 79.20 BMAML 85.80 89.71 76.66 81.34 REGRAB 87.95 92.54 80.26 86.72 Table 4.Results of few-shot classiﬁcation on NYT-25 test set (%). All the methods use the same sentence encoder for fair comparison. Category Algorithm 5-Way 1-Shot 5-Way 5-Shot 10-Way 1-Shot 10-Way 5-Shot Meta-learning Pair 80.78 88.13 72.65 79.68 MTB 88.90 95.53 83.08 92.23 Proto 77.63 87.25 66.49 79.51 MAML 86.96 93.36 79.62 88.32 Bayesian Meta-learning Versa 87.70 93.77 81.27 89.35 BMAML 87.03 93.90 79.74 88.72 REGRAB 89.76 95.66 84.11 92.48 Tab. 4 respectively. For fair comparison, the same sentence encoder BERTBASE is used for all the compared approaches. From Tab. 2, we can see that the results of GNN and SNAIL are less competitive, showing that they are less effective to model textual data. Compared with Pair and MTB, which are speciﬁcally designed for few-shot relation extraction, our approach achieves relatively better results in all the ta- bles, showing that our approach can better generalize to a variety of relations given a few examples. Besides, our approach also outperforms other meta-learning methods. Comparing our approach with MAML and the prototypical network (Proto), the performance gain mainly comes from two aspects. On the one hand, our approach considers a global graph of different relations, which provides prior knowledge about the relationships of all the relations, al- lowing our approach to better adapt to different relations. On the other hand, our approach uses a Bayesian learning framework, which effectively deals with the uncertainty of the prototype vectors for different relations. Moreover, our approach is also superior to other Bayesian meta-learning methods, i.e., Versa and BMAML. The reason is that we consider a graph-based prior in the posterior distribution, making our approach more powerful. Also, our approach performs optimization through Monte Carlo sampling with stochastic gradient Langevin dynamics, which models and optimizes the posterior distribution in a more effective way. 5.4.2. Analysis of the Graph-based Prior Compared with existing studies on few-shot relation extrac- tion, our approach uses a global relation graph. The relation graph provides knowledge of the relationship between differ- ent relations, allowing our approach to better generalize to various relations. To leverage such knowledge, our approach parameterizes the prior distribution of prototype vectors for different relations by applying a graph neural network to the relation graph. In this section, we conduct experiments to analyze the effect of such a graph-based prior distribution. Table 5.Analysis of the graph-based prior on FewRel validation set (%). Removing the graph-based prior reduces the accuracy. Algorithm 5-Way 1-Shot 10-Way 1-Shot REGRAB with graph prior 87.95 80.26 REGRAB w/o graph prior 85.82 77.70 We ﬁrst conduct some ablation study on the FewRel valida- tion set, where we compare two variants of our approach, i.e., with or without using the graph-based prior. The results are presented in Tab. 5. We can see that removing the graph- based prior results in signiﬁcantly worse results, showing the effect of the graph-based prior for improving the relation extraction performance in the few-shot learning setting. Moreover, with such a graph-based prior, our approach is able to handle relation extraction in the zero-shot learning setting, where no labeled sentences of each relation are given. Next, we present results on the FewRel validation set and NYT-25 test set to justify that. Remember our approach constructs the relation graph based on the pre-trained rela- tion embeddings, and then applies a graph neural network for parameterizing the prior of prototype vectors. Compared with using graph neural networks, a more straightforward way is to directly apply a feedforward neural network to the pre-trained relation embeddings to derive the prior of proto- type vectors. To demonstrate the advantange of graph neural networks, we also compare with the aforementioned variant.Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs 2 3 4 5 6 7 8 9 10 N-way 40 45 50 55 60 65 70 75Accuracy (%) Algorithm With GNN W/o GNN (a) FewRel Validation Set 2 3 4 5 6 7 8 9 10 N-way 30 40 50 60 70Accuracy (%) Algorithm With GNN W/o GNN (b) NYT-25 Test Set Figure 2.Results in zero-shot learning settings. Our approach can still achieve quite good results even without any labeled sentences. We present the results in Fig. 2. From the ﬁgure, we can see that even without any labeled sentences as demonstration, our approach is still able to achieve quite impressive results, which proves its effectiveness. Also, comparing with the variant without using graph neural networks, our approach achieves signiﬁcantly better results in both datasets. The observation shows that the graph neural network can help our approach better leverage the relationships of relations. 5.4.3. Analysis of the Optimization Algorithm Table 6.Ablation study of the optimization algorithm on FewRel validation set (%). The Langevin dynamics leads to better results. Algorithm 5-Way 1-Shot 10-Way 1-Shot REGRAB with Langevin 87.95 80.26 REGRAB with Amortized VI 85.74 77.41 Compared with existing methods, the other difference of our approach is that we use the stochastic gradient Langevin dynamics during training, where several samples of proto- type vectors are drawn for optimization. More speciﬁcally, we initialize a set of samples according to Eq.(10), and then perform multiple steps of update based on Eq. (9). In this part, we thoroughly analyze the optimization algorithm. We start with the ablation study on the FewRel validation set, where we compare with a variant which parameterizes the posterior of prototype vectors as a Gaussian distribu- tion. The mean of the Gaussian distribution is set as the value given by Eq. (10), which is the same as the initial- ization of samples we use in the Langevin dynamics. Such a variant is similar to the amortized variational inference method used in Versa (Gordon et al., 2019), an existing Bayesian meta-learning algorithm. We present the results in Tab. 6. We see that our approach with Langevin dynam- ics achieves relatively better results than the variant with amortized variational inference, which proves the effective- ness of approximating the posterior distribution of prototype vectors by drawing samples with the Langevin dynamics. In addition, when drawing samples from the posterior dis- tribution, the Langevin dynamics performs multiple steps 2 4 6 8 10 12 14 #Samples 85.0 85.5 86.0 86.5 87.0 87.5 88.0Accuracy of 5-way 1-shot Classification (%) Algorithm With Lanvein Dynamics W/o Lanvein Dynamics (a) Analysis of #Samples 0 2 4 6 8 10 #Steps 86.0 86.5 87.0 87.5 88.0Accuracy of 5-way 1-shot Classification (%) Algorithm With Lanvein Dynamics W/o Lanvein Dynamics (b) Analysis of #Steps Figure 3.Performance w.r.t. the number of samples and the num- ber of steps in the stochastic gradient Langevin dynamics. of update on a set of samples. Therefore two important hyperparameters of this process are the number of samples and the number of update steps. Next, we analyze these hyperparameters by doing sensitivity analysis. We take the FewRel validation set as an example, and report the accu- racy of 5-way 1-shot classiﬁcation. To better understand the result, we introduce a variant, where we only initialize the samples of prototype vectors through Eq. (10), without fur- ther updating them according to Eq. (9). Fig. 3(a) shows the results under different numbers of samples. We can see if only one or two samples are used, the results are quite poor, which are even worse than the variant without updating the samples. The reason is that if we only use few samples, the estimation of the log-probability in Eq. (8) can have high variance, leading to poor results. As we use more samples, the results are quickly improved, and the results converge when around 8-10 samples are used, which is quite efﬁcient. Besides, Fig. 3(b) presents the results under different num- bers of update steps. As the number of steps is increased, the accuracy also rises, since the samples are moving towards high-density regions of the posterior to perform exploration. After only 4-5 steps, the accuracy quickly converges, which is very efﬁcient. This observation proves the effectiveness of our initialization strategy presented in the Eq. (10). 6. Conclusion This paper studies relation extraction in the few-shot learn- ing setting, and the key idea is to consider a global relation graph, which captures the global relationship between rela- tions. We propose a novel Bayesian meta-learning approach, which aims to model the posterior distribution of prototype vectors for different relations. The prior distribution in the posterior is parameterized by applying a graph neural net- work to the global relation graph. The stochastic gradient Langevin dynamics is used to optimize the posterior distri- bution. Experiments on two datasets prove the effectiveness of our approach. In the future, we plan to study learning the structure of the relation graph automatically by following existing studies (Franceschi et al., 2019). Besides, we also plan to apply our approach to other applications, such as few-shot image classiﬁcation (Triantaﬁllou et al., 2020).Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs Acknowledgements This project is supported by the Natural Sciences and Engi- neering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants be- tween Microsoft Research and Mila, Amazon Faculty Re- search Award, Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R&D Project (AI4D-CORE-06). We would like to thank Andreea Deac and Zhaocheng Zhu for reviewing the paper before submission. References Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. Translating embeddings for modeling multi-relational data. In NeurIPS, 2013. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. In NAACL, 2019. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In ICML, 2017. Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M. Bilevel programming for hyperparameter optimization and meta-learning. In ICML, 2018. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learning discrete structures for graph neural networks. In ICML, 2019. Gao, T., Han, X., Zhu, H., Liu, Z., Li, P., Sun, M., and Zhou, J. Fewrel 2.0: Towards more challenging few-shot relation classiﬁcation. In EMNLP, 2019. Garcia, V . and Bruna, J. Few-shot learning with graph neural networks. In ICLR, 2018. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. In ICML, 2017. Gong, W., Li, Y ., and Hern ´andez-Lobato, J. M. Meta- learning for stochastic gradient mcmc. In ICLR, 2019. Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R. E. Meta-learning probabilistic inference for prediction. In ICLR, 2019. Han, X., Zhu, H., Yu, P., Wang, Z., Yao, Y ., Liu, Z., and Sun, M. Fewrel: A large-scale supervised few-shot relation classiﬁcation dataset with state-of-the-art evaluation. In EMNLP, 2018. Kim, T., Yoon, J., Dia, O., Kim, S., Bengio, Y ., and Ahn, S. Bayesian model-agnostic meta-learning. In NeurIPS, 2018. Kipf, T. N. and Welling, M. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR, 2017. Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In NeurIPS, 2016. Mintz, M., Bills, S., Snow, R., and Jurafsky, D. Distant supervision for relation extraction without labeled data. In ACL, 2009. Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. A simple neural attentive meta-learner. In ICLR, 2018. Qu, M., Ren, X., Zhang, Y ., and Han, J. Weakly-supervised relation extraction by pattern-enhanced embedding learn- ing. In WWW, 2018. Qu, M., Bengio, Y ., and Tang, J. Gmnn: Graph markov neural networks. In ICML, 2019. Ravi, S. and Beatson, A. Amortized bayesian meta-learning. In ICLR, 2019. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. In ICLR, 2017. Shwartz, V ., Goldberg, Y ., and Dagan, I. Improving hyper- nymy detection with an integrated path-based and distri- butional method. In ACL, 2016. Simic, S. On a global upper bound for jensen’s inequality. Journal of mathematical analysis and applications, 2008. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In NeurIPS, 2017. Soares, L. B., FitzGerald, N., Ling, J., and Kwiatkowski, T. Matching the blanks: Distributional similarity for relation learning. In ACL, 2019. Sun, Z., Deng, Z.-H., Nie, J.-Y ., and Tang, J. Rotate: Knowl- edge graph embedding by relational rotation in complex space. In ICLR, 2019. Sung, F., Yang, Y ., Zhang, L., Xiang, T., Torr, P. H., and Hospedales, T. M. Learning to compare: Relation net- work for few-shot learning. In CVPR, 2018. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., and Larochelle, H. Meta-dataset: A dataset of datasets for learning to learn from few examples. In ICLR, 2020. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks. In ICLR, 2018.Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In NeurIPS, 2016. Welling, M. and Teh, Y . W. Bayesian learning via stochastic gradient langevin dynamics. In ICML, 2011. Zeng, D., Liu, K., Lai, S., Zhou, G., and Zhao, J. Relation classiﬁcation via convolutional deep neural network. In COLING 2014, 2014. Zeng, D., Liu, K., Chen, Y ., and Zhao, J. Distant supervision for relation extraction via piecewise convolutional neural networks. In EMNLP, 2015. Zhang, Y ., Zhong, V ., Chen, D., Angeli, G., and Manning, C. D. Position-aware attention and supervised data im- prove slot ﬁlling. In EMNLP, 2017. Zhu, Q., Ren, X., Shang, J., Zhang, Y ., El-Kishky, A., and Han, J. Integrating local context and global cohesiveness for open information extraction. In WSDM, 2019a. Zhu, Z., Xu, S., Qu, M., and Tang, J. Graphvite: A high- performance cpu-gpu hybrid system for node embedding. In WWW, 2019b.Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs A. Justiﬁcation of the Initialization in the Langevin Dynamics In our approach, we leverage the Langevin dynamics to draw samples of the prototype vectors from their posterior distributions. Recall that the Langevin dynamics requires a burn-in period, which can take a long time. To accelerate the process, it is helpful to initialize the sample within a high-density area, which can prevent the sample from exploring low-density areas. Motivated by that, our approach aims to initialize the sample of prototype vector at a point with high posterior probability, and therefore we use the following strategy for initialization: ˆvT ←{ˆvr}r∈T, with each ˆvr ←mr + hr −m, where m is the mean encoding of all the sentences in the support set, and mr is the mean encoding for all the sentences of relation rin the support set. In the remainder of this section we justify this choice. Our goal is to ﬁnd the point with high posterior probability. Suppose we consider the N-way K-shot setting, where there are N relations in T (i.e., |T| = N), and each relation has Kexamples in the support set. Then the posterior is given as: log p(vT|xS,yS,G) = 1 K log p(yS|xS,vT) + logp(vT|G) +const = 1 K ∑ s∈S ∑ r∈T I{ys = r}log exp(E(xs) ·vr)∑ r′∈T exp(E(xs) ·vr′) + ∑ r∈T log exp ( −1 2 ∥vr −hr∥2 2 ) + const = 1 K ∑ s∈S ∑ r∈T I{ys = r}(E(xs) ·vr) − 1 K ∑ s∈S log ∑ r∈T exp(E(xs) ·vr) −1 2 ∑ r∈T ∥vr −hr∥2 2 + const, where we add a normalization term 1 K to the log-likelihood function, which makes the log-likelihood numerically stable as we increase the number of examples for each relation (i.e., K). Our goal is to ﬁnd a point of vT to maximize the above log-probability. However, the log-probability contains a log-partition function log ∑ r∈T exp(E(xs) ·vr), which is hard to compute. To address this challenge, we aim at deriving a lower bound of the log-probability function for approximation. For this purpose, in this paper we use an inequation proposed by Simic (2008), which is formally stated in the following theorem. Theorem A.1. Suppose that ˜x= {xi}n i=1 represents a ﬁnite sequence of real numbers belonging to a ﬁxed closed interval I = [a,b], a<b . If f is a convex function on I, then we have that: 1 n n∑ i=1 f(xi) −f ( 1 n n∑ i=1 xi ) ≤f(a) +f(b) −2f (a+ b 2 ) . Based on the above theorem, we can have the following corollary: Corollary A.1. Suppose that for all the s∈S and r∈T , we have exp(E(xs)·vr) ∈[a,b]. As (−log) is a convex function, we therefore have: − 1 |T| ∑ r∈T log(exp(E(xs) ·vr)) + log ( 1 |T| ∑ r∈T exp(E(xs) ·vr) ) ≤−log(a) −log(b) + 2 log (a+ b 2 ) . After some simpliﬁcation, we get: log (∑ r∈T exp(E(xs) ·vr) ) ≤ ∑ r∈T 1 |T| log(exp(E(xs) ·vr)) + log(|T|) −log(a) −log(b) + 2 log (a+ b 2 ) = ∑ r∈T 1 |T|E(xs) ·vr + log(|T|) −log(a) −log(b) + 2 log (a+ b 2 ) .Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs In practice, we can easily ﬁnd such aand bso that exp(E(xs) ·vr) ∈[a,b] is satisﬁed. Given this corollary, we are able to obtain a lower bound of the log-proability function as follows: log p(vT|xS,yS,G) = 1 K ∑ s∈S ∑ r∈T I{ys = r}(E(xs) ·vr) − 1 K ∑ s∈S log ∑ r∈T exp(E(xs) ·vr) −1 2 ∑ r∈T ∥vr −hr∥2 2 + const ≥1 K ∑ s∈S ∑ r∈T I{ys = r}(E(xs) ·vr) − 1 K ∑ s∈S [∑ r∈T 1 |T|E(xs) ·vr + log(|T|) −log(a) −log(b) + 2 log (a+ b 2 )] −1 2 ∑ r∈T ∥vr −hr∥2 2 + const = 1 K ∑ s∈S ∑ r∈T I{ys = r}(E(xs) ·vr) − 1 K ∑ s∈S ∑ r∈T 1 NE(xs) ·vr −1 2 ∑ r∈T ∥vr −hr∥2 2 + const. Based on that, let us denote m = 1 NK ∑ s∈SE(xs) to be the mean of encodings for all the sentences in the support set, and denote mr = 1 K ∑ s∈SI{ys = r}E(xs) to be the mean of encodings for sentences under relation rin the support set. In this way, the above lower bound can be rewritten as follows: log p(vT|xS,yS,G) ≥1 K ∑ s∈S ∑ r∈T I{ys = r}(E(xs) ·vr) − 1 K ∑ s∈S ∑ r∈T 1 NE(xs) ·vr −1 2 ∑ r∈T ∥vr −hr∥2 2 + const = ∑ r∈T [ vr ·mr −vr ·m + vr ·hr −1 2vr ·vr −1 2hr ·hr ] + const = ∑ r∈T [ vr ·mr −vr ·m + vr ·hr −1 2vr ·vr ] + const = ∑ r∈T [ −1 2 ∥vr −mr −hr + m∥2 2 ] + const. Therefore, under this lower bound, the optimal initialization of prototype vector vr for each relation r∈T is given by: v∗ r = mr + hr −m, where we can ensure v∗ r to have a pretty high probability under the posterior distribution, and hence the Langevin dynamics is likely to converge faster. B. Details on the Sentence Encoder In our approach, we use the entity marker method proposed in Soares et al. (2019) to generate the encoding of each sentence with BERTBASE (Devlin et al., 2019). More speciﬁcally, recall that the goal of relation extraction is to predict the relation between two entities expressed in a sentence. Therefore, each sentence contains two entity mentions, i.e., token spans corresponding to an entity. For example, a sentence can be “ Washington is the capital of the United States .”, where Washington and United States are the entity mentions. During preprocessing, we follow Soares et al. (2019) and add two markers for each entity mention, including a starting marker before the entity mention and an ending marker after the entity mention. In this way, the example sentence becomes “[E1] Washington [/E1] is the capital of the [E2] United States [/E2] .”. Here, [E1] and [E2] are the starting markers. [/E1] and [/E2] are the ending markers. Then we apply BERTBASE to the preprocessed sentence, yielding an embedding vector for each token in the sentence. Finally, we follow Soares et al. (2019) to concatenate the embeddings of token [E1] and token [E2] as the sentence encoding. C. Comparison of Similarity Measures In our approach, given the encoding E(x) of a sentence x and relation prototype vectors vT, we predict the label y as below: p(y = r|x,vT) = exp(E(x) ·vr)∑ r′∈T exp(E(x) ·vr′),Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs where we compute the dot product between sentence encodings and relation prototype vectors, and treat the value as logits for classiﬁcation. Intuitively, the dot product could be understood as a similarity measure between encodings and prototype vectors. Besides dot product, Euclidean distance is another widely-used similarity measure, and we could naturally change the similarity measure in our approach to Euclidean distance as follows: p(y = r|x,vT) = exp(−1 2 ∥E(x) −vr)∥2) ∑ r′∈T exp(−1 2 ∥E(x) −vr′∥2) . In this section, we empirically compare the results of the two similarity measures, where the same conﬁguration of hyperparameters is used for both similarity measures. The results are presented in Tab. 7 and Tab. 8, where we can see that dot product works better in the 1-shot learning setting, whereas Euclidean distance achieves higher accuracy in the 5-shot learning setting. Therefore, when the number of support sentences for each relation is very limited (e.g., 1 or 2), it is better to use dot product. When we have more support sentences (e.g., 5 or more per relation), Euclidean distance is a better choice. Table 7.Results on FewRel test set. Similarity Measure5-Way 1-Shot 5-Way 5-Shot 10-Way 1-Shot 10-Way 5-Shot Dot Product 90.30 94.25 84.09 89.93 Euclidean Distance86.74 94.34 78.56 88.95 Table 8.Results on FewRel validation set. Similarity Measure5-Way 1-Shot 5-Way 5-Shot 10-Way 1-Shot 10-Way 5-Shot Dot Product 87.95 92.54 80.26 86.72 Euclidean Distance86.79 94.44 78.48 88.92",
      "references": [
        "Translating embeddings for modeling multi-relational data.",
        "Bert: Pre-training of deep bidirectional transformers for lan- guage understanding.",
        "Model-agnostic meta- learning for fast adaptation of deep networks.",
        "Bilevel programming for hyperparameter optimization and meta-learning.",
        "Learning discrete structures for graph neural networks.",
        "Fewrel 2.0: Towards more challenging few-shot relation classiﬁcation.",
        "Few-shot learning with graph neural networks.",
        "Neural message passing for quantum chem- istry.",
        "Meta- learning for stochastic gradient mcmc.",
        "Meta-learning probabilistic inference for prediction.",
        "Fewrel: A large-scale supervised few-shot relation classiﬁcation dataset with state-of-the-art evaluation.",
        "Bayesian model-agnostic meta-learning.",
        "Semi-supervised classiﬁcation with graph convolutional networks.",
        "Stein variational gradient descent: A general purpose bayesian inference algorithm.",
        "Distant supervision for relation extraction without labeled data.",
        "A simple neural attentive meta-learner.",
        "Weakly-supervised relation extraction by pattern-enhanced embedding learn- ing.",
        "Gmnn: Graph markov neural networks.",
        "Amortized bayesian meta-learning.",
        "Optimization as a model for few-shot learning.",
        "Improving hyper- nymy detection with an integrated path-based and distri- butional method.",
        "On a global upper bound for jensen’s inequality.",
        "Prototypical networks for few-shot learning.",
        "Matching the blanks: Distributional similarity for relation learning.",
        "Rotate: Knowl- edge graph embedding by relational rotation in complex space.",
        "Learning to compare: Relation net- work for few-shot learning.",
        "Meta-dataset: A dataset of datasets for learning to learn from few examples.",
        "Graph attention networks.",
        "Matching networks for one shot learning.",
        "Bayesian learning via stochastic gradient langevin dynamics.",
        "Relation classiﬁcation via convolutional deep neural network.",
        "Distant supervision for relation extraction via piecewise convolutional neural networks.",
        "Position-aware attention and supervised data im- prove slot ﬁlling.",
        "Integrating local context and global cohesiveness for open information extraction.",
        "Graphvite: A high- performance cpu-gpu hybrid system for node embedding."
      ],
      "meta_data": {
        "arxiv_id": "2007.02387v1",
        "authors": [
          "Meng Qu",
          "Tianyu Gao",
          "Louis-Pascal A. C. Xhonneux",
          "Jian Tang"
        ],
        "published_date": "2020-07-05T17:04:41Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses few-shot relation extraction by leveraging inter-relation structure via a global relation graph. Proposes REGRAB, a Bayesian meta-learning framework that (i) models relation prototypes as random variables and infers their posterior (capturing uncertainty), (ii) uses a GNN-parameterized graph-based Gaussian prior over relation prototypes to transfer knowledge across relations and enable zero-shot RE, and (iii) performs posterior sampling/optimization with stochastic gradient Langevin dynamics (SGLD), yielding consistent gains over strong baselines in few-shot and zero-shot settings.",
        "methodology": "Formulates meta-test prediction as marginalizing over relation prototype vectors v_T: p(y_Q|x_Q,x_S,y_S,G)=∫p(y_Q|x_Q,v_T)p(v_T|x_S,y_S,G)dv_T. Likelihood for support/query uses a BERT sentence encoder E(x) and softmax over dot products E(x)·v_r. Posterior factorization: p(v_T|x_S,y_S,G)∝p(y_S|x_S,v_T)p(v_T|G). Prior p(v_r|G)=N(h_r,I) where h_r is produced by a GNN (one-layer GCN) run on a global KNN relation graph built from pretrained KG relation embeddings (TransE). Approximates intractable marginalization via Monte Carlo with L prototype samples. Samples drawn with SGLD updates: v←v+(ε/2)∇_v log p(y_S|x_S,v)p(v|G)+√ε z, z~N(0,I); initialization v_r←m_r+h_r−m (support class mean + graph mean − global mean), with tunable weights and temperature scaling in softmax. End-to-end episodic meta-training over tasks (N-way K-shot).",
        "experimental_setup": "Benchmarks: FewRel (64 train/16 val/20 test relations; 700 sentences per relation; test via remote evaluation) and NYT-25 (25 relations from NYT; split into 10 train/5 val/10 test; 100 sentences per relation). Relation universe: 828 Wikidata relations. Global relation graph: compute 512-d relation embeddings via TransE (GraphVite) on Wikidata; build 10-nearest-neighbor graph; use embeddings as initial node features for GNN prior. Encoder: BERTBASE with entity marker scheme (concatenate [E1] and [E2] token embeddings) for all compared methods to ensure fairness. Tasks: 5-way/10-way with 1-shot/5-shot; report accuracy on FewRel test, FewRel val, NYT-25 test; additionally evaluate zero-shot by using only graph prior (no labeled support) across varying N-way. Baselines: Pair, MTB, Proto, MAML, GNN, SNAIL, Versa, BMAML. Training details: annealing temperature 10; SGLD L=10 samples, M=5 update steps, step size ε=0.1; SGD lr=0.1; hyperparameters selected via grid search on FewRel val; ablations on graph prior and on posterior inference (SGLD vs amortized Gaussian VI), plus sensitivity to #samples and #SGLD steps.",
        "limitations": "Assumes availability of a meaningful global relation graph and quality pretrained relation embeddings; graph is built heuristically via KNN over TransE embeddings, which may not reflect true semantic relatedness and can propagate embedding biases. Prior uses independent isotropic Gaussians N(h_r,I), limiting expressiveness (no learned covariance; ignores inter-prototype posterior correlations). SGLD introduces extra compute (multiple samples and inner-loop steps) and requires careful tuning of step size, burn-in/number of steps; posterior approximation quality depends on these choices. Episodic setup assumes closed-world N-way classification over a sampled relation subset; does not address 'no-relation' / open-set or multi-label relations explicitly. Relies on BERTBASE encoding with entity markers; performance may be sensitive to entity mention detection and domain shift. FewRel test set is not public, limiting reproducibility beyond reported numbers.",
        "future_research_directions": "Learn the relation-graph structure jointly/end-to-end (e.g., bilevel/discrete structure learning) rather than fixing a KNN graph from TransE; incorporate edge types/weights and uncertainty over graph. Use richer priors/posteriors: non-isotropic or full-covariance priors, hierarchical Bayesian models, or coupled posteriors capturing correlations between relation prototypes. Improve sampling/inference efficiency (preconditioned SGLD, SGHMC, amortized samplers) and calibrate uncertainty for downstream decision-making. Extend to open-world RE (include 'no relation'), multi-label relations, and continual/streaming few-shot settings with evolving relation sets. Explore alternative sources for relation graphs (text descriptions, ontology schemas) and robustness under noisy or incomplete graphs; apply framework to other modalities/tasks (few-shot image classification, cross-lingual RE).",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction",
      "full_text": "Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction Haoran Luo1, Haihong E1∗, Yuhao Yang2, Tianyu Yao1, Yikai Guo3, Zichen Tang1, Wentai Zhang1, Shiyao Peng1, Kaiyang Wan1, Meina Song1, Wei Lin4, Yifan Zhu1, Luu Anh Tuan5 1School of Computer Science, Beijing University of Posts and Telecommunications, China 2School of Automation Science and Electrical Engineering, Beihang University, China 3Beijing Institute of Computer Technology and Application 4Inspur Group Co., Ltd., China 5College of Computing and Data Science, Nanyang Technological University, Singapore {luohaoran, ehaihong, yifan_zhu}@bupt.edu.cn, anhtuan.luu@ntu.edu.sg Abstract Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two enti- ties, which are closer to real-world facts with broader applications. However, the construction of NKGs remains at a coarse-grained level, which is always in a single schema, ignoring the order and variable arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging and output merging to accomplish fine-grained n-ary relation extraction in different arity. Further- more, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. The experimental results demonstrate that Text2NKG achieves state-of-the-art performance inF1 scores on the fine-grained n-ary relation extraction benchmark. Our code and datasets are publicly available1. 1 Introduction Photoelectric Effect Nobel Prize in Physics Physics 1921 n-ary relational fact_3 n-ary relational fact_1 n-ary relational fact_2 Einstein ... Einstein received his Doctorate degree in Physics from theUniversity of Zurich. From 1909 to 1911, Einstein was Professor for Theoretical Physics at the University of Zurich. Einstein was awarded the NobelPrize in Physics in 1921 for his services to Theoretical Physics, and especially for his discovery of the law of the PhotoelectricEffect. ... University of Zurich 1909 1911 Theoretical Physics Doctorate Professor Natural Language TextN-ary relational Knowledge Graph fine-grained n-ary relation extraction Figure 1: An example of NKG construction. Modern knowledge graphs (KGs), such as Free- base [2], Google Knowledge Vault [7], and Wiki- data [21], utilize a multi-relational graph struc- ture to represent knowledge. Because of the advantage of intuitiveness and interpretability, KGs find various applications in question an- swering [28], query response [ 1], logical rea- soning [4], and recommendation systems [29]. Traditional KGs are mostly composed of binary relational facts ( subject, relation, object), which represent the relationship between two entities [3]. However, it has been observed [20] that over 30% of real-world facts involve n-ary relation facts with more than two entities (n ≥ 2). As shown in Figure 1, an n-ary relational knowl- edge graph (NKG) is composed of many n-ary relation facts, offering richer knowledge expression ∗ Corresponding author. 1 https://github.com/LHRLAB/Text2NKG 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2310.05185v3  [cs.AI]  30 Oct 2024academic_degree :  Doctorate academic_major :  Physics Einstein received his Doctorate degree in Physics from the University of Zurich. (Einstein, Doctorate, Physics, University of Zurich) Span-tuple for Entities:Natural Language Sentence: Answer Label-list  for Relations: hyper-relaional schema Einstein educated_atUniversity of Zurich event-based schema education_head    :  Einstein  education_tail      :  University of Zurich  academic_major   :  Physics  academic_degree :  Doctorate  role-based schema trigger   :  received  person           :  Einstein  college      :  University of Zurich  academic_major    :  Physics  academic_degree :  Doctorate  Event Type       :  education hypergraph-based schema Einstein  University of Zurich    Physics  Doctorate  education  [educated_at, academic_major, academic_degree] [education, trigger, person, college,   academic_major, academic_degree] [education_head, education_tail,   academic_major, academic_degree] [ education ]  NKG schemas: Figure 2: Taking a real-world textual fact as an example, we can extract a four-arity structured span-tuple for entities (Einstein, University of Zurich, Doctorate, Physics) with an answer label-list for relations accordingly as a 4-ary relational fact from the sentence through n-ary relation extraction. and wider application capabilities. As a key step of constructing NKGs, n-ary relation extraction (n-ary RE) is a task of identifying n-ary relations among entities in natural language texts. Compared to binary relational facts, n-ary relational facts in NKGs have more diverse schemas for different sce- narios. For example, Wikidata utilizes n-ary relational facts in a hyper-relational schema [20, 10, 23], i.e., (s, r, o,{(ki, vi)}n−2 i=1 ) which adds (n − 2) key-value pairs to the main triple to represent aux- iliary information. In addition to the hyper-relational schema, the existing NKG schemas also include event-based schema (r,{(ki, vi)}n i=1) [11, 16], role-based schema ({(ki, vi)}n i=1) [12, 15], and hypergraph-based schema (r,{vi}n i=1) [26, 8], as shown in Figure 2. Currently, most existing NKGs in four schemas, such as JF17K [26], Wikipeople [12], WD50K [10], and EventKG [11], are manually constructed. Previous n-ary RE methods [13, 31] focus on extraction with a fixed number of entities in hypergraph-based schema or role-based schema. Existing event extraction methods [16, 17, 9] can achieve n-ary RE in event-based schema. Recently, CubeRE [5] introduce a cube-filling method, which is the only n-ary RE method in hyper-relational schema. However, there are still three main challenges in automated n-ary RE for NKG construction, which remains at a coarse-grained level: (1) Diversity of NKG schemas. Previous methods could only perform N-ary RE based on a specific schema, but currently, there is no flexible method that can perform n-ary RE for arbitrary schema with different number of relations. (2) Determination of the order of entities. N-ary RE involves more possible entity orders than binary RE, for example, as shown in Figure 2, in a hyper-relational schema, there is an order issue regarding which entity is the head entity, tail entity, or auxiliary entity. Previous methods often ignored the joint impact of different entity orders, leading to inaccurate extraction.(3) Variability of the arity of n-ary RE. Previous methods usually output a fixed number of entities and are not adept at determining the variable number of entities forming an n-ary relational fact. To tackle these challenges, we introduce Text2NKG, a novel fine-grained n-ary RE framework designed to automate the generation of n-ary relational facts from natural language text for NKG construction. Text2NKG employs a span-tuple multi-label classification method, which transforms n-ary RE into a multi-label classification task for span-tuples, including all combinations of entities in the text. Because the number of predicted relation labels corresponds to the chosen NKG schema, Text2NKG is adaptable to all NKG schemas, offering examples with hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, all of which have broad applications. Moreover, Text2NKG introduces a hetero-ordered merging method, considering the probabilities of predicted labels for different entity orders to determine the final entity order. Finally, Text2NKG proposes an output merging method, which is used to unsupervisedly derive n-ary relational facts of any number of entities for NKG construction. In addition, we extend the only n-ary RE benchmark for NKG construction, HyperRED [5], which is in the hyper-relational schema, to four NKG schemas. We’ve done sufficient n-ary RE experiments on HyperRED, and the experimental results show that Text2NKG achieves state-of-the-art performance in F1 scores of hyper-relational extraction. We also compared the results of Text2NKG in the other three schemas to verify applications. 22 Related Work 2.1 N-ary relational Knowledge Graph An n-ary relational knowledge graph (NKG) consists of n-ary relational facts, which containn entities (n ≥ 2) and several relations. The n-ary relational facts are necessary and cannot be replaced by combinations of some binary relational facts because we cannot distinguish which binary relations are combined to represent the n-ary relational fact in the whole KG. Therefore, NKG utilizes a schema in every n-ary relational fact locally and a hypergraph representation globally [18]. Firstly, the simplest NKG schema is hypergraph-based. [26] found that over 30% of Freebase [2] entities participate facts with more than two entities, first defined n-ary relations mathematically and used star-to-clique conversion to convert triple-based facts representing n-ary relational facts into the first NKG dataset JF17K in hypergraph-based schema (r,{vi}n i=1). [8] proposed FB-AUTO and M-FB15K with the same hypergraph-based schema. Secondly, [12] introduced role information for n-ary relational facts and extracted Wikipeople, the first NKG dataset in role-based schema ({(ki, vi)}n i=1), composed of role-value pairs. Thirdly, Wikidata [21], the largest knowledge base, utilizes an NKG schema based on hyper-relation (s, r, o,{(ki, vi)}n−2 i=1 ), which adds auxiliary key- value pairs to the main triple. [10] first proposed an NKG dataset in hyper-relational schema WD50K. Fourthly, as [11] pointed out, events are also n-ary relational facts. One basic event representation has an event type, a trigger, and several key-value pairs [16]. Regarding the event type as the main relation, the (trigger: value) as one of the key-value pairs, and the arguments as the rest key-value pairs, we can obtain an event-based NKG schema (r,{(ki, vi)}n i=1). Based on four common NKG schemas, we propose Text2NKG, the first method for extraction of structured n-ary relational facts from natural language text, which improves NKG representation and application. 2.2 N-ary Relation Extraction Relation extraction (RE) is an important step of KG construction, directly affecting the quality, scale, and application of KGs. While most of the current n-ary relation extraction (n-ary RE) for NKG construction depends on manual construction [26, 12, 10] but not automated methods. Most automated RE methods target the extraction of traditional binary relational facts. For example, [22] proposes a table-filling method for binary RE, and [30, 27] propose span-based RE methods with levitated marker and packed levitated marker, respectively. For automated n-ary RE, some approaches [13, 31] treat n-ary RE in hypergraph-based schema or role-based schema as a binary classification problem and predict whether the composition of n-ary information in a document is valid or not. However, these methods extract n-ary information in fixed arity, which are not flexible. Moreover, some event extraction methods [16, 17, 9] propose different event trigger and argument extraction techniques, which can achieve n-ary RE inevent-based schema. Recently, CubeRE [5] proposes an automated n-ary RE method in hyper-relational schema, which extends the table-filling extraction method to n-ary RE with cube-filling. However, these methods can only model one of the useful NKG schemas with limited extraction accuracy. In this paper, we propose the first fine-grained n-ary RE framework Text2NKG for NKG construction in four example schemas, proposing a span-tuple multi-label classification method with hetero- ordered merging and output merging to improve the accuracy of fine-grained n-ary RE extraction in all NKG schemas substantially. 3 Preliminaries Formulation of NKG. An NKG G = {E, R, F} consists of an entity set E, a relation set R, and an n-ary fact (n≥2) set F. Each n-ary fact fn ∈ Fconsists of entities ∈ Eand relations ∈ R. For hyper-relational schema [20]: fn hr = (e1, r1, e2, {ri−1, ei}n i=3) where {ei}n i=1 ∈ E, {ri}n−1 i=1 ∈ R. For event-based schema [16]: fn ev = (r1, {ri+1, ei}n i=1), where {ei}n i=1 ∈ E, {ri}n+1 i=1 ∈ R. For role- based schema [12]: fn ro = ({ri, ei}n i=1), where {ei}n i=1 ∈ E, {ri}n i=1 ∈ R. For hypergraph-based schema [26]: fn hg = (r1, {ei}n i=1), where {ei}n i=1 ∈ E, r1 ∈ R. 3Einstein received his Doctorate degree in Physics from the University of Zurich. Natural Language Sentence: Einstein              received his  Doctorate  degree in  Physics  from the  University of Zurich   [S] [/S] 0 1 2 3 4 5 6 7 8 9 10 12 [L] [/L] [L] [/L] [L] [/L] 5 5 8 8 11 13 Entity_1 Entity_2 Bert-based Encoder Entity_3 Entity_4 Entity_1 Entity_2 Entity_3 Entity_1 Entity_2Entity_3 Entity_1 Entity_2 Entity_1 Entity_2 Entity_1 Entity_3 Entity_1 Entity_4 Entity_4 Entity_4 Entity_4 Entity_3 Multi-label  Classification academic_major educated_at  educated_at academic_major  educated_at academic_degree  academic_major educated_at  academic_major academic_degree  academic_degree academic_major  Hetero-ordered  Merging  Input ： Span-tuple for Entities ： Answer Label-list for Relations ： 3-ary relational facts: n-ary relational facts: academic_degree:  Doctorate academic_major    :  Physics Einstein educated_atUniversity of Zurich [Einstein, educated_at, University of Zurich,  academic_degree, Doctorate] [Einstein, educated_at, University of Zurich,  academic_major, Physics] Output Merging 11 13 Figure 3: An overview of Text2NKG extracting n-ary relation facts from a natural language sentence in hyper-relational NKG schema for an example. Problem Definition. Given an input sentence with l words s = {w1, w2, ..., wl}, an entity e is a consecutive span of words: e = {wp, wp+1, ..., wq} ∈ Es, where p, q∈ {1, ..., l}, and Es = {ej}m j=1 is the entity set of all m entities in the sentence. The output of n-ary relation extraction, R(), is a set of n-ary relational facts Fs in given NKG schema in {fn hr, fn ev, fn ro, fn hg}. Specifically, each n-ary relational fact fn ∈ Fs is extracted by multi-label classification of one of the ordered span-tuple for n entities [ei]n i=1 ∈ Es, forming an answer label-list for nr relations [ri]nr i=1 ∈ R, where n is the arity of the extracted n-ary relational fact, and nr is the number of answer relations in the fact, which is determined by the given NKG schema: R([ei]n i=1) = [ri]n−1 i=1 , when fn = fn hr, R([ei]n i=1) = [ri]n+1 i=1 when fn = fn ev, R([ei]n i=1) = [ri]n i=1 when fn = fn ro, and R([ei]n i=1) = [r1] when fn = fn hg. 4 Methodology In this section, we first introduce the overview of the Text2NKG framework, followed by the span- tuple multi-label classification, training strategy, hetero-ordered merging, and output merging. 4.1 Overview of Text2NKG Text2NKG is a fine-grained n-ary relation extraction framework built for n-ary relational knowledge graph (NKG) construction. The input to Text2NKG is natural language text tokens labeled with entity span in sentence units. First, inspired by [ 27], Text2NKG encodes the entities using BERT-based Encoder [6] with a packaged levitated marker for embedding. Then each arrangement of ordered span-tuple with three entity embeddings will be classified with multiple labels, and the framework will be learned by the weighted cross-entropy with a null-label bias. In the decoding stage, in order to filter the n-ary relational facts whose entity compositions have isomorphic hetero-ordered characteristics, Text2NKG proposes a hetero-ordered merging strategy to merge the label probabilities of 3! = 6arrangement cases of span-tuples composed of the same entities and filter out the output 3-ary relational facts existing non-conforming relations. Finally, Text2NKG combines the output 3-ary relational facts to form the final n-ary relational facts with output merging. 4.2 Span-tuple Multi-label Classification For the given sentence token s = {w1, w2, ..., wl} and the set of entities Es, in order to perform fine- grained n-ary RE, we need first to encode a span-tuple (e1, e2, e3) consisting of every arrangement of three ordered entities, where e1, e2, e3 ∈ Es. Due to the high time complexity of training every 4span-tuple as one training item, inspired by [27], we achieve the reduction of training items by using packed levitated markers that pack one training item with each entity in Es separately. Specifically, in each packed training item, a pair of solid tokens, [S] and [/S], are added before and after the packed entity eS = {wpS , ..., wqS }, and ( |Es| −1) pairs of levitated markers, [L] and [/L], according to other entities in Es, are added with the same position embeddings as the beginning and end of their corresponding entities span eLi = {wpLi , ..., wqLi } to form the input token X: X ={w1, ...,[S], wpS , ..., wqS , [/S], ..., wpLi ∪ [L], ..., wqLi ∪ [/L], ..., wl}. (1) We encode such token by the BERT-based pre-trained model encoder [6]: {h1, h2, ..., ht} = BERT(X), (2) where t = |X| is the input token length, {hi}t i=1 ∈ Rd, and d is embedding size. There are several span-tuples (A, B, C) in a training item. The embedding of first entity hA ∈ R2d in the span-tuple is obtained by concat embedding of the solid markers, [S] and [/S], and the embeddings of second and third entities hB, hC ∈ R2d are obtained by concat embeddings of levitated markers, [L] and [/L] with all A2 m−1 arrangement of any other two entities in Es. Thus, we obtain the embedding representation of the three entities to form A2 m−1 span-tuples in one training item. Therefore, every input sentence contains m training items with mA2 m−1 = A3 m span-tuples for any ordered arrangement of three entities. We then define nr linear classifiers, each of which consists of 3 feedforward neural networks {FNNk i }nr i=1, k= 1, 2, 3, to classify the span-tuples for multiple-label classification. Each classifier targets the prediction of one relation ri, thus obtaining a probability lists (Pi)nr i=1 with all relations in given relation set R plus a null-label: Pi = FNN1 i (hA) +FNN2 i (hB) +FNN3 i (hC), (3) where FNNk i ∈ R2d×(|R|+1), and Pi ∈ R(|R|+1). 4.3 Training Strategy To train thenr classifiers for each relation prediction more accurately, we propose a data augmentation strategy for span-tuples. Taking the hyper-relational schema as an example, given a hyper-relational fact (A, r1, B, r2, C), we consider swapping the head and tail entities, and changing the main relation to its inverse (B, r−1 1 , A, r2, C), as well as swapping the tail entities with auxiliary values, and the main relation with the auxiliary key (A, r2, C, r1, B), also as labeled training span-tuple cases. Thus Rhr(A, B, C) = (r1, r2) can be augmented with 3! = 6orders of span-tuples:    Rhr(A, B, C) = (r1, r2), Rhr(B, A, C) = (r−1 1 , r2), Rhr(A, C, B) = (r2, r1), Rhr(B, C, A) = (r2, r−1 1 ), Rhr(C, A, B) = (r−1 2 , r1), Rhr(C, B, A) = (r1, r−1 2 ). (4) For other schemas, we can also obtain 6 fully-arranged cases of labeled span-tuples in a similar way, as described in Appendix A. If no n-ary relational fact exists between the three entities of span-tuples, then relation labels are set as null-label. Since most cases of span-tuple are null-label, we set a weight hyperparameter α ∈ (0, 1] between the null-label and other labels to balance the learning of the null-label. We jointly trained the nr classifiers for each relations by cross-entropy loss L with a null-label weight bias Wα: L = − nrX i=1 Wα log   exp (Pi[ri]) P|R|+1 j=1 exp (Pij) ! , (5) where Wα = [α, 1.0, 1.0, ...1.0] ∈ R(|R|+1). 5Dataset #Ent #R_hr #R_ev #R_ro #R_hg All Train Dev Test #Sentence #Fact #Sentence #Fact #Sentence #Fact #Sentence #Fact HyperRED 40,293 106 232 168 62 44,840 45,994 39,840 39,978 1,000 1,220 4,000 4,796 Table 1: Dataset statistics, where the columns indicate the number of entities, relations with four schema, sentences and n-ary relational facts in all sets, train set, dev set, and test set, respectively. 4.4 Hetero-ordered Merging In the decoding stage, since Text2NKG labels all 6 different arrangement of the same entity com- position, we design a hetero-ordered merging strategy to merge the corresponding labels of these 6 hetero-ordered span-tuples into one to generate non-repetitive n-ary relational facts unsupervisedly. For hyper-relational schema (nr = 2), we combine the predicted probabilities of two labels P1, P2 in 6 orders to (A, B, C) order as follows:    P1 = P(ABC) 1 + I(P(BAC) 1 ) +P(ACB) 2 + I(P(BCA) 2 ) +P(CAB) 2 + P(CBA) 1 , P2 = P(ABC) 2 + P(BAC) 2 + P(ACB) 1 + P(BCA) 1 + I(P(CAB) 1 ) +I(P(CBA) 2 ), (6) where I() is a function for swapping the predicted probability of relations and the corresponding inverse relations. Then, we take the maximum probability to obtain labels r1, r2, forming a 3-ary relational fact (A, r1, B, r2, C) and filter it out if there are null-label in (r1, r2). If there are inverse relation labels in (r1, r2), we can also transform the order of entities and relations as equation 4. For event-based schema, role-based schema, and hypergraph-based schema, all can be generated by hetero-ordered merging according to this idea, as shown in Appendix B. 4.5 Output Merging After hetero-ordered merging, we merge the output 3-ary relational facts to form higher-arity facts, with hyper-relational schema based on the same main triple, event-based schema based on the same main relation (event type), role-based schema based on the same key-value pairs, and hypergraph- based schema based on the same hyperedge relation. This way, we can unsupervisedly obtain n-ary relational facts with dynamic number of arity numbers for NKG construction. More details are discussed in Appendix G.2 and Appendix G.3. 5 Experiments This section presents the experimental setup, results, and analysis. We answer the following research questions (RQs): RQ1: Does Text2NKG outperform other n-ary RE methods? RQ2: Whether Text2NKG can cover NKG construction for various schemas? RQ3: Does the main components of Text2NKG work? RQ4: How does the null-label bias hyperparameter in Text2NKG affect performance? RQ5: Can Text2NKG get complete n-ary relational facts in different arity? RQ6: How is Text2NKG’s computational efficiency?RQ7: How does Text2NKG perform in specific case study? RQ8: What is the future development of Text2NKG in the era of large language models? 5.1 Experimental Setup Datasets. The existing fine-grained n-ary RE dataset is HyperRED [5] only in hyper-relational schema with annotated extracted entities. Therefore, we expand the HyperRED dataset to four schemas as standard fine-grained n-ary RE benchmarks and conduct experiments on them. The statistics of the HyperRED with four schemas are shown in Table 1 and the construction detail is in Appendix C. Baselines. We compare Text2NKG againstGenerative Baseline [14], Pipeline Baseline [24], and CubeRE [5] in fine-grained n-ary RE task ofhyper-relational schema. For n-ary RE in the other three schemas, we compared Text2NKG with event extraction models such as Text2Event [16], UIE [17], 6Model PLM hyper-relational schema/ Dev hyper-relational schema/ Test Precision Recall F1 Precision Recall F1 Unsupervised Method ChatGPT gpt-3.5-turbo 12.0583 11.2764 11.6542 11.4021 10.9134 11.1524 GPT-4 gpt-4 15.7324 15.2377 15.4811 15.8187 15.4824 15.6487 Supervised Method Generative Baseline BERT-base (110M) 63.79 ± 0.27 59.94 ± 0.68 61.80 ± 0.37 64.60 ± 0.47 59.67 ± 0.35 62.03 ± 0.21 Pipelinge Baseline 69.23 ± 0.30 58.21 ± 0.57 63.24 ± 0.44 69.00 ± 0.48 57.55 ± 0.19 62.75 ± 0.29 CubeRE 66.14 ± 0.88 64.39 ± 1.23 65.23 ± 0.82 65.82 ± 0.84 64.28 ± 0.25 65.04 ± 0.29 Text2NKG w/o DA 76.02 ± 0.50 72.28 ± 0.68 74.10 ± 0.55 73.55 ± 0.81 70.63 ± 1.40 72.06 ± 0.34 Text2NKG w/o α 88.77 ± 0.85 78.39 ± 0.47 83.26 ± 0.70 88.09 ± 0.69 76.64 ± 0.45 81.97 ± 0.58 Text2NKG w/o HM 61.74 ± 0.34 76.97 ± 0.44 68.52 ± 0.69 61.07 ± 0.73 76.16 ± 0.59 67.72 ± 0.48 Text2NKG (ours) 91.26 ± 0.69 79.36 ± 0.51 84.89 ± 0.44 90.77 ± 0.60 77.53 ± 0.32 83.63 ± 0.63 Generative Baseline BERT-large (340M) 67.08 ± 0.49 65.73 ± 0.78 66.40 ± 0.47 67.17 ± 0.40 64.56 ± 0.58 65.84 ± 0.25 Pipelinge Baseline 70.58 ± 0.78 66.58 ± 0.66 68.52 ± 0.32 69.21 ± 0.55 64.27 ± 0.24 66.65 ± 0.28 CubeRE 68.75 ± 0.82 68.88 ± 1.03 68.81 ± 0.46 66.39 ± 0.96 67.12 ± 0.69 66.75 ± 0.28 Text2NKG (ours) 91.90 ± 0.79 79.43 ± 0.42 85.21 ± 0.69 91.06 ± 0.81 77.64 ± 0.46 83.81 ± 0.54 Table 2: Comparison of Text2NKG with other baselines in the hyper-relational extraction on HyperRED. Results of the supervised baseline models are mainly taken from the original paper [5]. The best results in each metric are in bold. Model PLM event-based schema role-based schema hypergraph-based schema Precision Recall F1 Precision Recall F1 Precision Recall F1 Unsupervised Method ChatGPT gpt-3.5-turbo 10.4678 11.1628 10.8041 11.4387 10.4203 10.9058 11.2998 11.7852 11.5373 GPT-4 gpt-4 13.3681 14.6701 13.9888 13.6397 12.5355 13.0643 13.0907 13.6701 13.3741 Supervised Method Text2Event T5-base (220M) 73.94 ± 0.76 70.56 ± 0.58 72.21 ± 1.25 72.73 ± 0.79 68.45 ± 1.34 70.52 ± 0.62 73.68 ± 0.88 70.37 ± 0.51 71.98 ± 0.92 UIE 76.51 ± 0.28 73.02 ± 0.66 74.72 ± 0.18 72.17 ± 0.29 69.84 ± 0.11 70.98 ± 0.31 72.03 ± 0.41 68.74 ± 0.13 70.34 ± 1.07 LasUIE 79.62 ± 0.27 78.04 ± 0.75 78.82 ± 0.26 77.01 ± 0.20 74.26 ± 0.25 75.61 ± 0.24 76.21 ± 0.07 73.75 ± 0.17 74.96 ± 0.42 Text2NKG BERT-base (110M) 86.20 ± 0.57 79.25 ± 0.33 82.58 ± 0.20 86.72 ± 0.80 78.94 ± 0.59 82.64 ± 0.38 83.53 ± 1.18 86.59 ± 0.38 85.03 ± 0.86 Text2Event T5-large (770M) 75.58 ± 0.53 72.39 ± 0.82 73.97 ± 1.19 73.21 ± 0.45 70.85 ± 0.67 72.01 ± 0.31 75.28 ± 0.93 72.73 ± 1.07 73.98 ± 0.49 UIE 79.38 ± 0.28 74.69 ± 0.61 76.96 ± 0.95 74.47 ± 1.42 71.84 ± 0.77 73.14 ± 0.38 74.57 ± 0.64 71.93 ± 0.86 73.22 ± 0.19 LasUIE 81.29 ± 0.83 79.54 ± 0.26 80.40 ± 0.65 79.37 ± 0.92 76.63 ± 0.44 77.97 ± 0.76 77.49 ± 0.35 74.96 ± 0.60 76.20 ± 0.87 Text2NKG BERT-large (340M) 88.47 ± 0.95 80.30 ± 0.75 84.19 ± 1.29 86.87 ± 0.87 80.86 ± 0.29 83.76 ± 1.17 85.06 ± 0.33 86.72 ± 0.36 85.89 ± 0.69 Table 3: Comparison of Text2NKG with other baselines in the n-ary RE in event-based, role-based, and hypergraph-based schemas on HyperRED. The best results in each metric are in bold. and LasUIE [9]. Furthermore, we utilized different prompts to test the currently most advanced large-scale pre-trained language models ChatGPT [25] and GPT-4 [19] in an unsupervised manner, specifically for the extraction performance across the four schemas. The detailed baseline settings can be found in Appendix D. Ablations. To evaluate the significance of Text2NKG’s three main components, data augmentation (DA), null-label weight hyperparameter ( α), and hetero-ordered merging (HM), we obtain three simplified model variants by removing any one component (Text2NKG w/o DA, Text2NKG w/o α, and Text2NKG w/o HM) for comparison. Evaluation Metrics. We use the F1 score with precision and recall to evaluate the dev set and the test set. For a predicted n-ary relational fact to be considered correct, the entire fact must match the ground facts completely. Hyperparameters and Enviroment. We train 10 epochs on HyperRED using the Adam optimizer. All experiments were done on a single NVIDIA A100 GPU, and all experimental results were derived by averaging 5 random seed experiments. Appendix E shows Text2NKG’s optimal hyperparameter settings. Appendix F shows training details. 5.2 Main Results (RQ1) The experimental results of proposed Text2NKG and other baselines with both BERT-base and BERT-large encoders can be found in Table 2 for the fine-grained n-ary RE in hyper-relational schema. We can observe that Text2NKG shows a significant improvement over the existing optimal model CubeRE on both the dev and test datasets of HyperRED. The F1 score is improved by 19.66 percentage points in the dev set and 18.60 percentage points in the test set with the same BERT-base encoder, and 16.40 percentage points in the dev set and 17.06 percentage points in the test set with 7/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000033/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000048/uni00000046/uni00000044/uni0000004f/uni0000004f /uni00000029/uni00000014 (a) /uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b /uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000013 /uni00000014/uni00000017/uni00000013/uni00000013 /uni00000014/uni00000019/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055 /uni00000051/uni00000058/uni00000050/uni00000042/uni00000049/uni00000044/uni00000046/uni00000057 /uni00000051/uni00000058/uni00000050/uni00000042/uni00000053/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057 /uni00000051/uni00000058/uni00000050/uni00000042/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni00000046/uni00000057 (b) /uni00000014/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000033/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000048/uni00000046/uni00000044/uni0000004f/uni0000004f /uni00000029/uni00000014 (c) Figure 4: (a) Precision, Recall, and F1 changes in the dev set during the training of Text2NKG. (b) The changes of the number of true facts, the number of predicted facts, and the number of predicted accurate facts during the training of Text2NKG. (c) Precision, Recall, and F1 results on different null-label hyperparameter (α) settings. the same BERT-large encoder, reflecting Text2NKG’s excellent performance. Figure 4(a) and 4(b) intuitively show the changes of evaluation metrics and answers of facts in the dev set during the training of Text2NKG. It is worth noting that Text2NKG exceeds 90% in precision accuracy, which proves that the model can obtain very accurate n-ary relational facts and provides a good guarantee for the quality of fine-grained NKG construction. 5.3 Results on Various NKG Schemas (RQ2) As shown in Table 3, besides hyper-relational schema, Text2NKG also accomplishes the tasks of fine-grained n-ary RE in three other different NKG schemas on HyperRED, which demonstrates good utility. In the added tasks of n-ary RE for event-based, role-based, and hypergraph-based schemas, since no model has done similar experiments at present, we used event extraction or unified extraction methods such as Text2Event [16], UIE [17], and LasUIE [9] for comparison. Text2NKG still works best in these schemas, which demonstrates good versatility. 5.4 Ablation Study (RQ3) Data augmentation (DA), null-label weight hyperparameter (α), and hetero-ordered merging (HM) are the three main components of Text2NKG. For the different Text2NKG variants as shown in Table 2, DA, α, and HM all contribute to the accurate results of our complete model. By comparing the differences, we find that HM is most effective by combining the probabilities of labels of different orders, followed by DA and α. 5.5 Analysis of Null-label Weight Hyperparameters (RQ4) We compared the effect for different null-label weight hyperparameters (α). As shown in Figure 4(c), the larger the α, the greater the learning weight of null-label compared with other lables, the more relations are predicted as null-label. After filtering out the facts having null-label, fewer facts are extracted, so the precision is generally higher, and the recall is generally lower. The smaller theα, the more relations are predicted as non-null labels, thus extracting more n-ary relation facts, so the recall is generally higher, and the precision is generally lower. Comparing the results ofF1 values for different α, it is found that α = 0.01 works best, which can be adjusted in practice according to specific needs to obtain the best results. 5.6 Analysis of N-ary Relation Extraction in Different Arity (RQ5) Figure 5(a) shows the number of n-ary relational facts extracted after output merging and the number of the answer facts in different arity during training of Text2NKG on the dev set. We find that, as the training proceeds, the final output of Text2NKG converges to the correct answer in terms of the number of complete n-ary relational facts in each arity, achieving implementation of n-ary RE in indefinite arity unsupervised, with good scalability. 8/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000016 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055 /uni00000053/uni00000055/uni00000048/uni00000047/uni00000042/uni00000016 /uni00000044/uni00000051/uni00000056/uni00000042/uni00000016 /uni00000053/uni00000055/uni00000048/uni00000047/uni00000042/uni00000017 /uni00000044/uni00000051/uni00000056/uni00000042/uni00000017 /uni00000053/uni00000055/uni00000048/uni00000047/uni00000042/uni00000018 /uni00000044/uni00000051/uni00000056/uni00000042/uni00000018 /uni00000053/uni00000055/uni00000048/uni00000047/uni00000042/uni00000019 /uni00000044/uni00000051/uni00000056/uni00000042/uni00000019 /uni00000053/uni00000055/uni00000048/uni00000047/uni00000042/uni0000001a /uni00000053/uni00000055/uni00000048/uni00000047/uni00000042/uni0000001b /uni00000044/uni00000051/uni00000056/uni00000042/uni0000001b /uni00000053/uni00000055/uni00000048/uni00000047/uni00000042/uni0000001c /uni00000053/uni00000055/uni00000048/uni00000047/uni00000042/uni00000014/uni00000013 (a) He was born in Skirpenbeck, near York and attended Pocklington School from 1936 to 1943. [He, educated at, Pocklington School, {start time: 1936, end time: 1943}] [educated at, {educated at_h: He, educated at_t: Pocklington School, end time: 1943, start time: 1936}] [{educated at_h: He, educated at_t: Pocklington School,  start time: 1936, end time: 1943}] [educated at, {He, Pocklington School, 1936, 1943}] T ext2NKG  hyper-relational schema role-based schema event-based schema hypergraph-based schema (b) Figure 5: (a) The changes of the number of extracted n-ary RE in different arity, where \"pred_n\" represents the number of extracted n-ary facts with different arities by Text2NKG, and \"ans_n\" represents the ground truth. (b) Case study of Text2NKG’s n-ary relation extraction in four schemas on HyperRED. 5.7 Computational Efficiency (RQ6) As mentioned in Section 4.2, the main computational consumption of Text2NKG is selecting every span-tuple of three ordered entities to encode them and get the classified labels in multiple-label classification part. If we adopt an traversal approach with each span-tuple in one training items, the time complexity will be O(m3). To reduce the high time complexity of training every span-tuple as one training item, Text2NKG uses packed levitated markers that pack one training item with each entity in Es separately. We obtain the embedding representation of the three entities to form A2 m−1 span-tuples in one training item. Every input sentence contains m training items with mA2 m−1 = A3 m span-tuples for any ordered arrangement of three entities for multiple-label classification. Therefore, the time complexity decreased from O(m3) to O(m). 5.8 Case Study (RQ7) Figure 5(b) shows a case study of n-ary RE by a trained Text2NKG. For a sentence, \" He was born in Skirpenbeck, near York and attended Pocklin.\", four structured n- ary RE can be obtained by Text2NKG according to the requirements. Taking the hyper-relational schema for an example, Text2NKG can successfully extract one n-ary relational fact consisting of a main triple [ He, educated at, Pocklington], and two auxiliary key-value pairs {start time:1936}, {end time:1943}. This intuitively validates the practical performance of Text2NKG on fine-grained n-ary RE to better contribute to NKG construction. 5.9 Comparison with ChatGPT (RQ8) As shown in Table 2 and Table 3, we compared the extraction effects under four NKG schemas of the supervised Text2NKG with the unsupervised ChatGPT and GPT-4. We found that these large language models cannot accurately distinguish the closely related relations in the fine-grained NKG relation repository, resulting in their F1 scores ranging around 10%-15%, which is much lower than the performance of Text2NKG. On the other hand, the limitation of Text2NKG is that its performance is confined within the realm of supervised training. Therefore, in future improvements and practical applications, we suggest combining small supervised models with large unsupervised models to balance solving the cold-start and fine-grained extraction, which is detailed in Appendix G.1. 6 Conclusion In this paper, we introduce Text2NKG, a novel framework designed for fine-grained n-ary relation extraction (RE) aimed at constructing N-ary Knowledge Graphs (NKGs). Our extensive experiments demonstrate that Text2NKG outperforms all existing baseline models across a wide range of fine- grained n-ary RE tasks. Notably, it excels in four distinct schema types: hyper-relational, event- based, role-based, and hypergraph-based. Furthermore, we have extended the HyperRED dataset, transforming it into a comprehensive fine-grained n-ary RE benchmark that supports all four schemas. 9Acknowledgments This work is supported by the National Science Foundation of China (Grant No. 62176026, Grant No. 62406036, Grant No. 62473271, and Grant No. 62076035). This work is also supported by the SMP-Zhipu.AI Large Model Cross-Disciplinary Fund, the BUPT Excellent Ph.D. Students Foundation (No. CX2023133), the BUPT Innovation and Entrepreneurship Support Program (No. 2024-YC-A091 and No. 2024-YC-T022), and the Engineering Research Center of Information Networks, Ministry of Education. References [1] Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. Complex query answering with neural link predictors. In International Conference on Learning Representations, 2021. [2] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08, page 1247–1250, New York, NY , USA, 2008. Association for Computing Machinery. [3] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. [4] Xuelu Chen, Ziniu Hu, and Yizhou Sun. Fuzzy logic based logical query answering on knowledge graphs. Proceedings of the AAAI Conference on Artificial Intelligence, 36(4):3939– 3948, Jun. 2022. [5] Yew Ken Chia, Lidong Bing, Sharifah Mahani Aljunied, Luo Si, and Soujanya Poria. A dataset for hyper-relational extraction and a cube-filling approach. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10114–10133, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [7] Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. Knowledge vault: A web-scale approach to proba- bilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, page 601–610, New York, NY , USA, 2014. Association for Computing Machinery. [8] Bahare Fatemi, Perouz Taslakian, David Vazquez, and David Poole. Knowledge hypergraphs: Prediction beyond binary relations. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI’20, 2021. [9] Hao Fei, Shengqiong Wu, Jingye Li, Bobo Li, Fei Li, Libo Qin, Meishan Zhang, Min Zhang, and Tat-Seng Chua. Lasuie: Unifying information extraction with latent adaptive structure-aware generative language model. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 15460–15475. Curran Associates, Inc., 2022. [10] Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens Lehmann. Message passing for hyper-relational knowledge graphs. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7346–7359, Online, November 2020. Association for Computational Linguistics. 10[11] Saiping Guan, Xueqi Cheng, Long Bai, Fujun Zhang, Zixuan Li, Yutao Zeng, Xiaolong Jin, and Jiafeng Guo. What is event knowledge graph: A survey. IEEE Transactions on Knowledge and Data Engineering, pages 1–20, 2022. [12] Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. Link prediction on n-ary relational data. In The World Wide Web Conference, WWW ’19, page 583–593, New York, NY , USA, 2019. Association for Computing Machinery. [13] Robin Jia, Cliff Wong, and Hoifung Poon. Document-level n-ary relation extraction with multiscale representation learning. InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3693–3704, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [14] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online, July 2020. Association for Computational Linguistics. [15] Yu Liu, Quanming Yao, and Yong Li. Role-aware modeling for n-ary relational knowledge bases. In Proceedings of the Web Conference 2021, WWW ’21, page 2660–2671, New York, NY , USA, 2021. Association for Computing Machinery. [16] Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, and Shaoyi Chen. Text2Event: Controllable sequence-to-structure generation for end-to- end event extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2795–2806, Online, August 2021. Association for Computational Linguistics. [17] Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. Unified structure generation for universal information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 5755–5772, Dublin, Ireland, May 2022. Association for Computational Linguistics. [18] Haoran Luo, Haihong E, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, and Wei Lin. HAHE: Hierarchical attention for hyper-relational knowledge graphs in global and local level. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 8095–8107, Toronto, Canada, July 2023. Association for Computational Linguistics. [19] OpenAI. Gpt-4 technical report, 2023. [20] Paolo Rosso, Dingqi Yang, and Philippe Cudré-Mauroux. Beyond triplets: Hyper-relational knowledge graph embedding for link prediction. In Proceedings of The Web Conference 2020, WWW ’20, page 1885–1896, New York, NY , USA, 2020. Association for Computing Machinery. [21] Denny Vrande ˇci´c and Markus Krötzsch. Wikidata: A free collaborative knowledgebase. Commun. ACM, 57(10):78–85, sep 2014. [22] Jue Wang and Wei Lu. Two are better than one: Joint entity and relation extraction with table- sequence encoders. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1706–1721, Online, November 2020. Association for Computational Linguistics. [23] Quan Wang, Haifeng Wang, Yajuan Lyu, and Yong Zhu. Link prediction on n-ary relational facts: A graph-based approach. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 396–407, Online, August 2021. Association for Computational Linguistics. 11[24] Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou, Lei Li, and Junchi Yan. UniRE: A unified label space for entity relation extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 220–231, Online, August 2021. Association for Computational Linguistics. [25] Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han. Zero-shot information extraction via chatting with chatgpt, 2023. [26] Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. On the representation and embedding of knowledge bases beyond binary relations. In Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 1300–1307. IJCAI/AAAI Press, 2016. [27] Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. Packed levitated marker for entity and relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4904–4917, Dublin, Ireland, May 2022. Association for Computational Linguistics. [28] Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP . ACL - Association for Computational Linguistics, July 2015. Outstanding Paper Award. [29] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, page 353–362, New York, NY , USA, 2016. Association for Computing Machinery. [30] Zexuan Zhong and Danqi Chen. A frustratingly easy approach for entity and relation extraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 50–61, Online, June 2021. Association for Computational Linguistics. [31] Yuchen Zhuang, Yinghao Li, Junyang Zhang, Yue Yu, Yingjun Mou, Xiang Chen, Le Song, and Chao Zhang. ReSel: N-ary relation extraction from scientific text and tables by learning to retrieve and select. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 730–744, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. 12Appendix A Supplement to Data Augmentation In addition to the hyper-relational schema, the data augmentation strategies for other schemas are as follows: For event-based schema, given an event-based fact (r1, r2, A, r3, B, r4, C), we consider keeping the main relation r1 unchanged, and swapping other key-value pairs, { r2, A}, {r3, B}, and { r4, C}, positionally, also as labeled training span-tuple cases. Thus Rev(A, B, C) = (r1, r2, r3, r4) can be augmented with 6 orders of span-tuples:    Rev(A, B, C) = (r1, r2, r3, r4), Rev(B, A, C) = (r1, r3, r2, r4), Rev(A, C, B) = (r1, r2, r4, r3), Rev(B, C, A) = (r1, r3, r4, r2), Rev(C, A, B) = (r1, r4, r2, r3), Rev(C, B, A) = (r1, r4, r3, r2). (7) For role-based schema, given a role-based fact (r1, A, r2, B, r3, C), we consider swapping key-value pairs, {r1, A}, {r2, B}, and {r3, C}, positionally, also as labeled training span-tuple cases. Thus Rro(A, B, C) = (r1, r2, r3) can be augmented with 6 orders of span-tuples:    Rro(A, B, C) = (r1, r2, r3), Rro(B, A, C) = (r2, r1, r3), Rro(A, C, B) = (r1, r3, r2), Rro(B, C, A) = (r2, r3, r1), Rro(C, A, B) = (r3, r1, r2), Rro(C, B, A) = (r3, r2, r1). (8) For hypergraph-based schema, given a hypergraph-based fact (r1, A, B, C), we consider keeping the main relation r1 unchanged, and swapping entities, A, B, and C, positionally, also as labeled training span-tuple cases. Thus Rhg(A, B, C) = (r1) can be augmented with 6 orders of span-tuples:    Rhg(A, B, C) = (r1), Rhg(B, A, C) = (r1), Rhg(A, C, B) = (r1), Rhg(B, C, A) = (r1), Rhg(C, A, B) = (r1), Rhg(C, B, A) = (r1). (9) B Supplement to Hetero-ordered Merging In addition to the hyper-relational schema, the hetero-ordered merging strategies for other schemas are as follows: 13For event-based schema (nr = 4 ), we combine the predicted probabilities of four labels P1, P2, P3, P4 in 6 orders to (A, B, C) order as follows:    P1 = P(ABC) 1 + P(BAC) 1 + P(ACB) 1 + P(BCA) 1 + P(CAB) 1 + P(CBA) 1 , P2 = P(ABC) 2 + P(BAC) 3 + P(ACB) 2 + P(BCA) 4 + P(CAB) 3 + P(CBA) 4 , P3 = P(ABC) 3 + P(BAC) 2 + P(ACB) 4 + P(BCA) 2 + P(CAB) 4 + P(CBA) 3 , P4 = P(ABC) 4 + P(BAC) 4 + P(ACB) 3 + P(BCA) 3 + P(CAB) 2 + P(CBA) 2 . (10) Then, we take the maximum probability to obtain labels r1, r2, r3, r4, forming a 3-ary relational fact (r1, r2, A, r3, B, r4, C) and filter it out if there are null-label in (r1, r2, r3, r4). For role-based schema (nr = 3), we combine the predicted probabilities of three labels P1, P2, P3 in 6 orders to (A, B, C) order as follows:    P1 = P(ABC) 1 + P(BAC) 2 + P(ACB) 1 + P(BCA) 3 + P(CAB) 2 + P(CBA) 3 , P2 = P(ABC) 2 + P(BAC) 1 + P(ACB) 3 + P(BCA) 1 + P(CAB) 3 + P(CBA) 2 , P3 = P(ABC) 3 + P(BAC) 3 + P(ACB) 2 + P(BCA) 2 + P(CAB) 1 + P(CBA) 1 . (11) Then, we take the maximum probability to obtain labels r1, r2, r3, forming a 3-ary relational fact (r1, A, r2, B, r3, C) and filter it out if there are null-label in (r1, r2, r3). For hypergraph-based schema (nr = 1), we combine the predicted probabilities of one label P1 in 6 orders to (A, B, C) order as follows: ( P1 = P(ABC) 1 + P(BAC) 1 + P(ACB) 1 + P(BCA) 1 + P(CAB) 1 + P(CBA) 1 . (12) Then, we take the maximum probability to obtain labels r1, forming a 3-ary relational fact (r1, A, B, C) and filter it out if r1 is null-label. C Construction of Dataset Based on the original hyper-relational schema on HyperRED dataset [5], we construct other three schemas (event-based, role-based, and hypergraph-based) for fine-grained n-ary RE. Firstly, we view the main relation in the hyper-relational schema as the event type in the event-based schema, combine the head entity and tail entity with two extra head key and tail key to convert them into two key-value pairs, and remain the auxiliary key-value pairs in the hyper-relational schema. Taking ‘Einstein received his Doctorate degree in Physics from the University of Zurich.’ as an example, it can be represented as ( Einstein, educated, University of Zurich, {academic_major, Physics}, {academic_ degree, Doctorate}) in the hyper-relational schema and (education, {trigger, received}, {person, Einstein}, {college, University of Zurich}, {academic_major, Physics},{academic_degree, Doctorate}) in the event-based schema. Secondly, we remove the event type in the event-based schema to obtain the role-based schema. Thirdly, we remove all the keys in key-value pairs and remain the relation to build the hypergraph-based schema. 14D Baseline Settings Firstly, for the original hyper-relational schema of HyperRED, we adopted the same baselines as in the CubeRE paper [5] to compare with Text2NKG: Generative Baseline: Generative Baseline uses BART [14], a sequence-to-sequence model, to transform input sentences into a structured text sequence. Pipeline Baseline: Pipeline Baseline uses UniRE [24] to extract relation triplets in the first stage and a span extraction model based on BERT-Tagger [6] to extract value entities and corresponding qualifier labels in the second stage. CubeRE: CubeRE [5] is the only hyper-relational extraction model that uses a cube-filling model inspired by table-filling approaches and explicitly considers the interaction between relation triplets and qualifiers. Secondly, for the event-based schema, role-based schema, and hypergraph-based schema, we added the following baselines to further validate the effect of Text2NKG on the fine-grained N-ary relation fact extraction task in the HyperRED dataset: Text2Event: Text2Event [5] is a classic model in the Event extraction domain. However, it is not applicable to extractions of the hyper-relational schema. For the role-based schema extraction, we retained the key without referring to the main relation, while for the hypergraph-based schema extraction, we retained the main relation without referring to the key to get the final result for comparison. UIE / LasUIE: UIE [17] and LasUIE [9] are unified information extraction models that can handle most tasks like NER, RE, EE, etc. However, they are still only suitable for event extraction in the multi-relational extraction domain and are not applicable to extractions of the hyper-relational schema. Therefore, we adopted the same approach as with Text2Event to compare with Text2NKG. Thirdly, under the impact of the wave of large-scale language models brought about by ChatGPT on traditional natural language processing tasks, we added unsupervised large models as baselines to compare with Text2NKG in the n-ary RE tasks of the four schemas. ChatGPT / GPT4: Using different prompts, we tested the latest state-of-the-art large-scale pre- trained language models ChatGPT [25] and GPT-4 [19] in an unsupervised manner, evaluating their performance on the extraction of the four schemas. E Hyperparameter Settings We use the grid search method to select the optimal hyperparameter settings for both Text2NKG with Bert-base and Bert-large. We use the same hyperparameter settings in Text2NKG with different encoders. The hyperparameters that we can adjust and the possible values of the hyperparameters are first determined according to the structure of our model in Table 4. Afterward, the optimal hyperparameters are shown in bold. Hyperparameter HyperRED α {1.0, 0.1, 0.01, 0.001} Train batch size {2, 4, 8, 16} Eval batch size {1} Learning rate {1e − 5, 2e-5, 5e − 5} Max sequence length {128, 256, 512, 1024} Weight decay {0.0, 0.1, 0.2, 0.3} Table 4: Hyperparameter Selection. F Model Training Details We train 10 epochs on HyperRED with the optimal combination of hyperparameters. Text2NKG and all its variants have been trained on a single NVIDIA A100 GPU. Using our optimal hyperparameter settings, the time required to complete the training on HyperRED is 4h with BERT-base encoder and 10h with BERT-large encoder. 15G Further Discussions G.1 How does ChatGPT perform in Fine-grained N-ary RE tasks? We have tried to use LLM APIs such as ChatGPT and GPT to do similar n-ary RE tasks, i.e., prompting model input and output formats for extraction. The advantage of ChatGPT is that it can perform similar tasks in a few-shot situation, however, for building high-quality knowledge graphs, the performance and the fineness of the n-ary RE are much lower than Text2NKG. This is because ChatGPT is not good at multi-label classification tasks that contain less semantic interpretation. When the number of labels of relations in our relation collection is very large, we need to write a very long prompt to tell the LLM about our label candidate collection, which again leads to the problem of forgetting. Therefore, we have tried numerous prompt templates to enhance the extraction effect of ChatGPT, however, on fine-grained n-ary RE task, the best result of ChatGPT can only reach about 10% of F1 value on HyperRED, which is much lower than the result of 80%+ F1 value of Text2NKG. However, advanced LLMs such as ChatGPT are a good idea for training dataset generation for Text2NKG in such tasks to save some manual labor to only verify and correct the training items generated. For future work, we will continue our research in this direction and try to combine large language models with Text2NKG-like supervised models for automated fine-grained n-ary RE for n-ary relational knowledge graph construction. G.2 Why first Extracting 3-ary facts and then Merging them into N-ary Facts ? We use output merging to address the dynamic changes in the number of elements in n-ary relational facts. The atomic unit of an n-ary fact includes a 3-ary fact with three entities. For instance, in the hyper-relational fact (Einstein, educated_at, University of Zurich, degree: Doctorate degree, major: Physics), the Text2NKG algorithm allows us to extract two 3-ary atomic facts: (Einstein, educated_at, University of Zurich, degree: Doctorate degree) and (Einstein, educated_at, University of Zurich, major: Physics). These are then merged based on the same primary triple ( Einstein, educated_at, University of Zurich) to form a 4-ary fact. The same principle applies to facts of higher arities. As another example demonstrating the problem with merging binary relations: consider the statement “Einstein received his Bachelor’s degree in Mathematics and his Doctorate degree in Physics.\" When represented as binary relations, the facts become (Einstein, degree, Doctorate degree), (Einstein, major, Physics), (Einstein, degree, Bachelor), and (Einstein, major, Mathematics). With this representation, we cannot merge these binary relation facts effectively because there’s no way to determine whether Einstein’s doctoral major was Physics or Mathematics. This necessitates the use of NKG’s n-ary relationship facts to represent this information, as seen in (Einstein, degree, Doctorate degree, major, Physics). Therefore, using binary facts, we can’t merge them into n-ary facts based on shared elements within these facts. On the other hand, using facts with four entities or more makes it challenging to effectively extract 3-ary atomic facts. In Section 5.6 and Figure 5(a), we also analyzed the effects and detailed insights of unsupervised extraction of arbitrary-arity facts. G.3 How Text2NKG can address Long Contexts with Relations spread across Various Sentences ? As long as the text to be extracted is a lengthy piece with entities annotated, it can undergo long-form n-ary relation extraction. The maximum text segment size for our proposed method depends on the maximum text length that a transformer-based encoder can accept, such as Bert-base and Bert-large, which have a maximum limit of 512. To extract from larger documents, we simply need to switch to encoders with larger context length, which all serve as the encoder portion of Text2NKG and are entirely decoupled from the n-ary relation extraction technique we propose. This is one of the advantages of Text2NKG. Its primary focus is to address the order and combination issues of multi-ary relationships. We can seamlessly combine a transformer encoder that supports long texts with Span-tuple Multi-label Classification to process n-ary relation extraction in long chapters. 16",
      "references": [
        "Complex query answering with neural link predictors.",
        "Freebase: A collaboratively created graph database for structuring human knowledge.",
        "Translating embeddings for modeling multi-relational data.",
        "Fuzzy logic based logical query answering on knowledge graphs.",
        "A dataset for hyper-relational extraction and a cube-filling approach.",
        "BERT: Pre-training of deep bidirectional transformers for language understanding.",
        "Knowledge vault: A web-scale approach to proba- bilistic knowledge fusion.",
        "Knowledge hypergraphs: Prediction beyond binary relations.",
        "Lasuie: Unifying information extraction with latent adaptive structure-aware generative language model.",
        "Message passing for hyper-relational knowledge graphs.",
        "What is event knowledge graph: A survey.",
        "Link prediction on n-ary relational data.",
        "Document-level n-ary relation extraction with multiscale representation learning.",
        "BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension.",
        "Role-aware modeling for n-ary relational knowledge bases.",
        "Text2Event: Controllable sequence-to-structure generation for end-to- end event extraction.",
        "Unified structure generation for universal information extraction.",
        "HAHE: Hierarchical attention for hyper-relational knowledge graphs in global and local level.",
        "Gpt-4 technical report,",
        "Beyond triplets: Hyper-relational knowledge graph embedding for link prediction.",
        "Wikidata: A free collaborative knowledgebase.",
        "Two are better than one: Joint entity and relation extraction with table- sequence encoders.",
        "Link prediction on n-ary relational facts: A graph-based approach.",
        "UniRE: A unified label space for entity relation extraction.",
        "Zero-shot information extraction via chatting with chatgpt,",
        "On the representation and embedding of knowledge bases beyond binary relations.",
        "Packed levitated marker for entity and relation extraction.",
        "Semantic parsing via staged query graph generation: Question answering with knowledge base.",
        "Collaborative knowledge base embedding for recommender systems.",
        "A frustratingly easy approach for entity and relation extraction.",
        "ReSel: N-ary relation extraction from scientific text and tables by learning to retrieve and select."
      ],
      "meta_data": {
        "arxiv_id": "2310.05185v3",
        "authors": [
          "Haoran Luo",
          "Haihong E",
          "Yuhao Yang",
          "Tianyu Yao",
          "Yikai Guo",
          "Zichen Tang",
          "Wentai Zhang",
          "Kaiyang Wan",
          "Shiyao Peng",
          "Meina Song",
          "Wei Lin",
          "Yifan Zhu",
          "Luu Anh Tuan"
        ],
        "published_date": "2023-10-08T14:47:13Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes Text2NKG, a fine-grained n-ary relation extraction framework for automatically constructing n-ary relational knowledge graphs (NKGs) from text, addressing three core challenges in prior work: (1) supporting diverse NKG schemas, (2) resolving entity order in n-ary facts, and (3) handling variable arity. Introduces a span-tuple multi-label classification formulation, hetero-ordered merging to reconcile predictions across permutations and inverse relations, and output merging to assemble higher-arity facts from predicted 3-ary “atomic” facts. Extends the HyperRED benchmark from hyper-relational schema to four schemas (hyper-relational, event-based, role-based, hypergraph-based) and reports state-of-the-art F1 on hyper-relational extraction.",
        "methodology": "Encodes entity spans with a BERT-based encoder using packed levitated markers to efficiently represent all ordered 3-entity span-tuples. Performs multi-label classification over ordered entity triples using multiple linear/FNN heads (one per relation slot determined by schema) with weighted cross-entropy and a null-label bias (alpha) to handle class imbalance. Applies schema-specific data augmentation by permuting entity orders and using inverse relations (where applicable). Decoding uses hetero-ordered merging to combine label probabilities from all 3! permutations into a canonical order (accounting for inverse relations) and filters tuples containing null labels. Finally, output merging groups consistent 3-ary facts (e.g., same main triple/event type) to form variable-arity n-ary facts in an unsupervised post-processing step.",
        "experimental_setup": "Benchmarks on HyperRED (originally hyper-relational) and an extended version supporting four schemas derived via deterministic conversions. Dataset stats: ~40k entities; relations: 106 (hyper-relational), 232 (event-based), 168 (role-based), 62 (hypergraph-based); splits: train 39,840 sentences/39,978 facts, dev 1,000/1,220, test 4,000/4,796. Baselines: for hyper-relational—Generative baseline (BART), Pipeline baseline (UniRE + span tagger), CubeRE; for other schemas—event/unified IE models Text2Event, UIE, LasUIE; plus unsupervised ChatGPT and GPT-4 via prompting. Evaluation: exact-match precision/recall/F1 where a predicted fact must match the full gold fact. Training: 10 epochs, Adam, single NVIDIA A100; results averaged over 5 seeds; ablations remove data augmentation, null-label weighting, or hetero-ordered merging; analysis varies alpha and examines extraction across arities and computational efficiency.",
        "limitations": "Requires gold (or pre-identified) entity spans; does not address end-to-end entity detection. Supervised learning dependence limits cold-start/generalization to new domains/relations and explains large gap vs LLM zero-shot. Enumerating ordered 3-entity tuples can still be heavy when sentences contain many entities (even with packed markers); memory/latency may be problematic for very entity-dense texts. Output merging is heuristic/unsupervised and may propagate early tuple errors, struggle with overlapping facts, or fail when higher-arity structure cannot be decomposed cleanly into consistent 3-ary atoms. Long-context handling is limited by the underlying encoder context length (BERT 512) unless replaced; cross-sentence/document-level aggregation is not deeply validated beyond the claim of encoder substitution. Inverse-relation assumptions and schema conversion rules may not hold universally across datasets/ontologies.",
        "future_research_directions": "Develop end-to-end extraction that jointly detects entities and n-ary facts; extend to document-level and cross-sentence n-ary relations with long-context encoders and explicit coreference/discourse modeling. Replace heuristic output merging with structured global inference (e.g., ILP/CRF/graph decoding) or neural set/graph generation to improve consistency for high-arity facts. Improve efficiency via candidate pruning, sparse retrieval over entity tuples, or adaptive arity prediction without enumerating all triples. Leverage LLMs for weak supervision/data generation, schema induction, or relation verbalization to reduce annotation cost and improve transfer; explore hybrid LLM+Text2NKG systems for cold-start plus fine-grained labeling. Validate on additional real-world corpora and more complex schemas (nested events, temporal constraints) and study robustness to noisy entity boundaries and ontology drift.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting",
      "full_text": "Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting Hejie Cui1∗ Xinyu Fang2∗ Zihan Zhang2 Ran Xu1 Xuan Kan1 Xin Liu3 Yue Yu4 Manling Li5 Yangqiu Song3 Carl Yang1† 1Emory University 2Tongji University 3 The Hong Kong University of Science and Technology 4 Georgia Institute of Technology 5 Northwestern University Abstract Images contain rich relational knowledge that can help machines understand the world. Existing methods on visual knowledge extraction often rely on the pre- defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation types), restricting the expressiveness of the extracted knowledge. In this work, we take a first exploration to a new paradigm of open visual knowledge extraction. To achieve this, we present OpenVik which consists of an open relational region detector to detect regions potentially containing relational knowledge and a visual knowledge generator that generates format-free knowledge by prompting the large multimodality model with the detected region of interest. We also explore two data enhancement techniques for diversifying the generated format-free visual knowledge. Extensive knowledge quality evaluations highlight the correctness and uniqueness of the extracted open visual knowledge by OpenVik. Moreover, inte- grating our extracted knowledge across various visual reasoning applications shows consistent improvements, indicating the real-world applicability of OpenVik. 1 Introduction Knowledge extraction has been widely studied on texts [8, 1, 13, 9] for enhancing logical reason- ing [45, 14, 6] and explainable AI [18, 57, 5, 55], and recent studies have explored open knowledge extraction through categorizing seed relations [ 64, 40] and eliciting from language models [ 47]. Visual knowledge extraction, on the other hand, captures intricate details like tools, sizes, and positional relationships, which are often difficult to express exhaustively in texts [ 39, 28, 48, 7]. Yet existing approaches of visual knowledge extraction are either restricted by a fixed knowledge format [52, 63, 20, 22] or the predefined sets of objects/relations [ 52, 63, 21]. While efficient at capturing interactions between objects, the produced visual knowledge is often limited in richness and confined to a single format, falling short in representing the diverse real-world information that can be complemented by visual data. In this endeavor, we propose to further explore a new paradigm of open visual knowledge extraction (OpenVik). Specifically, we propose to generate relation-oriented, but format-free knowledge that includes a wider variety of elements, such as descriptions, insertions, and attributes, among others. Drawing inspiration from the wealth of knowledge encapsulated in large models [ 49, 61, 46], we propose to leverage pre-trained large multimodality models by eliciting open visual knowledge through relation-oriented visual prompting. This approach allows for a more nuanced understanding of visual data, mirroring how humans naturally emphasize certain aspects of visual scenes when perceiving and describing visual information, leading to more flexible visual knowledge extraction. ∗These authors contributed equally to this work. †Correspondence to: j.carlyang@emory.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.18804v1  [cs.CL]  28 Oct 2023Our proposed OpenVik framework consists of two modules, an open relational region detector and a format-free visual knowledge generator. It is a unique challenge to detect the regions potentially containing relational knowledge, since traditional region detectors primarily focus on learning predefined object classes. To learn the regression of relational regions, we propose to use free-form knowledge descriptions as supervision and leverage knowledge generation as a training objective. With the detected regions, the remaining question is how to interpret these regions into free-form knowledge. We propose a visual knowledge generator by harnessing the power of language variety enhancement in large pre-trained multimodality models. Specifically, we prompt them to generate knowledge descriptions of any formats and condition the generation on the detected relational regions. However, establishing a new paradigm of open visual knowledge extraction is challenging due to the absence of comprehensive and diverse training data. Existing datasets sources such as scene graphs [51, 24], dense captions [ 20], and dense relational subsets [ 22] often exhibit a long-tail distribution biased to more prevalent relations and entities [44]. Brute-force merging of these datasets could exacerbate the distribution bias inherent in the data. To alleviate the bias, we propose two diversity-driven data enhancement strategies based on an adapted TF-IDF+ score, involving random dropping and data augmentation with external knowledge resources. These strategies optimize data distributions and richness, thus fostering diverse open visual knowledge extraction. We implement extensive evaluations to assess the quality and utility of the open visual knowledge ex- tracted by OpenVik, encompassing: 1) directly evaluating the performance of knowledge generation; 2) engaging human evaluators for a multi-faceted assessment of in-depth knowledge quality; and 3) comparing the open visual knowledge extracted withOpenVik with existing knowledge sources, such as non-parametric knowledge from the ConceptNet knowledge graph, and parametric knowledge from the GPT-3.5 large language model. Furthermore, the utility of the extracted open visual knowledge is validated through its integration with several common applications that require visual understanding, including text-to-image retrieval, grounded situation recognition, and visual commonsense reasoning. These applications demonstrate consistent improvements, affirming the practical utility ofOpenVik. 2 Related Work Visual knowledge extraction. Recent advancements in knowledge extraction have extended from being purely text-driven to incorporating images [11, 29]. VisKE [39] is designed to verify relations between pairs of entities, e.g., eat(horse, hay). Scene graphs, which locate objects in the image and identify visual predicates between subjects and objects in a triple format, e.g., (man, on, chair), are extensively studied for vision understanding [52, 63, 60]. A recent work OpenSGG [17] extends SGG to open-vocabulary objects, enabling the relation prediction for unseen objects. Other studies have explored caption-like formats, like dense captioning [20] with a set of object-centric descriptions across regions, and relational captioning [22] focusing on relational information between objects. Despite these advancements, existing methods either adhere to a pre-defined format and vocabulary or are constrained by the biased distribution of training sets. This highlights the pressing need for a format-free approach in visual knowledge extraction with knowledge diversity. Large model prompting. Recently, large language and multimodality models have exhibited re- markable successes in capturing commonsense knowledge across various tasks, especially facilitating few-shot [15, 53, 25, 58] and zero-shot learning [23, 66, 59]. The potential of prompt-based learning for pre-trained vision-language models [2, 37, 42] has been explored for handling diverse data types across multiple modalities, such as images and texts, with improved performance in tasks including image classification [33, 67], segmentation [32] and visual question answering [ 16]. Leveraging the substantial information encapsulated within these pre-trained multimodality models to extract explicit knowledge can enrich existing resources, potentially laying the groundwork for advances in interpretability research and mitigating the hallucination issue associated with large models [19, 10]. 3 Method In this section, we introduce our new paradigm and two key model design novelty featuringOpenVik, relation-oriented multimodality model prompting and diversity-driven data enhancement. 2+Relationaboatrestsonwater+Entitythejetinblueskysmokepathinair relationenhancedentityenhanced Detector TrainingDatatheboatonwaterplaneinaircrowdstandonground Diversity-DrivenDataEnhancementFormat-freeVisualKnowledgeGeneratorOpenRelationalRegionDetector  RandomDroppingAugmentationwithExternalResources Inference Relation-OrientedRegions 𝓛𝑲selectedoriginaldata Low𝓛𝑹𝑫 𝓛𝑴𝑳𝑬 OpenVisualKnowledgeOpenVikLargeMultimodalityModel TF-IDF+High 𝓛𝑽 GeneratedKnowledgeSimilarity KGLLMPrompting largeboatdockedatpierflyingjetleaving behindsmokebuilding blocksacross frombrave bluewaterstandingcrowdwatchas a jet flies highabovewater Figure 1: The overview of OpenVik. The left orange and purple panels illustrate key components of relation-oriented multimodality model prompting: open relational region detector and format-free visual knowledge generator. The right green one depicts diversity-driven data enhancement strategy. OpenVik is designed to extract relation-oriented format-free open visual knowledge with novel entities , diverse relations , and nuanced descriptive details . 3.1 Open Visual Knowledge Extraction Given a dataset D = {(Ii, Ti, Ui)}M i=1 consisting of M samples, Ii is the i-th image (such as the input image in Figure 1), Ti = {Tj}ni j=1 is a set of ni region descriptions (such as “ the boat on water” in Figure 1), Ui = {Uj}ni j=1 is the set of ni relation-oriented visual regions, where each Tj corresponds to a visual region Uj ∈ Ui in image Ii. The goal of our open visual knowledge discovery is to train a model M capable of producing a set of format-free knowledge descriptions (such as “large boat docked at pier” in Figure 1) given any image Ik during the inference stage. 3.2 Relation-Oriented Multimodality Model Prompting The overall architecture of OpenVik is shown in Figure 1. It comprises two modules: an open relational region detector Mv and a format-free visual knowledge generator Mt. The two modules are learned separately during training with our diversity-enhanced data (Section 3.3) and combined to produce format-free visual knowledge at inference. Specifically, the relational region detector Mv takes an image Ii as the input and learns to select a flexible number of relational regions Ui = {(Uj)}ni j=1 that captures object interactions, each corresponding to a description Tj in Ti; the visual knowledge generator Mt generates format-free knowledge descriptions by prompting and fine-tuning the multimodality model with the guidance of detected visual region Uj. All notations for the region detector and knowledge generator are detailed in Table 1 and Table 2, respectively. Table 1: Notations for open region detector. Notation Meaning Ii input image of the relational region detectorUj relation-centric box labelUi set of relation-centric boxes of an imageTj region description of a boxTi set of region descriptions of the an imageLRD region regression loss supervised by union regional boxesLK knowledge generation loss supervised by GT relational knowledgeLv the overall objective of the relational region detector Table 2: Notations for knowledge generator. Notation Meaning Ii the input image of the knowledge generatorTa, Tb two regional knowledge descriptions of one same imageNi the number of generated knowledge descriptions of an imageϕ hyper-parameter controlling the penalty slightly different sequencesLMLE the language modeling loss of the generation decoderLV inter-sequence information variety regularizerα weight hyper-parameter balancing generation accuracy and varietyLl the overall objective of the knowledge generator Open relational region detector. Although existing object detection algorithms have been widely recognized for their efficiency in object detection, they are usually restricted to object-centric visual regions in a predefined set, and thus cannot directly capture open relational information with a single box. Detecting regions containing relational knowledge remains to be a challenge. We make two adaptions on the object detection FasterRCNN [38] to train the open relational region detector: • Region Regression: we change the original object-centric region labels to our newly created relation- centric box labels, denoted asUj. The foreground of each relation-centric region labelUj is created by taking the union of the object-level bounding boxes of the entities, i.e., boat, water, contained in a ground truth region knowledge description Tj. This forms the region regression loss LRD. 3• Knowledge Supervision: To assist with the refinement of the bounding box, we replaced the object-centric label classification in traditional object detectors with knowledge supervision. A pre-trained generator is finetuned to create the regional description grounded to the given region. This is supervised by the cross-entropy loss LK with region description Tj. The training objective Ll of the relational region detector is formulated as below, whereLRD is the regional regression loss and LK is the knowledge supervision loss, Lv = LRD + LK. (1) Format-free visual knowledge generator. OpenVik provides better knowledge grounding by conditioning the generator on the detected relational region, leading to a reasoning-driven generation. Specifically, the detected bounding box (such as the box containing “boat” and “pier” on the far left) is utilized as a visual prompt when fine-tuning the visual knowledge generator. The model architecture of the knowledge generator is built upon a combined large multimodality model, which composes a pre-trained vision transformer ViT-B [12] and the image-grounded text decoder of BLIP [27]. The two modules are jointly trained on a generic image-text paired dataset comprising over 14 million entries and fine-tuned on the image captioning task, which delivered state-of-the-art performance. In our visual knowledge generator, the decoder takes the ViT visual representation of the entire image as input and leverages the detected regional mask as a binary visual prompt. This prompt aids in filtering out the background and directing attention toward the relational foreground. The generation of format-free knowledge from the decoder is supervised by the language modeling loss LMLE, which further refines visual attention during the knowledge generation process. As a result, our approach facilitates the production of format-free outcomes that extend beyond the conventional sub-verb-obj form. Besides, to improve information variety, we introduce an amplifying penalty factor for highly similar knowledge generation. For any two generated sequences Ta and Tb describing image Ii, LV = 1 Ni X Ni ReLU (−log (1− (s (Ta, Tb) − ϕ))) , (2) where Ni is the number of generated knowledge of image Ii, s (Ta, Tb) indicates the semantic cosine similarity, and ϕ is a hyper-parameter set as 0.01 controlling the penalty on sequences with only slight difference (e.g. “dog chasing the man” and “dog licking the man”) to be relatively small. The training objective Ll of the format-free visual knowledge generator is formulated as Ll = α × LMLE + (1− α) × LV, (3) where α is a weighting hyper-parameter we set as 0.7. The trained relational region detector and visual knowledge generator are combined during inference. Given any image I, the open relational region detector first detects a flexible number of open relations regions of interest, then each detected region R is passed to the format-free visual knowledge generator, where a relation-oriented format-free knowledge phrase (such as “flying jet leaving behind smoke” in Figure 1) is generated to describe the given visual focus subarea R of the image. To further encourage within-sequence language variety during inference, we leverage the contrastive decoding strategy from [ 43], which improves over nucleus sampling and beam search. 3.3 Diversity-driven Data Enhancement The training data for relational knowledge extraction usually exhibits a long-tail distribution, where more prevalent but simple relations such as in, on, and wear dominate the training set [44]. Conse- quently, the model trained with such a biased dataset may render limited, and repetitive knowledge. As a remedy, we propose two data enhancement techniques to optimize the data distribution. As the foundational measure for given relation r’s importance, we design a grid TF-IDF+ score Sr [34, 54]: Sr = (log( N 1 +fr ∗ α1 ))α2 , (4) where N is the total number of knowledge phrases in the datasets, fr is the number of occurrences of the relation r, α1 and α2 are the grid scales whose values are selected based on fr. Random dropping on low-quality data. We first remove repeated knowledge descriptions in the same image and then randomly drop descriptions that contain frequently occurring yet meaningless 4relations with a low Sr (e.g., “people on ground”) from the original dataset. Specifically, if the Sr of the relation in a description is relatively low, i.e., 0.4, we remove it at a random dropping rate of 0.5. This process repeats for all descriptions in an image until the remaining set is 0.6 times the size of the original training set. Consequently, the training data bias is mitigated by removing low-quality data. Data augmentation with external knowledge resources.For the relations with high TF-IDF+ scores, we leverage external knowledge resources from both non-parametric (i.e., ConceptNet [ 41]) and parametric (i.e., COMET [4]) knowledge resources to promote diverse knowledge generation [56]. ✔ Enhance Relation Recognition: For each training description with a high TF-IDF+ score, we perform semantic parsing to get all the objects and complement additional relations (e.g., “rest” in Figure 1) between each pair of them by mapping the nodes and retrieving edges from the ConceptNet. Each retrieved knowledge triplet is converted to a knowledge phrase and added to the training set for generator training. With this introduced external knowledge, the knowledge generator ultimately yields a more robust and detailed representation of the underlying visual information of objects. This, in turn, bolsters the relation recognition of the visual knowledge generator.✔ Boost Entity Perception: For the description with the highest-scored TF-IDF+ relation given each image, we also leverage ConceptNet to enrich similar objects (e.g., “jet”) to the original object (e.g., “plane”). Additionally, we further introduce new entities (e.g., “smoke” in Figure 1) and attribute descriptions (e.g., “blue”) by prompting the pre-trained attribute commonsense branch of the COMET model (Refer to Appendix A for more details). The entity-based enrichment potentially helps in boosting entity understanding and at the same time enhances the occurrence of important but rare relations in the training set. 3.4 Implementation Details Our training data are built based on Visual Genome [24] and its relation-enhanced version Dense Relational Captioning [22]. Each sample includes an image identified by a unique ID and a set of relational descriptors describing interactions among objects in the image. Specifically, each relational descriptor includes the full description text, the subject and object names contained in the description text, the relation between them, as well as the bounding box coordinates of the subject and object. The dataset statistic information is summarized in Table 8 in the Appendix B. Our model is implemented in PyTorch [35] and trained on two Quadro RTX 8000 GPUs. The open relational region detector is initialized from the ResNet50-FPN backbone, then finetuned for another 20 epochs with the relational bounding box. The model detects a maximum of 30 bounding boxes for each image with the highest confidence to avoid misleading noises. The format-free visual knowledge generator is initialized from BLIPbase with the basic ViT-B/16 and finetuned for 20 epochs. Full details on learning parameters can be referred to in Appendix C. 4 Evaluation In this section, we directly evaluate the extracted open visual knowledge from OpenVik from two perspectives: (1) knowledge generation performance with traditional generative metrics and in-depth knowledge quality assessment; (2) comparison with existing knowledge sources. Besides, ablation studies are conducted to study the influence of diversity design on the generated knowledge and data. 4.1 Evaluation on Generated Knowledge Generation performance. To directly evaluate the visual knowledge generator, we compare the knowledge generated by OpenVik with a variety of baselines, including scene graph generation [52, 63, 44, 17] (of which Ov-SGG employs an open vocabulary), dense relational captioning [22], and region captioning [20, 65, 27, 26]. Evaluation metrics are traditional language generation measures such as BLEU, ROUGE-L, and METEOR. The results, displayed in the left side of Table 3, reveal that OpenVik outperforms captioning-based approaches and yields results on par with the best scene graph generation baseline. These findings underscore the effectiveness of the format-free visual knowledge generator through relation-oriented prompting of the large multimodality model. In-depth knowledge quality. To more thoroughly evaluate the quality and richness of the format- free visual knowledge extraction, beyond simply evaluating it as a language generation model with the 5Table 3: Knowledge comparison of OpenVik and baselines on performance and in-depth quality (%). Method Generation Performance In-Depth Knowledge Quality BLEU↑ ROUGE-L↑ METEOR↑ Validity↑ Conformity↑ Freshness↑ Diversity↑ Closed/Open Scene Graph Generation IMP [52] 0.075 0.123 0.118 0.800 0.823 0.676 0.316 Neural Motifs [63] 0.229 0.283 0.273 0.822 0.767 0.667 0.349 UnbiasSGG [44] 0.217 0.258 0.194 0.739 0.733 0.666 0.357 Ov-SGG [17] 0.167 0.210 0.183 0.712 0.633 0.693 0.413 Dense Relational Captioning MTTSNet+REM [22]0.240 0.226 0.228 0.897 0.852 0.754 0.375 Region Captioning DenseCap [20] 0.248 0.245 0.196 0.883 0.843 0.790 0.543 Sub-GC [65] 0.272 0.263 0.221 0.892 0.871 0.795 0.547 BLIP [27] 0.264 0.266 0.252 0.886 0.855 0.760 0.531 BLIP2 [26] 0.275 0.285 0.257 0.892 0.871 0.766 0.535 Open Visual Knowledge Extraction OpenVik 0.280 0.283 0.250 0.907 0.883 0.809 0.619 limitation of training data, we incorporate four additional metrics [31], which delve into an in-depth quality evaluation of the extracted visual knowledge from four distinct perspectives: • Validity (↑): whether the generated visual knowledge is valid to human. • Conformity (↑): whether the generated knowledge faithfully depicts the scenarios in the images. • Freshness (↑): the novelty of the knowledge, i.e., the proportion not present in the training set. • Diversity (↑): the language variance between a randomly sampled pair of knowledge pieces. Among the four metrics, both the validity and conformity metrics involve human annotators. We randomly selected 100 images as the evaluative subset. Details regarding the scoring guidance and the interface provided to the annotators can be found in Appendix D. The remaining metrics, i.e., freshness and diversity, are calculated automatically. The in-depth knowledge quality evaluation results are displayed in the right part of Table 3, where the average pairwise Cohen’s κ on human evaluation results is 0.76 (good agreement). The findings demonstrate that trained with the diversity- enhanced datasets, the format-free visual knowledge extracted by OpenVik significantly outperforms other types of baselines in terms of all four metrics. The improvement of diversity, in particular, reaches 14% relatively compared with the inference results from the second runner DenseCap, indicating the advantage of OpenVik in generating rich and comprehensive visual knowledge. 4.2 Comparison with Existing Knowledge Sources Spatial Details,Motion Dynamics,Scene Insights Attributes,Belongings Non-parametric KnowledgeParametric Knowledge OpenViK DescriptiveElements Abstract InsightsLogical Relations ExamplesKnowledge Sourcedog IsAanimal (ConceptNet); conceptnet IsAknowledge graph (ConceptNet);Non-parametric Knowledgedog HasPropertyblack (ConceptNet); dog and brown fur covering black (OpenVik)computer HasAkeyboard (ConceptNet); keyboard with computer (OpenVik)people using light bulbs to illuminate the room; (LLM)Parametric Knowledgeyellow sign in corner (both); black seat attached to bike (both);three layercake on table; blue trash canfull of garbage next to brown dresser; blue box sitting beside a sneaky garage; (OpenVik)Open Visual Knowledge(OpenViK)people wearing fashionable black hats are skiing; baby elephants walking around adventurous wood; (OpenVik)the light shining from bright black background; hanging fan are above tall shelf;  brown chair in the background of the room; (OpenVik) Figure 2: The Venn diagram of knowledge comparison between the open visual knowledge from OpenVik with the non-parametric knowledge from existing knowledge graph (i.e., ConceptNet) and parametric knowledge from large language model (i.e., COMET). We compare the extracted visual knowledge with the non-parametric knowledge in the existing knowledge graph (KG) and the parametric knowledge from the large language model (LLM). The comparison insights from the three knowledge resources are shown in the Venn Diagram in Figure 2. Compare with non-parametric knowledge. We take ConceptNet [ 41] as the representative in the comparison with non-parametric knowledge. To map the knowledge generated by OpenVik to ConceptNet, we parse the knowledge into triplets and associate the endpoints of these triplets with 6nodes in ConceptNet. Then we calculate the similarity of embeddings3 between the parsed relation and all the edge relations among the mapped nodes in ConceptNet. If the similarity score exceeds a predetermined threshold, i.e., 0.75, we consider the mapping successful. As illustrated in Figure 2, we observe that compared with the non-parametric knowledge in KG, the extracted visual knowledge captures richer and more meaningful spatial details, e.g., “ three layer cake on table”, and motion dynamics, e.g., “baby elephants walking around adventurous wood”. Compare with parametric knowledge. We compare with parametric knowledge contained in LLM by prompting the gpt-3.5-turbo4 model with the object information in the image. The prompt template used is detailed in Appendix E. The mapping process follows the approach mentioned earlier. It is found that compared with the parametric knowledge in LLM, the extracted visual knowledge exhibits unique fine-grained visual details, e.g., “red sticker on fence”, and provides precise scene information, e.g., “the light shining from bright black background”. 4.3 Ablation Study /uni00000039/uni00000044/uni0000004f/uni0000004c/uni00000047/uni0000004c/uni00000057/uni0000005c /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni0000001c /uni00000014/uni00000011/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000026/uni00000052/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni0000004c/uni00000057/uni0000005c /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni0000001c /uni00000014/uni00000011/uni00000013 /uni00000029/uni00000055/uni00000048/uni00000056/uni0000004b/uni00000051/uni00000048/uni00000056/uni00000056 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni0000001c /uni00000014/uni00000011/uni00000013 /uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni00000056/uni0000004c/uni00000057/uni0000005c /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni00000018/uni00000018 /uni00000013/uni00000011/uni00000019/uni00000013 /uni00000013/uni00000011/uni00000019/uni00000018 /uni00000013/uni00000011/uni0000001a/uni00000013 /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000033/uni00000055/uni00000048/uni00000027/uni00000048/uni00000057/uni0000005a/uni00000012/uni00000052/uni00000003v  /uni0000005a/uni00000012/uni00000052/uni00000003/uni00000027/uni00000055/uni00000052/uni00000053/uni0000005a/uni00000012/uni00000052/uni00000003aug  /uni0000005a/uni00000012/uni00000052/uni00000003aug  /uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f Figure 3: The influence of information variety regularization and diversity-driven data enhancement strategies. The influence on knowledge quality with information variety regulariza- tion and data strategies. We con- ducted ablation studies to evaluate the effectiveness of the information vari- ety regularizer, LV, and our diversity- driven data enhancement strategies. This involves an in-depth assessment of knowledge quality on the same eval- uation subset. The results are presented in Figure 3. It is evident from the results that our proposed information variety design primarily impacts freshness and diversity, without compromising validity and conformity. For the freshness, the omission of data augmentation for entities and relations results in the most significant performance degradation. This implies the crucial role these strategies play in infusing novel knowledge into the generation process. As for diversity, the most notable changes in metrics are observed when the LV and random dropping are removed. The strategy for augmenting entities and relations also plays a valuable role in enriching diversity. Ablation of the pre-training for the open relational region detector. We conducted a comparison of the outcomes when loading a pre-trained detector backbone versus training the detector from scratch, as shown by the yellow bar in Figure 3. Results demonstrate a noticeable decrease in both knowledge diversity and freshness, which indicates the importance of loading the pre-trained model for region detection. This may be because omitting the pre-training step of the FasterRCNN model tends to result in the detection of more overlapping regions, which in turn causes the drop. The influence on dataset diversity with data strategies. We conduct a direct analysis of the knowledge diversity of the existing datasets and our diversity-enhanced one, compared with the visual knowledge generated from OpenVik. The findings, presented in Table 4, show that the diversity-driven data enhancement strategies significantly boost knowledge diversity. Trained with this enhanced data, OpenVik can extract visual knowledge that exhibits greater diversity than that found in the Visual Genome and Relational Caps, indicating the advantage of OpenVik to format-free visual knowledge generation and its ability to yield richer knowledge diversity. Table 4: Diversity of existing and enhanced datasets and generated knowledge from OpenVik. Metrics Training Dataset Generate KnowledgeVisual Genome [24] Relational Caps [22]Diversity Enhanced (Ours)OpenVik(Ours) Diversity 0.589 0.604 0.632 0.619 4.4 Case Study We present two case studies in Figure 4 (See Appendix F for more) to showcase the format-free visual knowledge generated by OpenVik, in comparison to Visual Genome (Scene Graph and Region Description) and Relational Caps. Contrary to the rigidity of scene graphs, which strictly adhere to a 3Embeddings are produced by ConceptNet API: https://github.com/commonsense/conceptnet-numberbatch. 4https://platform.openai.com/docs/models/gpt-3-5 7predefined format, OpenVik can generate knowledge with a flexible semantic structure, not strictly bound to the sub-verb-obj format (e.g., “blue post attached to wall with white letter”). Examples of this adaptability are highlighted in red. When compared to dense region descriptions, the relational knowledge extracted by OpenVik offers a deeper understanding of the multiple entity interactions within an image. In comparison to Relational Caps, which mainly focus on interactions between two objects, OpenVik significantly broadens the diversity of relation with vivid verbs (e.g., “attached to”, “adorning”). Moreover, it introduces novel entities (e.g., “post”, “mane”) and enriches the knowledge representation with nuanced details (e.g., “full of ”, “striped”) that are missed by Relational Caps. Visual Genome-Scene Graph: <drink, in, cooler> <orange, in, box> <banner, on, building> <item, on, table> Visual Genome-Region Descriptions: oranges in a wood thing green leaves on oranges red writing on a white sign drink in red cooler Relational Caps: snow-covered oranges in wood thing the frost on snow-covered oranges green leaves on snow-covered oranges red writing on white sign OpenVik:  blue post attached to wall with white letter     the open window to snowy ground wood box full of different size of orange white banner on a building with letter o blue box sitting beside a sneaky garage a orange covered with ice and green leaves Visual Genome-Scene Graph: <hair, on, head> <zebra, eat, grass> <eye, on, zebra> <grass, on, ground> Visual Genome-Region Descriptions: black and white striped leg light shining on the zebra thin line of black hair  two zebras grazing in the grassOpenVik:  striped mane belongs to grazing zebra    zebra with striped ears eating green grass  white stripe adorning leg                         dark brown mane growing behind head grass everywhere surround standing zebra         black nose above green lively grass Relational Caps: sticking up ear of grazing zebra black eye of eating zebra grazing zebra in green grass the muzzle of grazing zebra Figure 4: Case study on the extracted open visual knowledge fromOpenVik. Examples of format-free knowledge are highlighted in red. Compared with VG and Relational Caps, OpenVik performs better at capturing novel entities , broadening object interactions with diverse relations , and enriching the knowledge representation with nuanced descriptive details . Note that we observe the unbalanced and noisy distributions within the training data can lead to errors in the knowledge produced. Viewing hallucinations as erroneous inferences based on input, the inaccuracies observed in OpenVik and similar baselines often stem from detection errors. These errors are typically caused by data biases that incorrectly associate features with a specific class or label. We further two illustrative failure cases in Figure 5. For example, a “ black speaker by flat tv” is generated, although the speaker is not present in the image—possibly reflecting common co-occurrences within the dataset. Similarly, a ladder in the right figure has been misidentified as a towel, leading to the erroneous description of a “blue towel hanging from dry shower”. The key to mitigating such incorrect inference is identifying the cofounder feature of class labeling. Visual Genome-Scene Graph: <window, in, screen> <eye, of, cat> <keyboard, on, laptop> <cat, beside, laptop> Visual Genome-Region Descriptions: laptop key board cat beside the laptop pictures on wall behind cat the cat is brown Relational Caps: white cat beside black laptop the window in screen the wall behind cat OpenVik:  white cat sitting near computer black speaker by flat tv black wire plugged into computer the picture in screen OpenVik:  framed picture hanging on wall recessed lighting in ceiling blue towel hanging from dry shower the sunlight coming through window Visual Genome-Scene Graph: <paint, in, frame> <picture, on, wall> <window, in, bathroom> <curtain, over, window> Visual Genome-Region Descriptions: leg of an elephant baby elephant in grass Grass within the enclosure A big rock on the ground Relational Caps: blue painting in metal frame the shampoo bottle on rack blue jar in window Figure 5: Examples of incorrectly knowledge resulting from distribution bias are highlighted . 5 Application This section explores whether the extracted open visual knowledge from OpenVik can bolster reasoning and inference capabilities in multimodality downstream tasks by augmenting a baseline in the challenging zero-shot setting. 5.1 Text-to-Image Retrieval Task Setting. In the text-to-image retrieval task, a given caption is matched to a large set of candidate images, with the most relevant image returned as the result. Adopting the challenging zero-shot 8setting, we generate the visual representation v and textual representation t of the given image I and caption T using a pre-trained clip-retrieval model [3]. The baseline involves the image and text embedding similarly based on zero-shot CLIP Retrieval [3] and the fine-tuned model from BLIP [26]. To explore the potential of the extracted visual knowledge fromOpenVik, we enrich the given caption T with related contexts derived from the extracted visual knowledge. Specifically, for each query caption, we parse the caption to extract all subject-object pairs (s, o) with the NLTK parser. Then s and o are mapped to the open visual knowledge, where knowledge phrases that contain relations occurring more than 30% of the time between s and o are enriched to the original caption T . Original text:A little room and dining room area with furniture.Aliving room with a big table next to a book shelf. A living room decorated with a modern theme. Aliving room with wooden floors and furniture. The large room has a wooden table with chairs and a couch.  Enriched text: big table in room. a decorated living room with wooden furniture. brown couch in room. book on table. wooden table with shelf. shelf next to couch. wooden bookshelf with books next to table. Figure 6: An example of OpenVik enrichment on text- to-image retrieval (See Appendix G.1 for more). Method Recall@1 Recall@5 Recall@10Avg ZS-CLIP 36.16 65.47 78.66 60.10OpenVik+ ZS-CLIP40.55 73.29 84.53 66.12BLIP 63.11 86.30 91.10 80.17OpenVik+ BLIP 65.23 87.71 91.90 81.61 Table 5: Text-to-image retrieval results (%) of OpenVik enrichment compared with zero-shot baselines. Qualitative examples. Figure 6 presents an example ofOpenVik-based visual knowledge enrichment on captions. By incorporating related contexts from the generated open visual knowledge, the enriched captions convey more precise visual details, which enhances the alignment for text-image alignment. Quantitative results. We curated a subset of 680 images from the testing set of the MS-COCO dataset containing parsed knowledge with at least eight nouns. This ensures an adequate degree of enrichment is achieved through the use of OpenVik. Standard image retrieval metrics, i.e., Recall@1/5/10/ and Avg, are employed to evaluate the performance. The results are presented in Table 5. It is evident that relational context enrichment leads to the average correction of more than 6.0% of the initial zero-shot, highlighting the practical benefits of extracted visual knowledge in visual reasoning tasks. 5.2 Grounded Situation Recognition Task setting. The event type prediction for the grounded situation recognition task is to predict the best match from predefined 504 event types [ 36] based on the image. We convert each candidate event verb into a description T : “An image of <verb>” for image description matching. Similarly to text-to-image retrieval, we include zero-shot CLIP and the fine-tuned model from BLIP as baselines. To enrich with contextual knowledge from OpenVik, for each given verb v, we find its nearest synonym in the extracted open visual knowledge and enrich the text description with the most common knowledge phrase containing it, regularized by the objects present in the image. Instead of directly concatenating the retrieved knowledge triplets to the original textual description, we employ an additive decomposition strategy: the similarity s(I, v) of the candidate verb v with respect to the given image I is calculated as s(I, v) = 1 |D(v)| P d∈D(v) ϕ(I, v), where D(v) is the set of descriptors, including the original description and the enriched ones, and ϕ represents the single log probability that descriptor d pertains to the image I. Verb: talkingOriginal text:This is an image of talking. Enriched text:man talking on a small white telephone. adult male with white shirt talking on chatty cellphone.the man waving arms is talking on phone. Figure 7: An example of OpenVik context enrichment on task GSR (See Appendix G.2 for more). Method Accuracy Precision Recall F1 ZS-CLIP 53.14 42.54 45.19 43.82OpenVik+ ZS-CLIP75.16 61.63 62.7562.18BLIP 70.42 65.32 69.25 67.23OpenVik+ BLIP 80.25 72.55 70.6171.57 Table 6: Grounded situation recognition results (%) of OpenVik enrichment com- pared with zero-shot baselines. Qualitative examples. Figure 7 presents a qualitative example ofOpenVik-based context enrichment in the grounded situation recognition task. We observed that verbs like “shopping” and “talking” were appropriately enriched with their frequently occurring contexts from the open visual knowledge, leading to a reduced embedding distance between the description and its matching image. Quantitative results. We assembled a test set of 900 samples from the testing set of GSR that included verbs such as “talking”, “filming”, and “picking”, among others, from a list of 256 words 9that can be accurately mapped to extracted visual knowledge, as well as 138 verbs that have a fuzzy match through ConceptNet embedding comparison. The full lists of the exact and fuzzy-matched verbs are detailed in Appendix H. The evaluated metrics include Accuracy, Precision, Recall, and F1. The results are presented in Table 6. It can be observed that knowledge enrichment significantly outperforms the zero-shot and BLIP baselines. This suggests that the verb-related contexts introduced by OpenVik-generated knowledge are intuitive and greatly assist in understanding the semantics of event verbs, bolstered by related visual information. 5.3 Visual Commonsense Reasoning Task setting. The goal of visual commonsense reasoning is to predict an answer from four given option candidates for a given image and question. For the baseline approach, we compare the backbone model R2C from the VCR paper [62] and BLIP [27]. In the visual knowledge-enhanced OpenVik Enriched approach, we perform two-level context augmentation, incorporating both entities and relations: (1) we parse the question and options to obtain all (S, O) pairs and, for each entity pair, apply the same relation augmentation as in the image retrieval task; (2) for the V in each option, we enrich the visual context using the same method as illustrated in grounded situation recognition. Question: Is  Person1winning the game? the person engaged in a game.AYes, he is about to go for a run. the person engaged in a game.the person walking near runway.BNo, he is losing. the person engaged in a game. frustrated person lose game. CNo, he’s not really enjoying it. the person engaged in a game. person enjoy at celebration.DYes, he looks like he has a good hand. the person engaged in a game.the person is watching left hand.Answer: DYes, he looks like he has a good hand. Figure 8: An example of OpenVik context enrichment on the VCR task (See Appendix G.3 for more). Method Accuracy Precision Recall F1 R2C 56.66 56.73 56.72 56.72OpenVik+ R2C 59.96 60.01 60.0360.02BLIP 62.50 62.50 62.45 62.47OpenVik+ BLIP67.40 67.54 67.4367.48 Table 7: Visual commonsense reasoning results (%) of OpenVik context enrich- ment compared with zero-shot baselines. Qualitative examples. Figure 8 presents an example before and after applying the two-level visual knowledge-based enrichment for visual commonsense reasoning. The results indicate that visual knowledge enhances the correspondence between the correct answer and the image itself. Quantitative results. We assembled a test set of 939 samples from the validation set of the VCR dataset [62]. Each sample in this test set contains questions and answers with a minimum of five nouns and two relations, guaranteeing an adequate level of information complexity for meaningful engagement with open visual knowledge. The results can be found in Table 7. We observe that the enriched visual knowledge helps especially when solving reasoning questions on humans and their interactions with visually impressive entities, such as “game” in Figure 8. This enhancement results in a performance improvement above 3.0% over the zero-shot baseline. 6 Conclusion, Limitations, and Future Work This work is the first exploration of a new paradigm of open visual knowledge extraction, which combines an open relational region detector to flexibly pinpoint relational regions and a format-free visual knowledge generator that generates visual knowledge by prompting a multimodality model conditioned on the region of interest. To further enhance the diversity of the generated knowledge, we explore two distinct data enhancement techniques. Extensive knowledge evaluations underscore the correctness and uniqueness of our extracted open visual knowledge, and the consistent improvements observed across various visual reasoning tasks highlight the real-world applicability of OpenVik. While our approach has been shown effective in various scenarios, its performance at larger scales or on more diverse datasets remains to be studied. Future work could investigate its effectiveness across a broader range of tasks and contexts. Also, the current model requires fine-tuning for the visual knowledge extractor. Developing a model that can generalize well with prompt tuning or demonstration augmentation could be another interesting direction for future work. 7 Acknowledgments Carl Yang was supported by the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913. 10References [1] Harith Alani, Sanghee Kim, David E Millard, Mark J Weal, Wendy Hall, Paul H Lewis, and Nigel R Shadbolt. Automatic ontology-based knowledge extraction from web documents. IEEE Intell Syst, 18:14–21, 2003. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In NeurIPS, 2022. [3] Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them. https://github.com/rom1504/clip-retrieval, 2022. [4] Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. COMET: commonsense transformers for automatic knowledge graph construction. In ACL, 2019. [5] Aaron Chan, Jiashu Xu, Boyuan Long, Soumya Sanyal, Tanishq Gupta, and Xiang Ren. Salkg: Learning from knowledge graph explanations for commonsense reasoning. In NeurIPS, 2021. [6] Xuelu Chen, Ziniu Hu, and Yizhou Sun. Fuzzy logic based logical query answering on knowledge graphs. In AAAI, 2022. [7] Hejie Cui, Rongmei Lin, Nasser Zalmout, Chenwei Zhang, Jingbo Shang, Carl Yang, and Xian Li. PV2TEA: Patching visual modality to textual-established information extraction. In ACL, 2023. [8] Hejie Cui, Jiaying Lu, Yao Ge, and Carl Yang. How can graph neural networks help document retrieval: A case study on cord19 with concept map generation. In European Conference on IR Research (ECIR), 2022. [9] Hejie Cui, Jiaying Lu, Shiyu Wang, Ran Xu, Wenjing Ma, Shaojun Yu, Yue Yu, Xuan Kan, Tianfan Fu, Chen Ling, et al. A survey on knowledge graphs for healthcare: Resources, application progress, and promise. In ICML Workshop on Interpretable Machine Learning in Healthcare, 2023. [10] Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. Plausible may not be faithful: Probing object hallucination in vision-language pre-training. In EACL, 2023. [11] Yang Ding, Jing Yu, Bang Liu, Yue Hu, Mingxin Cui, and Qi Wu. Mukea: Multimodal knowledge extraction and accumulation for knowledge-based visual question answering. In CVPR, 2022. [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [13] James Fan, Aditya Kalyanpur, David C Gondek, and David A Ferrucci. Automatic knowledge extraction from documents. IBM J Res Dev, 56:5–1, 2012. [14] Mikhail Galkin, Zhaocheng Zhu, Hongyu Ren, and Jian Tang. Inductive logical query answering in knowledge graphs. In NeurIPS, 2022. [15] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In ACL-IJCNLP, 2021. [16] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven CH Hoi. From images to textual prompts: Zero-shot vqa with frozen large language models. In CVPR, 2023. [17] Tao He, Lianli Gao, Jingkuan Song, and Yuan-Fang Li. Towards open-vocabulary scene graph generation with prompt-based finetuning. In ECCV, 2022. 11[18] Andreas Holzinger, Peter Kieseberg, Edgar Weippl, and A Min Tjoa. Current advances, trends and challenges of machine learning and knowledge extraction: from machine learning to explainable ai. In CD-MAKE, 2018. [19] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Comput Surv, 55:1–38, 2023. [20] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In CVPR, 2016. [21] Xuan Kan, Hejie Cui, and Carl Yang. Zero-shot scene graph relation prediction through commonsense knowledge integration. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2021. [22] Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. Dense relational captioning: Triple-stream networks for relationship-based captioning. In CVPR, 2019. [23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. [24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123:32–73, 2017. [25] Hunter Lang, Monica N Agrawal, Yoon Kim, and David Sontag. Co-training improves prompt- based learning for large language models. In ICML, 2022. [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language- image pre-training for unified vision-language understanding and generation. In ICML, 2022. [28] Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare V oss, et al. Gaia: A fine-grained multimedia knowledge extraction system. In ACL, 2020. [29] Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and David S Rosenblum. Mmkg: multi-modal knowledge graphs. In ESWC, 2019. [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [31] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. NeurIPS, 2022. [32] Timo Lüddecke and Alexander Ecker. Image segmentation using text and image prompts. In CVPR, 2022. [33] Sachit Menon and Carl V ondrick. Visual classification via description from large language models. In ICLR, 2023. [34] Jiaul H Paik. A novel tf-idf weighting scheme for effective ranking. In SIGIR, 2013. [35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019. [36] Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and Aniruddha Kembhavi. Grounded situation recognition. In ECCV, 2020. 12[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. NeurIPS, 2015. [39] Fereshteh Sadeghi, Santosh K Kumar Divvala, and Ali Farhadi. Viske: Visual knowledge extraction and question answering by visual verification of relation phrases. In CVPR, 2015. [40] Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. Atomic: An atlas of machine commonsense for if-then reasoning. In AAAI, 2019. [41] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In AAAI, 2017. [42] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. In ICLR, 2020. [43] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive framework for neural text generation. In NeurIPS, 2022. [44] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased scene graph generation from biased training. In CVPR, 2020. [45] Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning. In ICML, 2020. [46] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. NeurIPS, 2021. [47] Chenguang Wang, Xiao Liu, and Dawn Song. Language models are open knowledge graphs. arXiv preprint arXiv:2010.11967, 2020. [48] Meng Wang, Sen Wang, Han Yang, Zheng Zhang, Xi Chen, and Guilin Qi. Is visual context really helpful for knowledge graph? a representation learning perspective. In ACM MM, 2021. [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. [50] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training, 2019. [51] Danfei Xu, Yuke Zhu, Christopher Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In CVPR, 2017. [52] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In CVPR, 2017. [53] Ran Xu, Yue Yu, Hejie Cui, Xuan Kan, Yanqiao Zhu, Joyce Ho, Chao Zhang, and Carl Yang. Neighborhood-regularized self-training for learning with few labels. In AAAI, 2023. [54] Ran Xu, Yue Yu, Joyce Ho, and Carl Yang. Weakly-supervised scientific document classification via retrieval-augmented multi-stage training. In SIGIR, pages 2501–2505, 2023. [55] Ran Xu, Yue Yu, Chao Zhang, Mohammed K Ali, Joyce C Ho, and Carl Yang. Counterfactual and factual reasoning over hypergraphs for interpretable clinical predictions on ehr. In Machine Learning for Health, 2022. [56] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. Generative data augmenta- tion for commonsense reasoning. In Findings of EMNLP, 2020. 13[57] Yue Yu, Kexin Huang, Chao Zhang, Lucas M Glass, Jimeng Sun, and Cao Xiao. Sumgnn: multi- typed drug interaction prediction via efficient knowledge graph summarization. Bioinformatics, 2021. [58] Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen, and Chao Zhang. Cold-start data selection for few-shot language model fine-tuning: A prompt-based uncertainty propagation approach. arXiv preprint arXiv:2209.06995, 2022. [59] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. Large language model as attributed training data generator: A tale of diversity and bias. arXiv preprint arXiv:2306.15895, 2023. [60] Alireza Zareian, Svebor Karaman, and Shih-Fu Chang. Bridging knowledge graphs to generate scene graphs. In ECCV, 2020. [61] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. NeurIPS, 2022. [62] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In CVPR, 2019. [63] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global context. In CVPR, 2018. [64] Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, and Cane Wing-Ki Leung. Aser: A large-scale eventuality knowledge graph. In WWW, 2020. [65] Yiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, and Yin Li. Comprehensive image captioning via scene graph decomposition. In ECCV, 2020. [66] Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Prompt consistency for zero-shot task generalization. In Findings of EMNLP, 2022. [67] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022. 14A Details of Data Augmentation with External Knowledge Resources ✔ Enhance Relation Recognition: We enriched the relationships between objects parsed from the original knowledge descriptions by leveraging the external resource of ConceptNet. ConceptNet comprises commonly observed entities and their connections, where edge weights signify the re- liability and frequency of these relationships. The typical value of edge weights in ConceptNet is 1. To prevent the redundancy of common information and to maintain the validity of the enriched relations, we categorized the relationships based on their weights. Relationships with weights less than 1 were deemed “weak” and those with a weight of 1 were labeled “average”. We refrained from using these categories for relation enhancement. Instead, only relationships with weights greater than 1, indicative of high reliability, were employed for augmenting the relations. ✔ Boost Entity Perception: On the entity side, we augment complement entities and descriptive information with two external knowledge resources. On one hand, for descriptions with a high TF- IDF+ score, we enrich related entities of the object from ConceptNet to create additional knowledge descriptions. The relatedness is based on the between-word relatedness score provided by ConceptNet and we take the threshold as 0.85. On the other hand, we employ the Commonsense Transformers (COMET) [4] model to enrich related new objects and descriptive information. The COMET model is a language model designed to generate commonsense knowledge and understand causal relationships between descriptions. It is pretrained using the atomic dataset, which consists of structured, crowd- sourced knowledge about everyday events and their associated causes and effects. The COMET model can provide neighbor descriptions of the given input of nine different categories of relation. We take the xAttr and oEffect relation categories and augmented the COMET model by formulating the existing knowledge description texts as the input and choose the corresponding category branch during generation for enriching objects and descriptions respectively. B Dataset Information Table 8: Dataset statistics. split #image #descriptor #relation #subject & object Train 75,456 832,351 30,241 302,735 Validation 4,871 64,137 5,164 34,177 Test 4,873 62,579 5,031 32,384 The statistic information of our augmented dataset is summarized in Table 8, wheresplit specifies the dataset split, #image indicates the number of images in the split, #descriptor indicates the total number of relational descriptors of the images, #relation is the total number of unique relations in the relational descriptors after deduplication, and #subject & object is the total number of subjects and objects contained in the description text. C Implementation Details Hyperparameter Assignment batch size 4 learning rate optimizer Adam Adam epsilon 1e-8 Adam initial learning rate 1e-5 learning rate scheduler cosine scheduler Adam decay weight 0.05 Table 9: Hyperparameters for training open relational region detector. Hyperparameter Assignment batch size 4 learning rate optimizer Adam Adam epsilon 1e-8 Adam initial learning rate 1e-5 learning rate scheduler cosine scheduler Adam decay weight 0.05 α 0.7 ϕ 0.01 Table 10: Hyperparameters for training format-free visual knowledge generator. Open relational region detector. The visual feature extraction backbone is constructed upon a pre-trained ResNet50-FPN. The detector head incorporates a BLIPbase equipped with the essential 15ViT-B/16 for text supervision, using multiple fully connected layers to derive region features. For each candidate region, we engage a regressor to conduct boundary regression on these features. The detector undergoes fine-tuning for 20 epochs using the relational region bounding box dataset and an Adam optimizer [30]. The hyperparameters for training are detailed in Table 9. Format-free visual knowledge generator. The format-free visual knowledge generator is initialized from BLIPbase, which incorporates the basic ViT-B/16. We fine-tune the generator model for 20 epochs using the same optimizer as the one employed for the region detector. Detailed hyperparameters for the visual knowledge generator can be found in Table 10. D Human Evaluation Guidance and Interface We perform the human evaluation on two of the four in-depth knowledge quality assessment metrics. We build an interface by referring to [ 50], where raters are presented with a given image and the corresponding knowledge descriptions and are required to choose one from the multiple choice for two questions on whether the knowledge is valid to humans and whether the knowledge description depicts the image. The detailed scoring criteria for Validity and Conformity are provided below: • Validity (↑): whether the generated visual knowledge is valid to humans. – 0 (Invalid): The knowledge description does not conform to human cognition, rendering it unreliable or misleading to humans. – 1 (Valid): The knowledge description is valid and accurately conforms to human cognition, providing reliable and meaningful knowledge to humans. • Conformity (↑): whether the generated knowledge faithfully depicts the scenarios in the images. – 0 (Inconsistent): The knowledge description does not faithfully depict the scenarios in the images, showing significant deviations or discrepancies, making it difficult for users to relate the textual information to the visual context. – 1 (Partially Conforming): The knowledge description partially conforms to the scenarios in the images, but there might be minor inconsistencies or missing relevant details. – 2 (Moderately Conforming): The knowledge description exhibits a moderate level of con- formity with the scenarios in the images, capturing the key aspects and providing coherent descriptions. – 3 (Highly Conforming): The knowledge description highly conforms to the scenarios in the images, accurately capturing the details and faithfully representing the visual context. Figure 9: The human evaluation interface for in-depth knowledge quality evaluation. Agreement/validation We use Cohen’sκ as the agreement score to measure potential subjectivity involved in ratings of knowledge quality. Cohen’sκ is a statistic that is used to measure inter-rater reliability for qualitative items and is scaled from -1 (perfect systematic disagreement) to 1 (perfect agreement), where values ≤ 0 as indicating no agreement and 0.01-0.20 as none to slight, 0.21-0.40 as fair, 0.41–0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1.00 as almost perfect agreement. Our calculated average pairwise Cohen’s κ on human evaluation results from three different raters is 0.76, which indicates a good agreement. 16E Parametric Knowledge Prompting Template Given an image I and the corresponding extracted visual knowledge from it based on OpenVik, we perform knowledge comparison with parametric knowledge contained in LLM by prompting the gpt-3.5-turbo model with the object information contained in the I. The prompt format is shown in the followings: Suppose you are looking at an image that contains the following subject and object entities: Subject list: [Insert the subject names here] Object list: [Insert the object names here] Please extract 5-10 condensed descriptions that describe the interactions and/or relations among those entities in the image. Try to elucidate the associations and relationships with diverse language formats instead of being restricted to sub-verb-obj tuples. F More Case Studies of Open Visual Knowledge from OpenVik Figure 10 shows some other cases on the extracted open visual knowledge from OpenVik. In comparison to VG and Relational Caps, OpenVik exhibits superior performance at capturing novel entities , expanding object interactions through diverse relations , and enriching knowledge repre- sentation with nuanced descriptive details . For example for the bottom right image, OpenVik can extract novel entities such as “ tracks ”, “ shoe ”, diverse relations such as “ sticking out of ”, and nuanced descriptive details such as “ cold thick ”, “ with man feet on it ”, “ brave ”. The generated knowledge with a more format-free semantic structure is highlighted in red. Visual Genome-Scene Graph: <kite, in, sky> <people, in, water> <board, with, person> <shirt, on, person> Visual Genome-Region Descriptions: the person wearing white shirt the woman on beach the person standing on beach the person in water Relational Caps: a blue board with a person someone built a sand castle the cloudy sky behind the kit white clouds in blue sky OpenVik:  yellow tail of a red and yellow kite the people enjoying fun beach playing sands flying a long tail kite has orange appearance the woman running to flying kite excitedly flying kite in the open blue sky the man looking at orange kite OpenVik:  white ground covered with cold thick snow brown grass sticking out of snow-covered field the tracks in fresh snow with man feet on it the shadow cast by brave skier skiing man wearing yellow shoe young skier has red hat and smiling Visual Genome-Scene Graph: <tree, in, field> <hat, on, head> <grass, on, ground> <shadow, of, skier> Visual Genome-Region Descriptions: snow divot in the hill writing on the pants lens on the sunglass ski attached to right foot Relational Caps: downhill skier wearing jacket the ski pole stuck in snow red hat worn on head the weeds in snow Visual Genome-Scene Graph: <woman, has, hair> <person, wear, hat> <leaf, on, plant> <man, in, suit> Visual Genome-Region Descriptions: top of brown umbrella man in a suit man under an umbrella the people are enjoying their day Relational Caps: the people close to building the people are walking in a city the woman wearing blue jeans the woman has glasses OpenVik:  green shrubbery growing along sidewalk  purple parasol in rain holding by people green trees in distance growing aside green bush on side of building people are trying not to get wet the woman look back in astonishment OpenVik:  white and brown egg inside cardboard box the kale bunch wrapped using rope green lettuce on crowded table white table cloth has flowers decorated purple onion next to oranges very bright red polka dots on cloth Visual Genome-Scene Graph: <orange, on, table> <egg, in, carton> <pea, in, bag> <onion, next to, limes> Visual Genome-Region Descriptions: a white colored egg seven oranges on a table green onion stalks that have been cut bunch of kale on table  Relational Caps: the half dozen of white eggs green vegetables on table purple onion between orange the eggs in carton Figure 10: Case studies of open visual knowledge from OpenVik. 17G More Qualitative Examples on Applications G.1 Text-to-Image Retrieval Original text:A row of parked motorcycles sitting in front of a tall building. A stone street with bicycles and motor bikes parked on the side and people standing on the sidewalks in front of buildings. Cityscape of pedestrians enjoying an old European city. a row of bikes and mopeds is parked along the street. Motorcycles and mopeds line a side street during the day in a city. Original text:Three young men playing Wii on a projection television. Three men laughing at some pictures from a projector. A group of gentleman playing video games in a dimly lit room. Some people chilling on the couch playing with a Nintendo Wii. A group of men playing a game with remote controllers. Enriched text: men in group. men behind people. men playing. men in room playing video game. group of people. men in group are playing video game. people playing. people watching game. playing game. Enriched text:row made of stone leading into city. motor in row. row of people. street made of stone. wall made of stone next to side. stone wall behind people. people in line crossing street. street in city. motor on side. people riding motor in city. motor in line. people in line in city. day at city.Original text:An elderly woman sitting on the bench resting. An old woman leans on her back while sitting on an ornate bench. A woman is sitting on a bench near a fence. Older woman in dress sitting on a park bench. An old woman sitting on a bench next to a fence. Original text: A herd of cattle is feeding at the river's edge. Many cows next to a body of water in a field. A herd of cowsgrazes in a field near a river. A herd of cattle standing in grassy area next to water. A herd of cattle is near a flock of birds swimming in the water.Enriched text:woman sitting on bench with aornate. woman behind fence. woman wearing dress. woman in park. bench by fence. bench in park. woman in ornate dress on the bench. fence behind park. Enriched text:herd of cattle crossing river. herd traveling by water. cattle crossing river. cattle in field. river across field in front of area. water near field. water near area. water next to flock. Birds inside of water. flock in field.  Original text: A man is leaning over a fence offering food to an elephant. A man reaching out to an elephantstrunk near a gate. A man is feeding an elephant over a fence. A man handing an elephant a stick in an enclosure at a zoo. A man reaches out to give the elephant something.  Original text:A white refrigerator freezer sitting inside of a kitchen. A corner of a kitchen with a big fridge. A kitchen has a plain white fridge in the corner. A refrigerator in the corner of a kitchen just off the dining room a room showing a very big fridge and a dining table. Enriched text: refrigerator has freezer. refrigerator in corner. refrigerator inbrightkitchen.refrigerator in room. refrigerator next to table sitting in kitchen. freezer next to table. corner window in room. corner of table. fridge in kitchen. table in kitchen. fridge table next to table in room.Enriched test:man behind fence. mannext to trunk preparing food.  man holding stick in enclosure. man pointing at something. fence truck behind food. fence wrapped around trunk. fence behind elephant. fence made of stick. fence surrounds enclosure. trunk of elephant. elephant in enclosure.  Figure 11: Qualitative examples of OpenVik context enrichment on text-to-image retrieval. Figure 11 presents more qualitative examples of OpenVik-based visual knowledge enrichment on captions. The enriched text is based on the objects present in the images themselves, supplemented with additional relationships from our generated visual knowledge in OpenVik. It is shown that the introduced relationships often provide new context information that aligns with the visual content of the images. For example, in the image of an old woman sitting on a bench in a park, the enriched context information includes the positional relationship between the “bench”, “fence”, and “park”, which provides a more comprehensive description of the original image. G.2 Grounded Situation Recognition Figure 12 presents more qualitative examples of OpenVik-based context enrichment in the grounded situation recognition (GSR) task. Our context enrichment setting for the GSR task is to perform enrichment based on verbs like “shopping” and “carrying”. We further restrict the enriched context with the objects contained in the image to avoid noisy enrichment. For example, for the image showing people shopping at a market, the enriched knowledge contexts could be “the people shopping at market”, “standing person shopping for fruit”. The idea is to enrich the original description T : “An image of <verb>” with relevant actions and relations with the extracted visual knowledge from OpenVik, which can potentially help in drawing-in the matched candidates. G.3 Visual Commonsense Reasoning Figure 13 presents more qualitative examples of OpenVik-based context enrichment in the visual commonsense reasoning (VCR) task. The context enrichment on VCR is performed at two-level, 18Verb:shoppingOrigin text:This is an image of shoppingEnrich text:the people shopping at market. standing person shopping for fruit. Verb: sprayingOriginal text:This is an image of spraying.Enriched text: the water spraying from fountain. the water spraying from spout. the water spraying in park.  Verb:walkingOriginal text: This is an image of walking.Enriched text: the person walking through forest. the people walking on sidewalk.  Verb: typingOriginal text:This is an image of typing.Enriched text:sitting woman typing on smart open laptop.  Verb: carvingOriginal text:This is an image of carving. Enriched text:wood carving in center. man carving wood.   Verb:carryingOriginal text:This is an image of carrying.Enriched text:walking person carrying bag. man carrying hay in the field.  Verb:lickingOriginal text:This is an image of licking. Enriched text:black dog licking food. Figure 12: Qualitative examples of OpenVik context enrichment on task GSR. incorporating both entities and relations: (1) we parse the question and options to obtain all (S, O) pairs and, for each entity pair, apply the same relation augmentation as in the image retrieval task; (2) for the V in each option, we enrich the visual context using the same method as illustrated in GSR. It is shown that unrelated answers are usually enriched with contexts that are not relevant to the image, thus enlarging the distance between incorrect answers and the question, e.g., the enriched contexts “squating person fixing handy bathroom” for example 3 in Figure 13. At the same time, the knowledge description of the correct answer is enhanced by incorporating information that aligns with the image contents, e.g., the enriched knowledge contexts “sitting people on red ground” for example 1 in Figure 13. H Full List of Filtered Verbs for GSR We provide the full list of verbs out of the predefined 504 candidates of GSR [ 36] that can be accurate-matched or fuzzy-matched to extracted visual knowledge in Table 11, based on which we compose the testing subset for our evaluation on GSR application in Section 5.2. 19Question: What will  Person2do next? APerson2will speak angrily at  diningtable2, then walk off. BPerson2will sit down on  chair1. painting person near giant chair.CPerson2will feed  bowl1. the person skate boardingin aathletic bowl.DPerson2will open the box. the person holding a box full of oranges.Answer: BPerson2will sit down on  chair1.  Question:Why is  Person7in motion? APerson14is running desperately. BPerson7is climbing over the boat. the person standing inside white boat.CPerson7is walking fast to the bathroom. squating person fixing handy bathroom.DPerson7is going to try to protect  Person10from a threat. Person7is moving forward to challenge what ever could be there. Answer: BPerson7is climbing over the boat. Question:Where is’Person1sitting?AHe is in a laboratory. BHe is sitting at a bar. the person sitting behind sneaky barrier.CIn a fort in his house. the person walking by light house.DHe is sitting on the ground. sitting person on redground.Answer: D He is sitting on the ground.  Question:Where is Person2going?APerson2is going into the store. the person walking into store.BPerson2is getting into a carriage. sitting person inside carriage. CPerson1is going to the bathroom.squating person fixing handy bathroom. DPerson1is going outside to play after the conversation withPerson2is over.Answer: APerson2is going into the store.  Question: WherearePerson1and Person2?APerson1and Person2aresittingoutsideof a general store. the person walking by store.BPerson1and Person2are standing on top of a train car. jumping person on top board.walking person next to white train.the person walking near active car.yellow train sitting atop track. sliced carrot on top counterred car of old train.CPerson1and Person2are in an office.walking person outside office.DPerson1and Person2are in the kitchen. the person eating in hungry kitchen.Answer: CPerson1and Person2are in an office. Question:What is  Person1doing here? AHe is in prison serving a prison sentence. person writing sentences.BHe is trying to get information. person gaining information.CPerson1is a waiter. person talking with waiter in restaurant.DHe is existing a building. walking person near large building.Answer: CPerson1is a waiter. Figure 13: Qualitative examples of OpenVik context enrichment on task VCR. 20Table 11: The full list of filtered verbs for GSR. Matching Type The Word List of Event Types Accurate putting, butting, bathing, dusting, rearing, turning, skating, placing, carting, staring, biting, mashing, folding, wetting, sprinkling, branch- ing, drying, standing, flaming, taxiing, performing, circling, molding, parachuting, glowing, fishing, drinking, speaking, pawing, blocking, milking, racing, stripping, potting, spinning, eating, making, kicking, catching, lacing, urinating, sleeping, pressing, buttering, shearing, slid- ing, hiking, glaring, dipping, swimming, shopping, slicing, shelling, wag- ging, grilling, crafting, raining, clawing, splashing, rubbing, snowing, breaking, guarding, clipping, sewing, braiding, telephoning, buttoning, waiting, serving, picking, camping, leaning, working, kissing, wrapping, trimming, tripping, pasting, soaring, driving, kneeling, pumping, col- oring, lighting, training, ducking, bowing, arching, cooking, checking, pushing, flipping, rocking, cresting, cleaning, reading, nailing, stitching, building, climbing, covering, shelving, attaching, calming, selling, glu- ing, dyeing, lapping, photographing, peeling, sprouting, licking, display- ing, combing, stacking, planting, fastening, buying, mopping, burning, erasing, measuring, dining, tattooing, gardening, decorating, clearing, fixing, weeding, pulling, feeding, watering, crowning, shaking, dripping, emptying, typing, chasing, poking, leaping, pouring, hanging, sniffing, piloting, falling, overflowing, resting, crashing, carving, ballooning, wad- ing, loading, shaving, boarding, pinning, rowing, juggling, shoveling, hugging, throwing, calling, singing, carrying, walking, writing, crouch- ing, floating, painting, opening, tying, riding, strapping, dialing, saying, bubbling, signing, camouflaging, operating, leading, laughing, parading, skiing, drawing, gnawing, celebrating, spreading, filling, giving, running, smelling, plowing, helping, brushing, scooping, adjusting, wrinkling, steering, biking, smiling, spraying, boating, paying, chewing, stuffing, clinging, landing, wheeling, talking, scoring, teaching, jogging, pitching, flapping, tipping, scrubbing, sitting, surfing, stirring, competing, drum- ming, jumping, filming, dancing, waxing, hitting, recording, baking, waving, washing, signaling, chopping, stretching, rafting, microwaving, phoning, lifting, swinging, releasing, ramming, towing, packing, hauling, frying (244 words) Fuzzy educating, marching, spanking, descending, smearing, heaving, cram- ming, inflating, stooping, inserting, squeezing, tugging, tilting, moisten- ing, swarming, subduing, waddling, winking, flexing, punching, attack- ing, nuzzling, sprinting, sucking, puckering, sketching, rotting, video- taping, complaining, tuning, locking, hurling, pricking, arranging, con- structing, slapping, sweeping, restraining, dousing, frisking, twisting, wringing, hoisting, immersing, shredding, blossoming, igniting, spying, offering, pouting, confronting, docking, assembling, prying, grinning, sharpening, pruning, disciplining, nipping, coaching, nagging, storming, handcuffing, apprehending, bouncing, clenching, taping, distributing, striking, studying, plunging, curling, aiming, sowing, grinding, rinsing, punting, mowing, hitchhiking, skipping, leaking, providing, hunching, spoiling, kneading, burying, foraging, lathering, vaulting, ejecting, mend- ing, pinching, deflecting, ascending, peeing, bothering, repairing, ped- aling, ailing, fueling, skidding, scraping, soaking, grimacing, scolding, spitting, knocking, crushing, bandaging, saluting, fording, stumbling, discussing, raking, launching, whirling, fetching, brawling, retrieving, snuggling, exercising, colliding, stroking, whipping, tilling, betting, farming, browsing, examining, dropping, barbecuing, ignoring, asking, flinging, perspiring, embracing, slipping, flicking, smashing, arresting, lecturing, tearing, gasping, applying, counting, spilling, dragging, recov- ering, practicing, scratching, shooting, packaging, hunting, stinging (154 words) 21",
      "references": [
        "Automatic ontology-based knowledge extraction from web documents.",
        "Flamingo: a visual language model for few-shot learning.",
        "Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them.",
        "COMET: commonsense transformers for automatic knowledge graph construction.",
        "Salkg: Learning from knowledge graph explanations for commonsense reasoning.",
        "Fuzzy logic based logical query answering on knowledge graphs.",
        "PV2TEA: Patching visual modality to textual-established information extraction.",
        "How can graph neural networks help document retrieval: A case study on cord19 with concept map generation.",
        "A survey on knowledge graphs for healthcare: Resources, application progress, and promise.",
        "Plausible may not be faithful: Probing object hallucination in vision-language pre-training.",
        "Mukea: Multimodal knowledge extraction and accumulation for knowledge-based visual question answering.",
        "An image is worth 16x16 words: Transformers for image recognition at scale.",
        "Automatic knowledge extraction from documents.",
        "Inductive logical query answering in knowledge graphs.",
        "Making pre-trained language models better few-shot learners.",
        "From images to textual prompts: Zero-shot vqa with frozen large language models.",
        "Towards open-vocabulary scene graph generation with prompt-based finetuning.",
        "Current advances, trends and challenges of machine learning and knowledge extraction: from machine learning to explainable ai.",
        "Survey of hallucination in natural language generation.",
        "Densecap: Fully convolutional localization networks for dense captioning.",
        "Zero-shot scene graph relation prediction through commonsense knowledge integration.",
        "Dense relational captioning: Triple-stream networks for relationship-based captioning.",
        "Large language models are zero-shot reasoners.",
        "Visual genome: Connecting language and vision using crowdsourced dense image annotations.",
        "Co-training improves prompt- based learning for large language models.",
        "Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models.",
        "BLIP: bootstrapping language- image pre-training for unified vision-language understanding and generation.",
        "Gaia: A fine-grained multimedia knowledge extraction system.",
        "Mmkg: multi-modal knowledge graphs.",
        "Decoupled weight decay regularization.",
        "Quark: Controllable text generation with reinforced unlearning.",
        "Image segmentation using text and image prompts.",
        "Visual classification via description from large language models.",
        "A novel tf-idf weighting scheme for effective ranking.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Grounded situation recognition.",
        "Learning transferable visual models from natural language supervision.",
        "Faster r-cnn: Towards real-time object detection with region proposal networks.",
        "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases.",
        "Atomic: An atlas of machine commonsense for if-then reasoning.",
        "Conceptnet 5.5: An open multilingual graph of general knowledge.",
        "Vl-bert: Pre-training of generic visual-linguistic representations.",
        "A contrastive framework for neural text generation.",
        "Unbiased scene graph generation from biased training.",
        "Inductive relation prediction by subgraph reasoning.",
        "Multimodal few-shot learning with frozen language models.",
        "Language models are open knowledge graphs.",
        "Is visual context really helpful for knowledge graph? a representation learning perspective.",
        "Chain-of-thought prompting elicits reasoning in large language models.",
        "Neural text generation with unlikelihood training,",
        "Scene graph generation by iterative message passing.",
        "Bridging knowledge graphs to generate scene graphs.",
        "Star: Bootstrapping reasoning with reasoning.",
        "From recognition to cognition: Visual commonsense reasoning.",
        "Neural motifs: Scene graph parsing with global context.",
        "Aser: A large-scale eventuality knowledge graph.",
        "Comprehensive image captioning via scene graph decomposition.",
        "Neighborhood-regularized self-training for learning with few labels.",
        "Weakly-supervised scientific document classification via retrieval-augmented multi-stage training.",
        "Counterfactual and factual reasoning over hypergraphs for interpretable clinical predictions on ehr.",
        "Generative data augmenta- tion for commonsense reasoning.",
        "Sumgnn: multi- typed drug interaction prediction via efficient knowledge graph summarization.",
        "Cold-start data selection for few-shot language model fine-tuning: A prompt-based uncertainty propagation approach.",
        "Large language model as attributed training data generator: A tale of diversity and bias.",
        "Prompt consistency for zero-shot task generalization.",
        "Conditional prompt learning for vision-language models."
      ],
      "meta_data": {
        "arxiv_id": "2310.18804v1",
        "authors": [
          "Hejie Cui",
          "Xinyu Fang",
          "Zihan Zhang",
          "Ran Xu",
          "Xuan Kan",
          "Xin Liu",
          "Yue Yu",
          "Manling Li",
          "Yangqiu Song",
          "Carl Yang"
        ],
        "published_date": "2023-10-28T20:09:29Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes a new paradigm for open visual knowledge extraction (OpenVik) that outputs relation-oriented but format-free knowledge from images, overcoming prior constraints of fixed tuple formats (e.g., ⟨sub,rel,obj⟩) and closed relation vocabularies. Key contributions: (1) an open relational region detector that predicts relation-centric regions likely to contain interactions/relational knowledge; (2) a format-free visual knowledge generator that prompts/fine-tunes a large multimodal model conditioned on detected regions to generate free-form knowledge phrases; (3) diversity-driven data enhancement to mitigate long-tail and bias in existing supervision by TF-IDF+-guided random dropping and augmentation with external commonsense resources; (4) extensive evaluation showing high correctness, novelty, and diversity of extracted knowledge and consistent gains when integrating this knowledge into downstream visual reasoning tasks (text-to-image retrieval, grounded situation recognition, visual commonsense reasoning).",
        "methodology": "OpenVik is a two-stage framework trained separately and composed at inference.\n1) Open relational region detector: adapts Faster R-CNN with ResNet50-FPN. Instead of object boxes, it regresses relation-centric boxes formed by the union of subject and object boxes from ground-truth relational descriptions (region regression loss L_RD). It replaces standard object classification with knowledge supervision: a pretrained regional text generator is fine-tuned to produce the region description given the region, optimized with cross-entropy L_K. Detector objective: L_v = L_RD + L_K. It outputs a flexible set of high-confidence relational regions (max 30) per image.\n2) Format-free visual knowledge generator: built on BLIPbase (ViT-B/16 encoder + BLIP image-grounded text decoder). Fine-tuned with detected region as a binary mask prompt over visual tokens/attention to focus on relational foreground. Trained with language modeling loss L_MLE plus an inter-sequence variety regularizer L_V that penalizes high semantic cosine similarity between generated phrases for the same image beyond a small threshold φ=0.01. Overall generator objective: L_l = α L_MLE + (1-α) L_V with α=0.7. At inference, uses contrastive decoding to further promote within-sequence diversity.\n3) Diversity-driven data enhancement: addresses biased long-tail relation distribution using a TF-IDF+ score S_r=(log(N/(1+f_r·α1)))^α2 (grid-scaled) to quantify relation rarity/importance. (a) Random dropping removes redundant same-image descriptions and probabilistically drops low-score frequent/less-informative relations to reduce bias, keeping ~60% of original. (b) External augmentation for high-score relations: uses ConceptNet edges (filtered to weight>1, and node relatedness>0.85 for entity enrichment) to add plausible relation phrases between parsed object pairs; and uses COMET (xAttr, oEffect branches) to add attributes/new entities and descriptive expansions.",
        "experimental_setup": "Training data: constructed from Visual Genome and Dense Relational Captioning (Relational Caps). Each relational descriptor includes free-form description text, subject/object names, relation label, and subject/object boxes; relation-centric training boxes are unions of subject/object boxes. Dataset stats (after augmentation): Train 75,456 images / 832,351 descriptors / 30,241 unique relations; Val 4,871 images / 64,137 descriptors; Test 4,873 images / 62,579 descriptors.\nModels/training: implemented in PyTorch; trained on 2× Quadro RTX 8000 GPUs. Detector: ResNet50-FPN Faster R-CNN initialized from pretrained backbone, fine-tuned 20 epochs; max 30 boxes per image at inference. Generator: initialized from BLIPbase (ViT-B/16), fine-tuned 20 epochs. Optimizer Adam with lr 1e-5, cosine scheduler, weight decay 0.05, batch size 4.\nKnowledge evaluation: compares to baselines from (i) scene graph generation (IMP, Neural Motifs, UnbiasSGG, Ov-SGG), (ii) dense relational captioning (MTTSNet+REM), (iii) region/dense captioning (DenseCap, Sub-GC, BLIP, BLIP2). Automatic generation metrics: BLEU, ROUGE-L, METEOR. In-depth quality metrics: Validity (human), Conformity (human 0–3 scale), Freshness (percent not in training set), Diversity (pairwise language variance). Human eval on 100 sampled images; 3 raters; average Cohen’s κ=0.76.\nKnowledge-source comparison: maps OpenVik phrases (parsed to triplets) to ConceptNet edges and to GPT-3.5-generated parametric knowledge using embedding similarity threshold 0.75.\nDownstream applications (zero-shot settings):\n- Text-to-image retrieval on a curated 680-image MS-COCO subset; baseline ZS-CLIP retrieval and BLIP; caption enrichment by adding frequent relations between parsed subject-object pairs; metrics Recall@1/5/10 and average.\n- Grounded Situation Recognition (GSR): curated 900-sample subset; classify among 504 verbs by matching “An image of <verb>” prompts; OpenVik enriches verb prompts with synonym-matched common phrases; metrics Accuracy/Precision/Recall/F1.\n- Visual Commonsense Reasoning (VCR): 939-sample validation subset with minimum noun/relation counts; baselines R2C and BLIP; OpenVik enriches question/option text with entity-relation and verb-context augmentation; metrics Accuracy/Precision/Recall/F1.\nAblations: remove L_V, remove random dropping, remove entity/relation augmentation, train detector from scratch vs pretrained; analyze impacts on validity/conformity/freshness/diversity and dataset diversity.",
        "limitations": "Dependence on biased/noisy supervision: training relies on Visual Genome/Relational Caps whose long-tail and co-occurrence biases can cause hallucinations and incorrect inferences (e.g., generating objects not present) driven by detector mistakes and dataset confounders. Two-stage pipeline sensitivity: errors in relational region detection propagate to generation; overlapping/mislocalized regions reduce diversity and accuracy (noted especially without pretrained detector). Requires fine-tuning: current approach needs supervised fine-tuning of both detector and generator rather than being purely prompt-based, limiting ease of deployment and adaptation. Evaluation scope: scalability and robustness on larger-scale or more diverse datasets and broader tasks is not fully established; human evaluation is on a relatively small subset (100 images). External knowledge augmentation assumptions: mapping objects/relations to ConceptNet/COMET may introduce noise and presumes reliable parsing/mapping thresholds; may skew toward commonsense priors rather than image-grounded truth.",
        "future_research_directions": "Scale and generalize: evaluate and adapt OpenVik on larger, more diverse datasets (different domains, non-VG imagery) and assess robustness to distribution shifts and rare interactions. Reduce supervision/fine-tuning: explore prompt tuning, in-context demonstrations, or parameter-efficient adaptation to avoid full fine-tuning while retaining grounding and diversity. Improve region grounding: develop better relation-centric region proposal mechanisms (e.g., transformer-based detectors, phrase grounding, multi-region relational graphs) and uncertainty-aware selection to mitigate propagation errors and overlapping boxes. Hallucination/confounder mitigation: incorporate debiasing objectives, counterfactual training, negative sampling/unlikelihood, or verification modules to detect/avoid spurious co-occurrence-driven generations. Richer knowledge representations: extend from phrase-level to structured yet open schemas (hybrid free-form + latent graph), temporal/motion modeling for videos, and multi-hop reasoning across regions/images. Better evaluation: create standardized benchmarks for open visual knowledge extraction (coverage, grounding faithfulness, compositionality) and automatic metrics aligned with human judgments.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "MeshSDF: Differentiable Iso-Surface Extraction",
      "full_text": "MeshSDF: Differentiable Iso-Surface Extraction Edoardo Remelli ∗1 Artem Lukoianov ∗1,2 Stephan R. Richter 3 Benoît Guillard 1 Timur Bagautdinov 2 Pierre Baque 2 Pascal Fua 1 1CVLab, EPFL, {name.surname}@epfl.ch 2Neural Concept SA, {name.surname}@neuralconcept.com 3Intel Labs, {name.surname}@intel.com Abstract Geometric Deep Learning has recently made striking progress with the advent of continuous Deep Implicit Fields. They allow for detailed modeling of watertight surfaces of arbitrary topology while not relying on a 3D Euclidean grid, resulting in a learnable parameterization that is not limited in resolution. Unfortunately, these methods are often not suitable for applications that require an explicit mesh-based surface representation because converting an implicit ﬁeld to such a representation relies on the Marching Cubes algorithm, which cannot be differentiated with respect to the underlying implicit ﬁeld. In this work, we remove this limitation and introduce a differentiable way to pro- duce explicit surface mesh representations from Deep Signed Distance Functions. Our key insight is that by reasoning on how implicit ﬁeld perturbations impact local surface geometry, one can ultimately differentiate the 3D location of surface samples with respect to the underlying deep implicit ﬁeld. We exploit this to deﬁne MeshSDF, an end-to-end differentiable mesh representation which can vary its topology. We use two different applications to validate our theoretical insight: Single-View Reconstruction via Differentiable Rendering and Physically-Driven Shape Opti- mization. In both cases our differentiable parameterization gives us an edge over state-of-the-art algorithms. 1 Introduction Geometric Deep Learning has recently witnessed a breakthrough with the advent of continuous Deep Implicit Fields [39, 31, 8]. These enable detailed modeling of watertight surfaces, while not relying on a 3D Euclidean grid or meshes with ﬁxed topology, resulting in a learnable surface parameterization that is not limited in resolution. However, a number of important applications require explicit surface representations, such as tri- angulated meshes or 3D point clouds. Computational Fluid Dynamics (CFD) simulations and the associated learning-based surrogate methods used for shape design in many engineering ﬁelds [3, 54] are a good example of this where 3D meshes serve as boundary conditions for the Navier-Stokes Equations. Similarly, many advanced physically-based rendering engines require surface meshes to model the complex interactions of light and physical surfaces efﬁciently [37, 40]. Combining explicit representations with the beneﬁts of deep implicit ﬁelds requires converting the implicit surface parameterization to an explicit representation, which typically relies on one of ∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2006.03997v2  [cs.CV]  31 Oct 2020Image MeshSDF Raw Silhouette reﬁnement target silhouette MeshSDF Reﬁned MeshSDF Raw Drag minimization MeshSDF Reﬁned (a) (b) Figure 1: MeshSDF. (a) We condition our representation on an input image and output an initial 3D mesh, which we reﬁne via differentiable rasterization [24], thereby exploiting MeshSDF’s end- to-end differentiability. (b) We use our parameterization as a powerful regularizer for aerodynamic optimization tasks. Here, we start from an initial car shape and reﬁne it to minimize pressure drag. the many variants of the Marching Cubes algorithm [ 30, 36]. However, these approaches are not fully differentiable [26]. This effectively prevents the use of continuous Deep Implicit Fields as parameterizations when operating on explicit surface meshes. The non-differentiability of Marching Cubes has been addressed by learning differentiable approxi- mations of it [26, 56]. These techniques, however, remain limited to low-resolution meshes [26] or ﬁxed topologies [56]. An alternative approach has been to reformulate downstream tasks, such as differentiable rendering [21, 28] or surface reconstruction [32], directly in terms of implicit functions, so that explicit surface representations are no longer needed. However, doing so is not easy and may even not be possible for more complex tasks, such as solving CFD optimization problems. By contrast, we show that it is possible to usecontinuous signed distance functions to produce explicit surface representations while preserving differentiability. Our key insight is that 3D surface samples can be differentiated with respect to the underlying deep implicit ﬁeld. We prove this formally by reasoning about how implicit ﬁeld perturbations impact 3D surface geometry locally. Speciﬁcally, we derive a closed-form expression for the derivative of a surface sample with respect to the underlying implicit ﬁeld, which is independent of the method used to extract the iso-surface. This enables us to extract the explicit surface using a non-differentiable algorithm, such as Marching Cubes, and then perform our custom backward pass through the extracted surface samples, resulting in an end-to-end differentiable surface parameterization that can describe arbitrary topology and is not limited in resolution. We will refer to our approach as MeshSDF. We showcase the power and versatility of MeshSDF in the two different applications depicted by Fig. 1. First, we exploit end-to-end differentiability to reﬁne Single-View Reconstructions through differentiable surface rasterization [24]. Second, we use our parameterization as powerful regularizer in physically-driven shape optimization for CFD purposes [3]. We will demonstrate that in both cases our end-to-end differentiable parameterization gives us an edge over state-of-the art algorithms. In short, our core contribution is a theoretically well-grounded technique for differentiating through iso-surface extraction. This enables us to harness the full power of deep implicit surface representation to deﬁne an end-to-end differentiable surface mesh parameterization that allows topology changes. 2 Related Work From Discrete to Continuous Implicit Surface Models. Level sets of a 3D function effectively represent watertight surfaces with varying topology [47, 38]. As they can be represented on 3D grids and thus easily be processed by standard deep learning architectures, they have been an inspiration for many approaches [5, 11, 15, 44, 46, 50, 57, 58]. However, methods operating on dense grids have been limited to low resolution volumes due to excessive memory requirements. Methods operating on sparse representations of the grid tend to trade off the need for memory for a limited representation of ﬁne details and lack of generalisation [45, 46, 50, 51]. This has changed recently with the introduction of continuous deep implicit ﬁelds, which represent 3D shapes as level sets of deep networks that map 3D coordinates to a signed distance function [39] or occupancy ﬁeld [31, 8]. This yields a continuous shape representation wrt. 3D coordinates that is lightweight but not limited in resolution. This representation has been successfully used for single view reconstruction [31, 8, 60] and 3D shape completion [10]. 2However, for applications requiring explicit surface parameterizations, the non-differentiability of iso- surface extraction so far has largely prevented exploiting the advantages of implicit representations. Converting Implicit Functions to Surface Meshes. The Marching Cube (MC) algorithm [30, 36] is a widely adopted way of converting implicit functions to surface meshes. The algorithm proceeds by sampling the ﬁeld on a discrete 3D grid, detecting zero-crossing of the ﬁeld along grid edges, and building a surface mesh using a lookup table. Unfortunately, the process of determining the position of vertices on grid edges involves linear interpolation, which does not allow for topology changes through backpropagation [26], as illustrated in Fig. 2(a). Because this is a central motivation to this work, we provide a more detailed analysis in the Supplementary Section. In what follows, we discuss two classes of methods that tackle the non-differentiability issue. The ﬁrst one emulates iso-surface extraction with deep neural networks, while the second one avoids the need for mesh representations by formulating objectives directly in the implicit domain. Emulating Iso-Surface Extraction. Liao et al. [26] map voxelized point clouds to a probabilistic topology distribution and vertex locations deﬁned over a discrete 3D Euclidean grid through a 3D CNN. While this allows changes to surface topology through backpropagation, the probabilistic modelling requires keeping track of all possible topologies at the same time, which in practice limits resulting surfaces to low resolutions. V oxel2mesh [56] deforms a mesh primitive and adaptively increases its resolution. While this enables high resolution surface meshes, it prevents changes of topology. Reformulating Objective Functions in terms of Implicit Fields. In [33], variational analysis is used to re-formulate standard surface mesh priors, such as those that enforce smoothness, in terms of implicit ﬁelds. Although elegant, this technique requires carrying out complex derivations for each new loss function and can only operate on an Euclidean grid of ﬁxed resolution. The differentiable renderers of [22, 29] rely on sphere tracing and operate directly in terms of implicit ﬁelds. Unfortunately, since it is computationally intractable to densely sample the underlying volume, these approaches either deﬁne implicit ﬁelds over a low-resolution Euclidean grid [ 22] or rely on heuristics to accelerate ray-tracing [ 29], trading off in accuracy. 3D volume sampling efﬁciency can be improved by introducing a sparse set of anchor points when performing ray-tracing [ 27]. However, this requires reformulating standard surface mesh regularizers in terms of implicit ﬁelds using computationally intensive ﬁnite differences. Furthermore, these approaches [22, 27, 29] are tailored to differentiable rendering, and are not directly applicable to different settings that require explicit surface modeling, such as computational ﬂuid dynamics. 3 Method Tasks such as Single-View Reconstruction (SVR) [23, 19] or shape design in the context of CFD [3] are commonly performed by deforming the shape of a 3D surface mesh M = ( V,F ), where V = {v1,v2,...}denotes vertex positions in R3 and F facets, to minimize a task-speciﬁc loss function Ltask(M). Ltask can be, e.g., an image-based loss deﬁned on the output of a differentiable renderer for SVR or a measure of aerodynamic performance for CFD. To perform surface mesh optimization robustly, a common practice is to rely on low-dimensional parameterizations that are either learned [ 4, 39, 2] or hand-crafted [ 3, 54, 43]. In that setting, a differentiable function maps a low-dimensional set of parameters z to vertex coordinates V, implying a ﬁxed topology. Allowing changes of topology, an implicit surface representation would pose a compelling alternative but conversely require adifferentiable conversion to explicit representations in order to backpropagate gradients of Ltask. In the remainder of this section, we ﬁrst recapitulate deep Signed Distance Functions, which form the basis of our approach. We then introduce our main contribution, a differentiable approach to computing surface samples and updating their 3D coordinates to optimize Ltask. Finally, we present MeshSDF, a fully differentiable surface mesh parameterization that can represent arbitrary topologies. 3si > 0 sj < 0 v vx = si si−sj {s = 0} {s + ∆s = 0} v n(v) v′ n(v′) (a) (b) Figure 2: Marching cubes differentiation vs Iso-surface differentiation. (a) Marching Cubes determines the position vx of a vertex v along an edge via linear interpolation. This does not allow for effective back-propagation when topology changes because its behavior is degenerate when si = sj as shown in [26]. (b) Instead, we adopt a continuous model expressed in terms of how signed distance function perturbations locally impact surface geometry. Here, we depict the geometric relation between local surface change ∆v = v′−v and a signed distance perturbation ∆s< 0, which we exploit to compute ∂v ∂s even when the topology changes. 3.1 Deep Implicit Surface Representation We represent a generic watertight surface Sin terms of a signed distance function (SDF) s: R3 →R. Given the Euclidean distance d(x,S) = min y∈Sd(x,y) of a 3d point x, s(x) is d(x,S) if x is outside Sand −d(x,S) if it is inside. Given a dataset of watertight surfaces S, such as ShapeNet [6], we train a Multi-Layer Perceptron fθ as in [39] to approximate sover such set of surfaces Sby minimizing Lsdf({zS}S∈S,θ) = ∑ S∈S 1 |XS| ∑ x∈XS |fθ(x,zS) −s(x)|+ λreg ∑ S∈S ∥zS∥2 2 , (1) where zS ∈RZ is the Z-dimensional encoding of surface S, θdenotes network parameters, XS represent 3D point samples we use to train our network and λreg is a weight term balancing the contribution of reconstruction and regularization in the overall loss. 3.2 Differentiable Iso-Surface Extraction Once the weights θof Eq. 1 have been learned, fθ maps a latent vector z to a signed distance ﬁeld and the surface of interest is its zero level set. Recall that our goal is to minimize the objective function Ltask introduced at the beginning of this section. As it takes as input a mesh deﬁned in terms of its vertices and facets, evaluating it and its derivatives requires adifferentiable conversion from an implicit ﬁeld to a set of vertices and facets, something that marching cubes does not provide, as depicted by Fig. 2(a). More formally, we need to be able to evaluate ∂Ltask ∂z = ∑ v∈V ∂Ltask ∂v ∂v ∂fθ ∂fθ ∂z . (2) In this work, we take our inspiration from classical functional analysis [ 1] and reason about the continuous zero-crossing of the SDF srather than focusing on how vertex coordinates depend on the implicit ﬁeld fθ when sampled by the marching cubes algorithm. This results in a differentiable approach to compute surface samples v ∈V from the underlying signed distance ﬁeld s. We then simply exploit the fact that fθ is trained to emulate a true SDF sto backpropagate gradients from Ltask to the underlying deep implicit ﬁeld fθ. To this end, let us consider a generic SDF s, a point v lying on its iso-surface S = {q ∈R3|s(q) = 0}, and see how the iso-surface moves whensundergoes an inﬁnitesimal perturbation ∆s. Intuitively, ∆s <0 yields a local surface inﬂation and ∆s >0 a deﬂation, as shown in Fig. 2(b). In the Supplementary Section, we prove the following result, relating local surface change ∆v to ﬁeld perturbation ∆s. Theorem 1. Let us consider a signed distance function sand a perturbation function ∆ssuch that s+ ∆sis still a signed distance function. Given such ∆s, we deﬁne the associated local surface change ∆v = v′−v as the displacement between v′, the closest point to surface sample v on the perturbed surface S′= {q ∈R3|s+ ∆s(q) = 0}, and the original surface sample v. It then holds that ∂v ∂s(v) = −n(v) = −∇s(v) , (3) 4optimization iterations (a) surface-to-surface distance (b) image-to-image distance initialization target S T Figure 3: Topology-Variant Parameterization. We minimize (a) a surface-to-surface or (b) an image-to-image distance with respect to the latent vector z to transform a sphere (genus-0) into a torus (genus-1). This demonstrates that we can backpropagate gradient information from mesh vertices to latent vector while modifying surface mesh topology. where n denotes the surface normals. Because fθ is trained to closely approximate a signed distance function s, we can now replace ∂v ∂fθ in Eq. 2 by −∇fθ(v,z), which yields ∂Ltask ∂z = ∑ v∈V −∂Ltask ∂v ∇fθ(v,z)∂fθ ∂z (v,z) . (4) In short, given an objective function deﬁned with respect to surface samples v ∈V, we can back- propagate gradients all the way back to the latent code z, which means that we can deﬁne a mesh representation that is differentiable end-to-end while being able to capture changing topologies, as will be demonstrated in Section 4. When performing a forward pass, we simply evaluate our deep signed distance ﬁeld fθ on an Euclidean grid, and use marching cubes (MC) to perform iso-surface extraction and obtain surface mesh M= (V,F ). Conversely, we follow the chain rule of Eq. 4 to assemble our backward pass. This requires us to perform an additional forward pass of surface samples v ∈V to compute surface normals ∇fθ(v) as well as ∂fθ ∂z (v,z). We implement MeshSDF following the steps detailed in Algorithms 1 and 2. Refer to the Supplementary Section for a detailed analysis of the computational burden of iso-surface extraction within our pipeline. Algorithm 1: MeshSDF Forward 1: input: latent code z 2: output: surface mesh M= (V,F ) 3: assemble grid G3D 4: sample ﬁeld on grid S = fθ(z,G3D) 5: extract iso-surface (V,F ) = MC(S,G3D) 6: Return M= (V,F ) Algorithm 2: MeshSDF Backward 1: input: upstream gradient ∂L ∂v for v ∈V 2: output: downstream gradient ∂L ∂z 3: forward pass sv = fθ(z,v) for v ∈V 4: n(v) = ∇fθ(z,v) for v ∈V 5: ∂L ∂fθ (v) = −∂L ∂v n for v ∈V 6: Return ∂L ∂z = ∑ v∈V ∂L ∂fθ (v)∂fθ ∂z (v) 4 Experiments We ﬁrst use a simple example to show that, unlike marching cubes, our approach allows for differen- tiable topology changes. We then demonstrate that we can exploit surface mesh differentiability to outperform state-of-the-art approaches on two very different tasks, Single View Reconstruction1 and Aerodynamic Shape Optimization2. 4.1 Differentiable Topology Changes In the experiment depicted by Fig. 3, we used a database of spheres and tori of varying radii to train a network fθ that implements the approximate signed function sof Eq. 1. As a result, fθ associates to a latent vector z an implicit ﬁeld fθ(z) that deﬁnes spheres, tori, or a mix of the two. 1main corresponding author: edoardo.remelli@epﬂ.ch 2main corresponding author: artem.lukoianov@epﬂ.ch 5We now consider two loss functions that operate on explicit surfacesSand T Ltask1 = min s∈S d(s,T) + min t∈T d(S,t) , (5) Ltask2 = ∥DR(S) −DR(T)∥1 , (6) where dis the point-to-surface distance in 3D [42] and DR is the output of an off-the-shelf differen- tiable rasterizer [24], that is Ltask1 is the surface-to-surface distance whileLtask2 is the image-to-image distance between the two rendered surfaces. In the example shown in Fig. 3, Sis the sphere on the left and T is the torus on right. We initialize the latent vector z so that it represents S. We then use the pipeline of Sec. 3.2 to minimize either Ltask1 or Ltask2, backpropagating surface gradients to the underlying implicit representation. In both cases, the sphere smoothly turns into a torus, thus changing its genus. Note that even though we rely on a deep signed distance function to represent our topology-changing surfaces, we did not have to reformulate the loss functions in terms of implicit surfaces, as done in [ 33, 22, 29, 27]. We now turn to demonstrating the beneﬁts of having a topology-variant surface mesh representation through two concrete applications, Single-View Reconstruction and Aerodynamic Shape Optimization. 4.2 Single-View Reconstruction Single-View Reconstruction (SVR) has emerged as a standardized benchmark to evaluate 3D shape representations [11, 13, 17, 55, 8, 31, 41, 16, 45, 61, 51]. We demonstrate that our method is straightforward to apply to this task and validate our approach on two standard datasets, namely ShapeNet [6] and Pix3D [49]. More results, as well as failure cases, can be found in the Supplementary material. Differentiable Meshes for SVR. As in [31, 8], we condition our deep implicit ﬁeld architecture on the input images via a residual image encoder [18], which maps input images to latent code vectors z. These latent codes are then used to condition the architecture of Sec. 3.1 and compute the value of deep implicit function fθ. Finally, we minimize Lsdf (Eq. 1) wrt. θon a training set of image-surface pairs. This setup forms our baseline approach, MeshSDF (raw). To demonstrate the effectiveness of the surface representation proposed in Sec. 3.2, we exploit differentiability during inference via differentiable rasterization [ 24]. We refer to this variant as MeshSDF. Similarly to our baseline, during inference, the encoder predicts an initial latent code z. Different to our baseline, our full version reﬁnes the predicted shape M, as depicted by the top row of Fig. 1. That is, given the camera pose associated to the image and the current value of z, we project vertices and facets into a binary silhouette in image space through a differentiable rasterization function DRsilhouette [24]. Ideally, the projection matches the observed object silhouette Sin the image, which is why we deﬁne our objective function as Ltask = ∥DRsilhouette(M(z)) −S∥1 , (7) which we minimize with respect to z. In practice, we run 400 gradient descent iterations using Adam [25] and keep the z with the smallest Ltask as our ﬁnal code vector. Comparative results on ShapeNet. We report our results on ShapeNet [ 7] in Tab. 1. We com- pare our approach against state-of-the-art mesh reconstruction approaches: reconstructing surface patches [17], generating surface meshes with ﬁxed topology [ 55], generating meshes from vox- elized intermediate representations [ 16], and representing surface meshes using signed distance functions [61]. We used standard train/test splits along with the renderings provided in [ 61] for all the methods we tested. We evaluate on standard SVR metrics [ 51], which we deﬁne in the Supplementary Section. We report our results in Tab. 1. MeshSDF (raw) refers to reconstructions using our encoder-decoder architecture, which is similar to those of [ 31, 8], without any further reﬁnement. Our full method, MeshSDF, exploits end-to-end differentiability to minimize Ltask with respect to z. This improves performance by at least 12% over MeshSDF (raw) on all metrics. As a result, our full approach also outperforms all other state-of-the-art approaches. Comparative results on Pix3D. Whereas ShapeNet contains only rendered images, Pix3D [49] is a test dataset that comprises real images paired to 3D models. We follow the evaluation protocol and metrics proposed in [49], which we detail in the supplementary material. 6Table 1: Single view reconstruction results on ShapeNet Core. Exploiting end-to-end differentia- bility to perform image-based reﬁnement allows us to outperform all prior methods. Metric Method plane bench cabinet car chair display lamp speaker riﬂe sofa table phone boatmean IoU↑ AtlasNet [17]20 13 7 16 13 12 14 8 28 11 15 14 17 15Mesh R-CNN [16]24 25 17 21 21 21 20 15 32 19 26 26 26 23Pixel2Mesh [55]29 32 22 25 27 27 28 19 40 23 31 36 32 29DISN [61] 40 33 20 31 25 33 21 19 60 29 25 44 34 30MeshSDF (raw)32 32 19 30 24 28 20 18 45 26 24 48 28 28MeshSDF 36 38 22 32 28 34 25 22 52 29 31 54 30 32 EMD·102↓ AtlasNett [17]6.3 7.9 9.5 8.3 7.8 8.8 9.8 10.2 6.6 8.2 7.8 9.9 7.1 8.0Mesh R-CNN [16]4.5 3.7 4.3 3.8 4.0 4.6 5.7 5.1 3.8 4.0 3.9 4.7 4.1 4.2Pixel2Mesh [55]3.8 2.9 3.6 3.1 3.4 3.3 4.8 3.8 3.2 3.1 3.3 2.8 3.2 3.4DISN [61] 2.2 2.3 3.2 2.4 2.8 2.5 3.9 3.1 1.9 2.3 2.9 1.9 2.3 2.6MeshSDF (raw)3.3 2.5 3.2 2.2 2.8 3.0 4.2 3.5 2.6 2.7 3.1 1.9 2.9 3.0MeshSDF 2.5 2.1 3.0 2.0 2.4 2.4 3.2 2.9 1.9 2.4 2.7 1.7 2.3 2.5 CD-l2·103↓ AtlasNett [17]10.6 15.0 30.7 10.0 11.6 17.3 17.0 22.0 6.4 11.9 12.3 12.2 10.7 13.0Mesh R-CNN [16]13.3 8.3 10.5 7.2 9.8 10.9 16.4 14.8 6.9 8.7 10.0 6.9 10.4 10.3Pixel2Mesh [55]12.4 5.5 8.2 5.6 6.9 8.2 12.3 11.2 6.0 6.8 7.9 4.7 7.9 8.0DISN [61] 6.3 6.6 11.3 5.3 9.6 8.6 23.6 14.5 4.4 6.0 12.5 5.2 7.8 9.7MeshSDF (raw)10.6 9.5 8.8 4.2 8.2 12.4 25.9 20.4 8.9 11.5 14.6 6.2 17.1 12.0MeshSDF 6.3 5.4 7.8 3.5 5.9 7.3 14.9 12.1 3.4 7.8 10.7 3.9 10.0 7.8 Input Pixel2Mesh [55] DISN [61] MeshSDF (Ours) Figure 4: Pix3D Reconstructions. We compare our reﬁned predictions to the runner-up approaches for the experiment in Tab. 2. MeshSDF can represent arbitrary topology as well as learn strong shape priors, resulting in reconstructions that are consistent even when observed from view-points different from the input one. For this experiment we use the same function fθ as for ShapeNet, that is, we do not ﬁne-tune our model on Pix3D images, but train it on synthetic chair renders only so that to encourage the learning of stronger shape priors. We report our results in Tab. 2 and in Fig. 4. Interestingly, in this more challenging setting using real-world images, our simple baseline MeshSDF (raw) already performs on par with more sophisticated methods using camera information [61]. As for ShapeNet, our full model outperforms all other approaches. Table 2: Single view reconstruction results on Pix3D Chairs. Our full approach outperforms all prior methods in all metrics. Metric Pix3D [49] AtlasNet [17] Mesh R-CNN [16] Pixel2Mesh [55] DISN [61] MeshSDF (raw) MeshSDF IoU ↑ 0.282 - 0.240 0.254 0.333 0.337 0.407 EMD ↓ 0.118 0.128 0.125 0.115 0.117 0.119 0.098 CD-√l2 ↓ 0.119 0.125 0.110 0.104 0.104 0.102 0.089 4.3 Shape Optimization Computational Fluid Dynamics (CFD) plays a central role in designing cars, airplanes and many other machines. It typically involves approximating the solution of the Navier-Stokes equations using numerical methods. Because this is computationally demanding, surrogate methods [52, 59, 3, 54] have been developed to infer physically relevant quantities, such as pressure ﬁeld, drag or lift, directly from 3D surface meshes without performing actual physical simulations. This makes it possible to optimize these quantities with respect to the 3D shape using gradient-based methods and at a much lower computational cost. 7MeshSDF PolyCube FreeForm pmax0pmin optimized shapeinitial shape 0.597 0.852 0.889 Figure 5: Drag minimization. Starting from an initial shape (left column), Ltask is minimized using three different parameterizations: FreeForm (top), PolyCube (middle), and our MeshSDF (bottom). The middle column depicts the optimization process and the relative improvements in terms of Ltask. The ﬁnal result is shown in the right column. FreeForm and PolyCube lack a semantic prior, resulting in implausible details such as sheared wheels (orange inset). By contrast, MeshSDF not only enforces such priors but can also effect topology changes (blue inset). In practice, the space of all possible shapes is immense. Therefore, for the optimization to work well, one has to parameterize the space of possible shape deformations, which acts as a strong regularizer. In [3, 54] hand-crafted surface parameterizations were introduced. It was effective but not generic and had the potential to signiﬁcantly restrict the space of possible designs. We show here that we can use MeshSDF to improve upon hand-crafted parameterizations. Experimental Setup. We started with the ShapeNet car split by automatic deletion of all the internal car parts [48] and then manually selected N = 1400 shapes suitable for CFD simulation. For each surface Mi we ran OpenFoam [20] to predict a pressure ﬁeld pi exerted by air travelling at 15 meters per second towards the car. The resulting training set {Mi,pi}N i=1 was then used to train a Mesh Convolutional Neural Network [14] gβ to predict the pressure ﬁeld p= gβ(M), as in [3]. We use {Mi}N i=1 to also learn the representation of Sec. 3.2 and train the network that implements fθ of Eq. 1. Finally, we introduce the aerodynamic objective function Ltask(M) = ∫∫ M gβnxdM+ Lconstraint(M) , (8) where the integral term approximates drag given the predicted pressure ﬁeld,nxdenotes the projection of surface normals along airﬂow direction, and Lconstraint is designed to preserve the required amount of space for the engine and the passenger compartment. Minimizing the drag of the car can now be achieved by minimizing Ltask with respect to M. We provide further details about this process and the justiﬁcation for our deﬁnition of Ltask in the Supplementary Section. Comparative Results. We compare our surface parameterization to several baselines: (1) vertex- wise optimization, that is, optimizing the objective with respect to each vertex; (2) scaling the surface along its 3 principal axis; (3) using the FreeFormparameterization of [3], which extends scaling to higher order terms as well as periodical ones and (4) the PolyCube parameterization of [54] that deforms a 3D surface by moving a pre-deﬁned set of control points. We report quantitative results for the minimization of the objective function of Eq. 8 for a subset of 8 randomly chosen cars in Table 3, and show qualitative ones in Fig. 5. Not only does our method deliver lower drag values than the others but, unlike them, it allows for topology changes and produces semantically correct surfaces as shown in Fig. 5(c). 8Table 3: CFD-driven optimization.We minimize drag on car shapes comparing different surface parameterizations. Numbers in the table (avg ±std) denote relative improvement of the objective function L% task = Ltask/Lt=0 task for the optimized shape, as obtained by CFD simulation in OpenFoam. Parameterization None Scaling FreeForm [3] PolyCube [54] MeshSDF Degrees of Freedom ∼100k 3 21 ∼332 256 Simulated L% task ↓ not converged 0.931 ±0.014 0 .844 ±0.171 0 .841 ±0.203 0.675 ±0.167 5 Conclusion We introduce a new approach to extracting 3D surface meshes from Deep Signed Distance Functions while preserving end-to-end differentiability. This enables combining powerful implicit models with objective functions requiring explicit representations such as surface meshes. We believe that MeshSDF will become particularly useful for Computer Assisted Design, where having a topology- variant explicit surface parameterizations opens the door to new applications. 6 Acknowledgments This project was supported in part by the Swiss National Science Foundation. 7 Broader Impact Computational Fluid Dynamics is key to addressing the critical engineering problem of designing shapes that maximize aerodynamic, hydrodynamic, and heat transfer performance, and much else beside. The techniques we propose therefore have the potential to have a major impact in the ﬁeld of Computer Assisted Design by unleashing the full power of deep learning in an area where it is not yet fully established. References [1] Grégoire Allaire, François Jouve, and Anca-Maria Toader. A level-set method for shape optimization. Comptes Rendus Mathematique, 334(12):1125–1130, 2002. [2] T. Bagautdinov, C. Wu, J. Saragih, P. Fua, and Y . Sheikh. Modeling Facial Geometry Using Compositional V AEs. InConference on Computer Vision and Pattern Recognition, 2018. [3] Pierre Baqué, Edoardo Remelli, Francois Fleuret, and Pascal Fua. Geodesic Convolutional Shape Opti- mization. In ICML, 2018. [4] V . Blanz and T. Vetter. A Morphable Model for the Synthesis of 3D Faces. InACM SIGGRAPH, pages 187–194, August 1999. [5] André Brock, Theodore Lim, James M. Ritchie, and Nick Weston. Generative and discriminative voxel modeling with convolutional neural networks. In Advances in Neural Information Processing Systems, 2016. [6] A. Chang, T. Funkhouser, L. G., P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. Shapenet: An Information-Rich 3D Model Repository. In arXiv Preprint, 2015. [7] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. [8] Z. Chen and H. Zhang. Learning implicit ﬁelds for generative shape modeling. In Conference on Computer Vision and Pattern Recognition, 2019. [9] Shiyang Cheng, Michael Bronstein, Yuxiang Zhou, Irene Kotsia, Maja Pantic, and Stefanos Zafeiriou. Meshgan: Non-linear 3d morphable models of faces, 2019. [10] J. Chibane, T. Alldieck, and G. Pons-Moll. Implicit functions in feature space for 3d shape reconstruction and completion. In Conference on Computer Vision and Pattern Recognition, 2020. [11] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3D-R2N2: A uniﬁed approach for single and multi-view 3d object reconstruction. In European Conference on Computer Vision, 2016. [12] H. Fan, H. Su, and L. Guibas. A Point Set Generation Network for 3D Object Reconstruction from a Single Image. In Conference on Computer Vision and Pattern Recognition, 2017. [13] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object reconstruction from a single image. In Conference on Computer Vision and Pattern Recognition, 2017. 9[14] Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich Müller. SplineCNN: Fast geometric deep learning with continuous B-spline kernels. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [15] M. Gadelha, S. Maji, and R. Wang. 3D Shape Induction from 2D Views of Multiple Objects. In arXiv Preprint, 2016. [16] Georgia Gkioxari, Justin Johnson, and Jitendra Malik. Mesh r-cnn. In Conference on Computer Vision and Pattern Recognition, 2019. [17] Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. A papier-mâché approach to learning 3d surface generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 216–224, 2018. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, Jun 2016. [19] P. Henderson and V . Ferrari. Learning Single-Image 3D Reconstruction by Generative Modelling of Shape, Pose and Shading. International Journal of Computer Vision, 128(4):835–854, 2020. [20] Hrvoje Jasak, Aleksandar Jemcov, Zeljko Tukovic, et al. Openfoam: A c++ library for complex physics simulations. In International workshop on coupled methods in numerical dynamics, volume 1000, pages 1–20. IUC Dubrovnik Croatia, 2007. [21] Y . Jiang, D. Ji, Z. Han, and M. Zwicker. Sdfdiff: Differentiable rendering of signed distance ﬁelds for 3d shape optimization. In Conference on Computer Vision and Pattern Recognition, 2020. [22] Yue Jiang, Dantong Ji, Zhizhong Han, and Matthias Zwicker. Sdfdiff: Differentiable rendering of signed distance ﬁelds for 3d shape optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1251–1261, 2020. [23] A. Kanazawa, S. Tulsiani, A. Efros, and J. Malik. Learning Category-Speciﬁc Mesh Reconstruction from Image Collections. In arXiv Preprint, 2018. [24] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In Conference on Computer Vision and Pattern Recognition, 2018. [25] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. [26] Y . Liao, S. Donné, and A. Geiger. Deep Marching Cubes: Learning Explicit Surface Representations. In Conference on Computer Vision and Pattern Recognition, pages 2916–2925, 2018. [27] Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li. Learning to infer implicit surfaces without 3d supervision. In Advances in Neural Information Processing Systems, pages 8295–8306, 2019. [28] S. Liu, S.Saito, W.Chen, and Hao Li. Learning to Infer Implicit Surfaces without 3D Supervision. In Advances in Neural Information Processing Systems, 2019. [29] Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng Cui. Dist: Rendering deep implicit signed distance function with differentiable sphere tracing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2019–2028, 2020. [30] W.E. Lorensen and H.E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. In ACM SIGGRAPH, pages 163–169, 1987. [31] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy Networks: Learning 3D Reconstruction in Function Space. In Conference on Computer Vision and Pattern Recognition, pages 4460–4470, 2019. [32] M. Michalkiewicz, J.K. Pontes, D. Jack, M. Baktashmotlagh, and A.P. Eriksson. Implicit Surface Repre- sentations as Layers in Neural Networks. In International Conference on Computer Vision, 2019. [33] Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders Eriksson. Implicit surface representations as layers in neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4743–4752, 2019. [34] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5115–5124, 2017. [35] A. Mosi´nska, P. Marquez-Neila, M. Kozinski, and P. Fua. Beyond the Pixel-Wise Loss for Topology-Aware Delineation. In Conference on Computer Vision and Pattern Recognition, pages 3136–3145, 2018. [36] T.S. Newman and H. Yi. A Survey of the Marching Cubes Algorithm. Computers & Graphics, 30(5):854– 879, 2006. [37] M. Nimier-David, D. Vicini, T. Zeltner, and W. Jakob. Mitsuba 2: A retargetable forward and inverse renderer. ACM Transactions on Graphics, 38(6):1–17, 2019. [38] S. Osher and N. Paragios. Geometric Level Set Methods in Imaging, Vision, and Graphics. Springer, 2003. [39] J. J. Park, P. Florence, J. Straub, R. A. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Conference on Computer Vision and Pattern Recognition, 2019. [40] M. Pharr, W. Jakob, and G. Humphreys. Physically Based Rendering: From Theory to Implementation. Morgan Kaufmann, 2016. [41] Jhony K. Pontes, Chen Kong, Sridha Sridharan, Simon Lucey, Anders P. Eriksson, and Clinton Fookes. Image2mesh: A learning framework for single image 3d reconstruction. In Asian Conference on Computer Vision, 2018. [42] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Pytorch3d. https://github.com/facebookresearch/pytorch3d, 2020. 10[43] Edoardo Remelli, Anastasia Tkach, Andrea Tagliasacchi, and Mark Pauly. Low-dimensionality calibration through local anisotropic scaling for robust hand model personalization. In International Conference on Computer Vision, 2017. [44] Danilo Jimenez Rezende, S. M. Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, and Nicolas Heess. Unsupervised Learning of 3D Structure from Images. InAdvances in Neural Information Processing Systems, pages 4996–5004, 2016. [45] S. Richter and S. Roth. Matryoshka networks: Predicting 3d geometry via nested shape layers. In Conference on Computer Vision and Pattern Recognition, 2018. [46] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In Conference on Computer Vision and Pattern Recognition, 2017. [47] J. A. Sethian. Level Set Methods and Fast Marching Methods Evolving Interfaces in Computational Geometry, Fluid Mechanics, Computer Vision, and Materials Science. Cambridge University Press, 1999. [48] Fun Shing Sin, Daniel Schroeder, and Jernej Barbiˇc. Vega: non-linear fem deformable object simulator. Computer Graphics Forum, 32(1):36–48, 2013. [49] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B. Tenenbaum, and William T. Freeman. Pix3d: Dataset and methods for single-image 3d shape modeling. In Conference on Computer Vision and Pattern Recognition, 2018. [50] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree Generating Networks: Efﬁcient Convolutional Architectures for High-Resolution 3D Outputs. In International Conference on Computer Vision, 2017. [51] M. Tatarchenko, S. Richter, R. Ranftl, Z. Li, V . Koltun, and T. Brox. What Do Single-View 3D Recon- struction Networks Learn? In Conference on Computer Vision and Pattern Recognition, pages 3405–3414, 2019. [52] David Toal and Andy J. Keane. Efﬁcient multipoint aerodynamic design optimization via cokriging. Journal of Aircraft, 48(5):1685–1695, 2011. [53] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization, 2016. [54] Nobuyuki Umetani and Bernd Bickel. Learning three-dimensional ﬂow for interactive aerodynamic design. ACM Transactions on Graphics (TOG), 37(4):1–10, 2018. [55] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In European Conference on Computer Vision, 2018. [56] U. Wickramasinghe, E. Remelli, G. Knott, and P. Fua. V oxel2mesh: 3d mesh model generation from volumetric data. In Conference on Medical Image Computing and Computer Assisted Intervention, 2020. [57] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Conference on Computer Vision and Pattern Recognition, 2015. [58] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang. Pix2vox: Context- aware 3d reconstruction from single and multi-view images. In Conference on Computer Vision and Pattern Recognition, 2019. [59] Gang Xu, Xifeng Liang, Shuanbao Yao, Dawei Chen, and Zhiwei Li. Multi-objective aerodynamic optimization of the streamlined shape of high-speed trains based on the kriging model. PONE, 12(1):1–14, 01 2017. [60] Q. Xu, W. Wang, D. Ceylan, R. Mech, and U. Neumann. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. In Advances in Neural Information Processing Systems, 2019. [61] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. In Advances in Neural Information Processing Systems, pages 492–502, 2019. 118 Supplementary Material In this supplementary material, we ﬁrst remind the interested reader of why marching cubes are not differentiable and provide a formal proof of our main differentiability theorem. We then discuss our approach to speeding-up iso-surface extraction and performing end-to-end training. Finally, we give additional details about our experiments on single-view reconstruction and drag minimization. 8.1 Non-differentiability of Marching Cubes si ≥0 sj < 0 v x = si si−sj (a) (b) si −sj x Figure 6: Marching cubes differentiation. (a) Marching Cubes determines the relative positionxof a vertex v along an edge via linear interpolation. This does not allow for effective back-propagation when topology changes because of a singularity when si = sj. (b) We plot x, relative vertex position along an edge. Note the inﬁnite discontinuity for si = sj. The Marching Cubes (MC) algorithm [30] extracts the zero level set of an implicit ﬁeld and represents it explicitly as a set of triangles. As discussed in the related work section, it comprises the following steps: (1) sampling the implicit ﬁeld on a discrete 3D grid, (2) detecting zero-crossing of the ﬁeld along grid edges, (3) assembling surface topology (i.e. the number of triangles within each cell and how they are connected) using a lookup table and (4) estimating the vertex location of each triangle by performing linear interpolation on the sampled implicit ﬁeld. These steps can be understood as topology estimation followed by the determination of surface geometry. More formally, let S = {si}∈ RN×N×N denote an implicit ﬁeld sampled over a discrete Euclidean grid G3D ∈RN×N×N×3, where N denotes the resolution along each dimension. Within each voxel, surface topology is determined based on the sign of si at its 8 corners. This results in 28 = 256 possible surface topologies within each voxel. Once topology has been assembled, vertices are created in case the implicit ﬁeld changes sign along one of the edges of the voxel. Speciﬁcally, the vertex location v is determined using linear interpolation. Let x∈[0,1] denote the vertex relative location along an edge (Gi,Gj), where Gi and Gj are grid corners such that sj <0 and si ≥0. This implies that if x= 0 then v = Gi and conversely if x= 1 then v = Gj. In the MC algorithm, xis is determined as the zero crossing of the interpolant of si, sj, that is, x= si si −sj . (9) Fig. 6(a) depicts this process. The vertex location is then taken to be v = Gi + x(Gj −Gi). (10) Unfortunately, this function is discontinuous for si = sj, as illustrated in Fig 6(b). Because of this, we cannot swap the signs of si,sj through backpropagation. This prevents topology changes while differentiating, as discussed in [26]. 8.2 Proof of Differentiable Iso-Surface Result Here we formally prove Theorem 1 from the main manuscript. Theorem 2. Let us consider a signed distance function sand a perturbation function ∆ssuch that s+ ∆sis still a signed distance function. Given such ∆s, we deﬁne the associated local surface change ∆v = v′−v as the displacement between v′, the closest point to surface sample v on the 12{s = 0} {s + ∆s = 0} v n(v) v′ n(v′) v n(v) v′ n(v′) v n(v) = −∂v ∂s ∆s→0 Figure 7: Iso-surface differentiation. We adopt a continuous model in terms of how small perturba- tions of a signed distance function locally impact surface geometry. Here, we depict the geometric relation between local surface change ∆v = v′−v and a signed distance perturbation ∆s <0, which we exploit to compute ∂v ∂s in the formal derivation below. perturbed surface S′= {q ∈R3|s+ ∆s(q) = 0}, and the original surface sample v. It then holds that ∂v ∂s(v) = −n(v) = −∇s(v) , (11) where n denotes the surface normals. Proof. Recalling the deﬁnition of signed distance ﬁeld, from elementary geometry we have that ∆v = v′−v = n(v′)d(v,S′) = n(v′) (s(v) −∆s(v)) = −n(v′)∆s(v). (12) Now, observing that lim∆s→0 v′= v, we have ∂v ∂s(v) = lim ∆s→0 ∆v ∆s = lim ∆s→0 −n(v′) = −n(v). (13) Finally, recalling that for a signed distance ﬁeld n(v) = ∇s(v), follows our claim. Fig. 7 illustrates this proof. 8.3 Accelerating Iso-Surface Extraction Recall that our approach to iso-surface differentiation method is independent from the technique used to extract surface samples, meaning that any non-differentiable iso-surface extraction method could be used to obtain an explicit surface from the underlying deep implicit ﬁeld. In practice, when operating in an iterative optimization settings such as those considered in the main manuscript, we exploit the fact that the deep implicit ﬁeld fθ is expected not to change drastically from one iteration to another, and re-evaluate it only where we can expect new zero-crossings to appear. In this setting, we evaluate fθ only at grid corners where |fθ|was smaller than a given threshold at the previous iteration. This reduces the computational complexity of ﬁeld-sampling from O(N3) to O(N2) in terms of the grid size N, which brings noticeable speed ups, as illustrated in the benchmark of Fig. 8. 8.4 Comparison to Deep Marching Cubes Deep Marching Cubes (DMC) [26] is designed to convert point clouds into a surface mesh probability distribution. It can handle topological changes but is limited to low resolution surfaces for the reasons discussed in related work. In the visualization below, we compare our approach to DMC. We ﬁt both representations to a toy dataset consisting of two shapes: a genus-0 cow, and a genus-1 rubber duck. We use a latent space of size 2. Our metric is Chamfer l2 distance evaluated on 5000 samples for unit sphere normalized shapes and shown at the bottom of the ﬁgure. As reported in the original paper, we found DMC to be unable to handle grids larger than 323 because it has to keep track of all possible mesh topologies deﬁned within the grid. By contrast, deep implicit ﬁelds are not limited in resolution and can better capture high frequency details. 13MeshSDF, baseline MeshSDF, fast N log(time) Figure 8: Accellerated Iso-Surface extraction. When working in an iterative optimization setting, we can exploit the fact that fθ, the implicit ﬁeld underlying our surface mesh representation, will change only little between iterations to evaluate it only where we will expect it to change and consequently accelerate iso-surface extraction. CD-l2 ·102 ↓ DMC@323 1.87 Ours@323 1.84 Ours@2563 1.80 groundtruth DMC@323 1.98 Ours@323 1.94 Ours@2563 1.90 groundtruth 8.5 End-to-End Training Here, we demonstrate how our differentiable iso-surface extraction scheme can be used also to backpropagate gradient to the weights of MeshSDF, thus enabling end-to-end training. Speciﬁcally, let us consider a metric measuring the distance between two surfaces, such as the Chamferl2 distance Lchamfer = ∑ p∈P min q∈Q ∥p −q∥2 2 + ∑ q∈Q min p∈P ∥p −q∥2 2 , (14) where P and Qdenote surface samples. We exploit our differentiability result to compute ∂Lchamfer ∂θ = ∑ v∈V ∂Lchamfer ∂v ∂v ∂fθ ∂fθ ∂θ (15) = ∑ v∈V −∂Lchamfer ∂v ∇fθ ∂fθ ∂θ . (16) That is, we can train MeshSDF so that to minimize directly our metric of interest. We evaluate the impact of doing so in Tab. 4, where we ﬁne-tuneDeepSDF models trained minimizing the loss function Lsdf of the main manuscript by further minimizingLchamfer. We refer to this variant as MeshSDF. Unsurprisingly, ﬁne-tuning pre-trained models by minimizing the metric of interest allows us to obtain a boost in performance. In future work, we plan to pursue the following directions within end-to-end training: increasing the level of detail in the generated surfaces by exploiting Generative Adversarial Networks operating on surface mesh data [ 9], and train Single View Reconstruction architectures in a semi-supervised setting, that is by using only differentiable rasterization/rendering to supervise training. 8.6 Single View Reconstruction We ﬁrst provide additional details on the Single View Reconstruction pipeline presented in the main manuscript. Then, for each experimental evaluation of the main paper, we ﬁrst introduce metrics in 14Table 4: End-to-end training. We exploit end-to-end-differentiability to ﬁne-tune pre-trained DeepSDF networks so that to that to minimize directly our metric of interest, Chamfer distance. Category DeepSDF(train) MeshSDF(train) DeepSDF(test) MeshSDF(test) Cars 0.00071 0.00064 (↓9%) 0.00084 0.00067 (↓20%) Chairs 0.00145 0.00133 (↓8%) 0.00407 0.00259 (↓36%) Input ResNet18 MLPz x Prediction DR Silhouette Ltask Input silhouette Figure 9: Shilouette-driven reﬁnment. At inference time, given an input image, we exploit the differentiability of MeshSDF to reﬁne the predicted surface so that to match input silhouette in image space through Differentiable Rasterization [24]. details, and then provide additional qualitative results. To foster reproducibility, we will make our entire code-base publicly available. Architecture. Fig 9 depicts our full pipeline. As in earlier work [ 31, 8], we condition our deep implicit ﬁeld architecture on the input images via a residual image encoder [18], which maps input images to latent code vectors z. Speciﬁcally, our encoder consists of a ResNet18 network, where we replace batch-normalization layers with instance normalization ones [53] so that to make harder for the network to use color cues to guide reconstruction. These latent codes are then used to condition the signed distance function Multi-Layer Perceptron (MLP) architecture of the main manuscript, consisting of 8 Perceptrons as well as residual connections, similarly to [39]. We train this architecture, which we dub MeshSDF (raw), by minimizing Lsdf (Eq.1 on the main manuscript) wrt. θon a training set of image-surface pairs. At inference time, we exploit end-to-end differentiability to reﬁne predictions as depicted in Fig 9. That is, given the camera pose associated to the image and the current value of z, we project vertices and facets into a binary silhouette in image space through a differentiable rasterization function DRsilhouette [24]. Ideally, the projection matches the observed object silhouette Sin the image, which is why we deﬁne our objective function as Ltask = ∥DRsilhouette(M(z)) −S∥1 , (17) which we minimize with respect to z. In practice, we run 400 gradient descent iterations using Adam [25] and keep the z with the smallest Ltask as our ﬁnal code vector. Evaluation on ShapeNet. We used standard train/test splits along with the renderings provided in [61] for all the comparisons we report. We evaluate different approaches based on the following SVR metrics: • Chamfer l2 pseudo-distance: Common evaluation metric for measuring the distance between two uniformly sampled clouds of points P,Q, deﬁned as CD-l2(P,Q) = ∑ p∈P min q∈Q ∥p −q∥2 2 + ∑ q∈Q min p∈P ∥p −q∥2 2. (18) We evaluate this metric by sampling2048 points from reconstructed and target shape, which are re-scaled to ﬁt into a unit-radius sphere. • Earth Mover distance: This metric measures the distance between two point clouds by solving an assignment problem EMD(P,Q) = min Φ: P→Q ∑ p∈P ∥p −Φ(p)∥2, (19) 15where, for all but a zero-measure subset of point set pairs, the optimal bijection Φ is unique and invariant under inﬁnitesimal movement of the points. In practice, the exact computation of EMD is too expensive and we implement the (1 + ε) approximation scheme of [12]. We evaluate this metric by sampling 2048 points from reconstructed and target shape, which are re-scaled to ﬁt into a unit-radius sphere. • Intersection over Union: Since all information about an object’s shape is situated on its surface, and to allow comparison to methods that do not produce watertight surfaces (such as [17]) we propose to evaluate object similarity by measuring surface-to-surface IoU. In practice, denoting as Vthe function mapping a cloud of points to a binary voxel grid, this metric reads IoU(P,Q) = intersection(V(P),V(Q)) union(V(P),V(Q)) (20) We evaluate this metric by sampling 5000 points and setting up the voxel grid divide the object bounding box at resolution 50 ×50 ×50. • F-score: The F-Score has been recently proposed [51] for evaluating SVR algorithms. It explicitly evaluates the distance between object surfaces and is deﬁned as the harmonic mean between precision and recall at a given distance threshold d. We refer the reader to [ 51] for more details about this metric. We evaluate this metric by sampling 10000 points from reconstructed and target shape and set 5% of the object bounding box length as distance threshold. In Table 5, we further compare our method to state-of-the-art single view reconstruction algorithms in terms of F-score. Similarly to what reported in the main manuscript for CD, EMD and IoU, performing imaged-based reﬁnement allows us to outperforms all other state-of-the-art approaches also in terms of this metric. Table 5: Single view reconstruction results on ShapeNet Core. Exploiting end-to-end differentia- bility to perform image-based reﬁnement allows us to outperform all prior methods also in terms of F-Score. Metric Method plane bench cabinet car chair display lamp speaker riﬂe sofa table phone boatmean F-Score%↑ AtlasNet 91 86 74 94 91 84 81 80 96 91 91 90 90 89Pixel2Mesh88 95 94 97 94 92 89 89 95 96 93 97 94 93Mesh R-CNN87 91 90 95 90 89 83 85 93 92 90 95 91 90DISN 94 94 89 96 90 92 78 85 96 96 87 96 93 91MeshSDF(raw)92 95 92 98 94 91 85 86 96 94 91 95 93 91MeshSDF 96 97 94 98 97 95 91 91 98 96 94 98 95 95 Evaluation on Pix3D. We followed closely the evaluation pipeline proposed together with this dataset [49]. That is, we focus on the chair category, and exclude from the evaluation all images where the object we want to reconstruct is truncated or occluded, resulting in 2894 test images. We then use ground truth bounding boxes to crop the image to a window centered around the object. To evaluate fairly reconstruction performance, we segment the background off for all methods presented in Table 2 of the main paper but for [ 49], that achieves state-of-the-art performance in joint segmentation and reconstruction on this benchmark. We do so to give a sense of the impact of assuming to have accurate segmentation information on reconstruction quality. Finally, following the evaluation pipeline designed in [49], we only have access to ShapeNet synthetic data to train our models, that is we don’t have access to any Pix3D image at training time. The main challenge of this benchmark is therefore to design an architecture that is robust to the change of domain. Finally, we use evaluation metrics as originally proposed in [49]: • Chamfer √l2 pseudo-distance: CD- √ l2(P,Q) = ∑ p∈P min q∈Q ∥p −q∥2 + ∑ q∈Q min p∈P ∥p −q∥2, (21) where P and Qare clouds of points. We evaluate this metric by sampling 1024 points from reconstructed and target shape, which are re-scaled to ﬁt into a [−0.5,0.5]3 bounding box. 2 2In the main manuscript we have dubbed this metric as Chamfer l1 by mistake. We will ﬁx when we revise the paper. 16Grid Points Predicted SDF Reconstructed Mesh MeshSDF Gradient (Theorem 1) Ltask Figure 10: Aerodynamic optimization pipeline. We encode a shape we want to optimize using DeepSDF (denoted as SDF block on the ﬁgure) and obtain latent code z. Then we start our iterative process. First, we assemble an Euclidean grid and predict SDF values for each node of the grid. On this grid we run the Marching Cubes algorithm ( MC) to extract a surface mesh. We then run the obtained shape through a Mesh CNN (CFD) to predict pressure ﬁeld from which we compute drag as our objective function. Using the proposed algorithm we obtain gradients of the objective w.r.t. latent code zand do an optimization step. The loop is repeated until convergence. • Earth Mover distance: We use the same metric as above, but follow the approximation scheme of [49] in this case. We evaluate this metric by sampling 1024 points from recon- structed and target shape, which are re-scaled to ﬁt into a [−0.5,0.5]3 bounding box. • Intersection over Union: We evaluate object similarity by measuring volume-to-volume IoU. In practice, denoting as Fthe function mapping a surface mesh Mto a ﬁlled-in binary voxel grid, this metric reads IoUvol(P,Q) = intersection(F(P),F(Q)) union(F(P),F(Q)) , (22) where P,Qdenote surface meshes. We evaluate this metric by setting up the voxel grid divide the surface mesh bounding box at resolution 32 ×32 ×32. Additional qualitative results. We provide additional qualitative comparative results on both ShapeNet and Pix3D in Fig 13,14. Furthermore, in Fig 15 we show failure cases, which we obtain by selecting samples for which reﬁnement does not bring any improvement. Furthermore, we refer the reader to the supplementary video for animations depicting the impact of iterative reﬁnement on the reconstruction. 8.7 Aerodynamic Shape Optimization Here we provide more details on how we performed the aerodynamic optimization experiments presented in the main manuscript. The overall pipeline for the optimisation process is depicted in Fig. 10, and additional optimization results are shown in Fig. 16. 8.7.1 Dataset As described in the main manuscript, we consider the car split of the ShapeNet [7] dataset for this experiment. Since aerodynamic simulators typically require high quality surface triangulations to perform CFD simulations reliably, we (1) follow [48] and automatically remove internal part of each mesh as well as re-triangulate surfaces and (2) manually ﬁlter out corrupted surfaces. After that, we train a DeepSDF auto-decoder on the obtained data split and, using this model, we reconstruct the whole dataset from the learned parameterization. The last step is needed so that to provide fair initial conditions for each method of the comparison in Tab. 3 of the main manuscript, that is to allow all approaches to begin optimization from identical meshes. 17Predicted pmin 0 pmax Figure 11: Simulated and predicted pressure ﬁelds. Pressure ﬁelds for different shapes simulated with OpenFoam (top) and predicted by the Mesh Convolutional Neural Network (bottom). We obtain ground truth pressure values for each car shape with OpenFoam [20], setting an inﬂow velocity of 15 meters per second and airﬂow density equal 1.18. Each simulation was run for at most 5000 time steps and took approximately 20 minutes to converge. Some result of the CFD simulations are depicted in the top row of Fig. 11. We will make both the cleaned car split of ShapeNet and the simulated pressure values publicly available. 8.7.2 CFD prediction We train a Mesh Convolutional Neural Network to regress pressure values given an input surface mesh, and then compute aerodynamic drag by integrating the regressed ﬁeld. Speciﬁcally, we used the dense branch of the architecture proposed in [ 3] and replaced Geodesic Convolutions [34] by Spline ones [14] for efﬁciency. A comparison for the predicted and simulated pressure values may be seen in Fig. 11. 8.7.3 Implementation Details In this section we provide the details needed to implement the baselines parameterizations presented in the main manuscript. • Vertex-vise optimization In this baseline, we optimize surface geometry by ﬂowing gradi- ents directly into surface mesh vertices, that is without using a low-dimensional parame- terization. In our experiments, we have found this strategy to produce unrealistic designs akin to adversarial attacks that, although are minimizing the drag predicted by the network, result in CFD simulations that do not convergence. This conﬁrms the need of using a low-dimensional parameterization to regularize optimization. • Scaling We apply a function fCx,Cy,Cz(V) = (CxVx,CyVy,CzVz)T to each vertex of the initial shape. Here Ci are 3 parameters describing how to scale vertex coordinates along the corresponding axis. As we may see from the Tab. 3 of the main manuscript, such a simple parameterization already allows to improve our metric of interest. • FreeForm Freeform deformation is a very popular class of approaches in engineering optimization. A variant of this parameterization was introduced in [3], where it led to good design performances. In our experiments we are using the parameterization described in [3] with only a small modiﬁcation: to enforce the car left and right sides to be symmetrical we square sinuses in the corresponding terms. We also add l2-norm of the parameterization vector to the loss as a regularization. • PolyCube Inspired by [ 54] we create a grid of control points to change the mesh. The grid size is 8 ×8 ×8 and it is aligned to have 20% width padding along each axis. The displacement of each control point is limited to the size of each grid cell, by applying tanh. During the optimization we shift each control point depending on the gradient it has and then tri-linearly interpolate the displacement to corresponding vertices. Finally, we enforce the displacement ﬁeld to be regular by using Gaussian Smoothing (σ= 1, kernel size = 3). 18Initial Shape Vertex-vise Scaling FreeForm PolyCube MeshSDF Figure 12: Soft constraints reserving space for driver and engine. The ﬁgure illustrates the constraints we put on the surfaces during the optimization process. The constraints are shown for the initial shape, and then for all presented parameterizations. Note, that the constraints we put are soft, and thus may be violated. This results in a parameterization that allows for deformations that are very similar to the one of [54]. As we describe in the main paper, to prevent the surface from collapsing to a point, we put a set of soft-constraints to reserve space for driver and engine. The constraints are represented on the ﬁgure 12. 8.7.4 Additional Regularization for MeshSDF In order to avoid generating unrealistic designs with MeshSDF, we introduce an additional regular- ization term Lconstraint in the optimization, similarly to the regularizations introduced in the baseline parameterizations discussed above. In our experiments, we began by using a standard penalty on l2 norm of the latent code, Lconstraint = α||z||2 2. However, even though it prevented most of the runs from converging to unrealistic shapes, we found converged shapes to still be coarse and noisy in some cases. We therefore opted for a more conservative regularization strategy, reading Lconstraint = α ∑ z′∈Zk ||z −z′||2 2 |Zk| , (23) where Zk = z0,z1,..., zk denote the kclosest latent vectors to z from the training set of DeepSDF. In our experiments we set k= 10, α= 0.2. This regularization limits exploration of the latent space, but guarantees more robust and realistic optimisation outcomes. In our aerodynamics optimization experiments, different initial shapes yield different ﬁnal ones. We speculate that this behavior is due to the presence of local minima in the latent space of DeepSDF, even though we use the Adam optimizer [25] , which is known for its ability to escape some of them. We are planning to address the problem more thoroughly in future. 8.8 Comparison to implicit ﬁeld differentiable rendering Recent advances in differentiable rendering [ 29] have shown that is possible to render continuous SDFs differentiably by carefully designing a differentiable version of the sphere tracing algorithm. By contrast, we simply use MeshSDF end-to-end differentiability to exploit anoff-the-shelf differentiable rasterizer to achieve the same result. To highlight the advantages of doing so, we take the generative 19model of Section 1.4, initialize latent code so that to generate the cow, and then minimize silhouette distance with respect to the duck. In the table below we compare our approach to [29]. Sphere tracing requires to query the network along each camera ray in a sequential fashion, resulting in longer computational time with respect to our approach, which projects surface triangles to image space and then rasterizes them in parallel. Furthermore, our approach requires less function evaluation, as we do not need to sample densely the volume around the ﬁeld zero-crossing. Method l2 silhouette distance ↓ # network queries ↓ run time [s] ↓ Liu20 [most efﬁcient settings, 5122 renders] 0.005973 898k 1.24 MeshSDF [isosurface at 2563, 5122 renders] 0.004625 266k 0.29 Image Pixel2Mesh [55] DISN [61] MeshSDF(raw) MeshSDF Figure 13: Comparative results for SVR on ShapeNet. 20Image Pixel2Mesh [55] DISN [61] MeshSDF(raw) MeshSDF Figure 14: Comparative results for SVR on Pix3D. Image Pixel2Mesh [55] DISN [61] MeshSDF(raw) MeshSDF Figure 15: Failure cases for SVR on Pix3D. Reconstruction reﬁnement based on L1 silhouette distance fails at capturing ﬁne topological details for challenging samples. In the future, we plan to perform reﬁnement using image-based loss functions that are more sensitive to topological mistakes [35]. 21Figure 16: MeshSDF aerodynamic optimizations. 22",
      "references": [
        "A level-set method for shape optimization.",
        "Modeling Facial Geometry Using Compositional V AEs.",
        "Geodesic Convolutional Shape Opti- mization.",
        "A Morphable Model for the Synthesis of 3D Faces.",
        "Generative and discriminative voxel modeling with convolutional neural networks.",
        "Shapenet: An Information-Rich 3D Model Repository.",
        "Learning implicit ﬁelds for generative shape modeling.",
        "Meshgan: Non-linear 3d morphable models of faces",
        "Implicit functions in feature space for 3d shape reconstruction and completion.",
        "3D-R2N2: A uniﬁed approach for single and multi-view 3d object reconstruction.",
        "A Point Set Generation Network for 3D Object Reconstruction from a Single Image.",
        "SplineCNN: Fast geometric deep learning with continuous B-spline kernels.",
        "3D Shape Induction from 2D Views of Multiple Objects.",
        "Mesh r-cnn.",
        "A papier-mâché approach to learning 3d surface generation.",
        "Deep residual learning for image recognition.",
        "Learning Single-Image 3D Reconstruction by Generative Modelling of Shape, Pose and Shading.",
        "Openfoam: A c++ library for complex physics simulations.",
        "Sdfdiff: Differentiable rendering of signed distance ﬁelds for 3d shape optimization.",
        "Learning Category-Speciﬁc Mesh Reconstruction from Image Collections.",
        "Neural 3d mesh renderer.",
        "Adam: A method for stochastic optimization",
        "Deep Marching Cubes: Learning Explicit Surface Representations.",
        "Learning to infer implicit surfaces without 3d supervision.",
        "Dist: Rendering deep implicit signed distance function with differentiable sphere tracing.",
        "Marching Cubes: A High Resolution 3D Surface Construction Algorithm.",
        "Occupancy Networks: Learning 3D Reconstruction in Function Space.",
        "Implicit Surface Repre- sentations as Layers in Neural Networks.",
        "Geometric deep learning on graphs and manifolds using mixture model cnns.",
        "Beyond the Pixel-Wise Loss for Topology-Aware Delineation.",
        "A Survey of the Marching Cubes Algorithm.",
        "Mitsuba 2: A retargetable forward and inverse renderer.",
        "Geometric Level Set Methods in Imaging, Vision, and Graphics.",
        "Deepsdf: Learning continuous signed distance functions for shape representation.",
        "Physically Based Rendering: From Theory to Implementation.",
        "Image2mesh: A learning framework for single image 3d reconstruction.",
        "Pytorch3d.",
        "Low-dimensionality calibration through local anisotropic scaling for robust hand model personalization.",
        "Unsupervised Learning of 3D Structure from Images.",
        "Matryoshka networks: Predicting 3d geometry via nested shape layers.",
        "Octnet: Learning deep 3d representations at high resolutions.",
        "Level Set Methods and Fast Marching Methods Evolving Interfaces in Computational Geometry, Fluid Mechanics, Computer Vision, and Materials Science.",
        "Vega: non-linear fem deformable object simulator.",
        "Pix3d: Dataset and methods for single-image 3d shape modeling.",
        "Octree Generating Networks: Efﬁcient Convolutional Architectures for High-Resolution 3D Outputs.",
        "What Do Single-View 3D Recon- struction Networks Learn?",
        "Efﬁcient multipoint aerodynamic design optimization via cokriging.",
        "Instance normalization: The missing ingredient for fast stylization,",
        "Learning three-dimensional ﬂow for interactive aerodynamic design.",
        "Pixel2mesh: Generating 3d mesh models from single rgb images.",
        "V oxel2mesh: 3d mesh model generation from volumetric data.",
        "3d shapenets: A deep representation for volumetric shapes.",
        "Pix2vox: Context- aware 3d reconstruction from single and multi-view images.",
        "Multi-objective aerodynamic optimization of the streamlined shape of high-speed trains based on the kriging model.",
        "Disn: Deep implicit surface network for high-quality single-view 3d reconstruction."
      ],
      "meta_data": {
        "arxiv_id": "2006.03997v2",
        "authors": [
          "Edoardo Remelli",
          "Artem Lukoianov",
          "Stephan R. Richter",
          "Benoît Guillard",
          "Timur Bagautdinov",
          "Pierre Baque",
          "Pascal Fua"
        ],
        "published_date": "2020-06-06T23:44:05Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs",
      "full_text": "INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Jaejun Lee 1 Chanyoung Chung 1 Joyce Jiyoung Whang 1 Abstract Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to ap- pear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we pro- pose an INductive knowledge GRAph eMbedding method, INGRAM , that can generate embeddings of new relations as well as new entities at in- ference time. Given a knowledge graph, we de- fine a relation graph as a weighted graph consist- ing of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, INGRAM learns how to aggre- gate neighboring embeddings to generate relation and entity embeddings using an attention mech- anism. Experimental results show that INGRAM outperforms 14 different state-of-the-art methods on varied inductive learning scenarios. 1. Introduction Knowledge graphs represent known facts as a set of triplets, each of which is composed of a head entity, a relation, and a tail entity (Ji et al., 2022). Among various approaches to predicting missing triplets in knowledge graphs, embedding- based methods are known to be effective, where entities and relations are converted into low-dimensional embed- ding vectors (Nathani et al., 2019). Classical knowledge graph embedding models (Liu et al., 2017; Sun et al., 2019) assume a transductive learning. That is, it is assumed that all entities and relations are observed during training. Transduc- tive knowledge graph embedding methods predict a missing 1School of Computing, KAIST, Daejeon, South Korea. Corre- spondence to: Joyce Jiyoung Whang <jjwhang@kaist.ac.kr>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). Occidental College California HeadquarterIn Barack  Obama Graduated USA Nationality Contains Training Graph Lewis  Carroll University  of Oxford Graduated Nationality Oxford Contains Contains England Roman  Polanski Paris BornIn Victor  HugoDiedIn France Nationality Nationality Central  Time Zone Austin Bruce  Sterling Texas LivedIn TimeZone TimeZone BornIn Triplet with Known Relation Triplet with New Relation Predicted Triplet (a) Transductive Inference for relations (c) Inductive Inference for relations (b) Semi-Inductive Inference for relations Figure 1: For relations, (a) is a transductive inference, (b) is a semi-inductive inference, and (c) is an inductive inference. triplet by identifying a plausible combination of the ob- served entities and relations (Wang et al., 2017). In recent years, inductive knowledge graph completion has been studied to predict missing triplets between new en- tities that are not observed at training time (Teru et al., 2020). If an entity or a relation is observed during training, we call them known and new otherwise; they are some- times referred to as seen and unseen (Xu et al., 2022). To handle new entities, some methods focus on learning entity-independent relational patterns by logical rule min- ing (Sadeghian et al., 2019), while others exploit Graph Neural Networks (GNNs) (Yan et al., 2022). However, most existing methods assume that only entities can be new, and all relations should be observed during training. Thus, they perform inductive inference for entities but transductive inference for relations as shown in Figure 1(a). In this paper, we consider more realistic inductive learn- ing scenarios: (i) the relations at inference time consist of a mixture of known and new relations ( semi-inductive in- ference for relations), or (ii) the relations are all new due to an entirely new set of entities ( inductive inference for relations). Figure 1(b) and Figure 1(c) show these scenar- ios. We propose INGRAM , an INductive knowledge GRAph eMbedding method that can generate embedding vectors for new relations and entities only appearing at inference time. Figure 2 shows an overview of INGRAM when all relations and entities are new. A key idea is to define a relation graph 1 arXiv:2305.19987v3  [cs.LG]  17 Aug 2023INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Training Graph r1 r1 r2 r4 r2 r2 r4 r1 r5 r3 r4 r3 r1 r2r5 r4 r3 Relation Graph of Training Graph r4 r3 Relation-level Aggregation r5 r1 r2 Entity-level Aggregation Loss Optimization Inference Graph Relation Graph of Inference Graph Relation-level Aggregation Entity-level Aggregation Link Prediction r6 r7 r8r7 r8 r9 r6 r6 r6 r7 r7 r7 r6 r7 r9 r8 r7 r9 r6 r8 r6 r7 r8r7 r8 r9 r6 r6 r6 r7 r7 r7 r9 r7 r8 tailhead relation score Learned weights Learned weights Figure 2: Overview of INGRAM . Given a knowledge graph, a relation graph is created to define the neighboring relations of each relation. Based on the relation graph and the original knowledge graph, relation and entity embedding vectors are computed by aggregating their neighbors’ embeddings. During training, INGRAM learns how to aggregate the neighbors’ embeddings by maximizing the scores of training triplets. At inference time, INGRAM creates embeddings of new relations and entities by aggregating their neighbors’ embeddings and conducts link prediction in the way it learned during training. where a node corresponds to a relation, and an edge weight indicates the affinity between relations. Once the relation graph is defined, we can designate neighboring relations for each relation. Given the relation graph and the original knowledge graph, the relation and entity embedding vectors are computed by attention-based aggregations of their neigh- bors’ embeddings. The aggregation process is optimized to maximize the plausibility scores of triplets in a training knowledge graph. What INGRAM learns during training is how to aggregate neighboring embeddings to generate the relation and entity embeddings. At inference time, IN- GRAM generates embeddings of new relations and entities by aggregating neighbors’ embeddings based on the new relation graph computed from a given inference knowledge graph and the attention weights learned during training. To the best of our knowledge, INGRAM is the first method that introduces the relation-level aggregation that allows the model to be generalizable to new relations. Due to the fully inductive capability of INGRAM , we can generate embeddings by training INGRAM on a tractable, partially observed set and simply applying it to an entirely new set without retraining. Different from some inductive methods that rely on large language models (Zha et al., 2022), IN- GRAM makes inferences solely based on the structure of a given knowledge graph. Experimental results show that INGRAM significantly outperforms 14 different knowledge graph completion methods in inductive link prediction on 12 datasets with varied ratios of new relations. The perfor- mance gap between INGRAM and the best baseline method is substantial, especially when the ratio of new relations is high, which is a more challenging scenario.1 1https://github.com/bdi-lab/InGram 2. Related Work Rule Mining and Subgraph Reasoning. For induc- tive knowledge graph completion, (Yang et al., 2017) and (Sadeghian et al., 2019) have proposed learning first- order logical rules, while (Wang et al., 2021), (Zhu et al., 2021) and (Zhang & Yao, 2022) have considered relational context or paths. GraIL (Teru et al., 2020) has proposed a subgraph-based reasoning framework that extracts sub- graphs and scores them using a GNN. Some follow-up works of GraIL include (Mai et al., 2021), (Xu et al., 2022), and (Lin et al., 2022). (Yan et al., 2022) has focused on cycle-based rule learning while (Liu et al., 2021) has pro- posed GNN-based encoding capturing logical rules. Differ- ent from our method, all these methods assume only entities can be new, and relations should be known in advance. (Jin et al., 2022) has recently proposed GraphANGEL handling new relations, but it assumes all entities are known. Differences between RMPI and INGRAM . RMPI (Geng et al., 2023) has concurrently studied the problem of han- dling new relations, although how RMPI andINGRAM solve the problem is quite different. While RMPI extracts a local subgraph for every candidate entity to score the correspond- ing triplet, INGRAM directly utilizes the whole structure of a given knowledge graph. Also, RMPI uses an unweighted relation view per every individual triplet, whereas INGRAM defines one global relation graph where weights are impor- tant. Due to these fundamental differences, INGRAM is much more scalable and effective than RMPI. For exam- ple, INGRAM took 15 minutes and RMPI took 52 hours to process NL-100 dataset while INGRAM achieves much better link prediction performance than RMPI. Details will 2INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Star  Wars California San Francisco USA ActorMark  Hamill Director Clint Eastwood Directed Profession ActedIn LivedIn BornIn Nationality Brad  Pitt Tom  Hanks Saving  Private  Ryan Steven  Spielberg Catch Me If You Can (a) Knowledge Graph Directed Profession ActedIn BornIn LivedIn Nationality 0.33 0.11 0.11 0.11 0.36 0.81 0.22 0.50 (b) Relation Graph Profession 1.  ActedIn 2.  Directed 3.  BornIn 0.33 0.22 0.11 Directed 1.  ActedIn 2.  Profession 0.50 0.22 ActedIn 1.  Directed 2.  Profession 3.  BornIn 0.50 0.33 0.11 BornIn 1.  LivedIn 2.  Nationality 3.  Profession 3. ActedIn 0.81 0.36 0.11 0.11 LivedIn 1.  BornIn 2.  Nationality 0.81 0.11 Nationality 1.  BornIn 2.  LivedIn 0.36 0.11 (c) Affinity scores of the relations Figure 3: Given a knowledge graph, we define a relation graph as a weighted graph where each node indicates a relation, and each edge weight indicates the affinity between two relations. Self-loops in the relation graph are omitted for brevity. be discussed in Section 6. Furthermore, RMPI is designed only for knowledge graph completion and does not com- pute embedding vectors, while INGRAM returns a set of embedding vectors for entities and relations that can be also utilized in many other downstream tasks. Reasoning on Evolving Graphs. Some methods have focused on modeling emerging entities (Hamaguchi et al., 2017; Wang et al., 2022). For example, (Wang et al., 2019) has considered rule and network-based attention weights, and (Dai et al., 2021) has extended RotatE (Sun et al., 2019). All these methods assume that a triplet should be composed of one known entity and one new entity. (Cui et al., 2022) has proposed a GCN-based (Kipf & Welling, 2017) method to more flexibly handle emerging entities. Compared to these methods, we tackle a more challenging problem where all entities are new instead of only some portions being new. Textual Descriptions and Language Models. (Daza et al., 2021), (Ali et al., 2021) and (Gesese et al., 2022) have used pre-trained vectors by BERT (Devlin et al., 2019) using textual descriptions. Some methods have utilized language models to handle new entities (Markowitz et al., 2022). For example, (Zha et al., 2022) has leveraged a pre-trained language model and fine-tuned it. All these methods employ a rich language model or require text descriptions that might not always be available. On the other hand, INGRAM only utilizes the structure of a given knowledge graph. 3. Problem Definition and Setting In inductive knowledge graph embedding, we are given two graphs: a training graph fGtr and an inference graph gGinf. A training graph is defined by fGtr = (Vtr, Rtr, Etr) where Vtr is a set of entities, Rtr is a set of relations, and Etr is a set of triplets in fGtr. We divide Etr into two disjoint sets such as Etr := Ftr ∪ Ttr where Ftr is a set of known facts and Ttr is a set of triplets a model is optimized to predict. An inference graph is defined by gGinf = ( Vinf, Rinf, Einf) where Vinf is a set of entities, Rinf is a set of relations, and Einf is a set of triplets in gGinf. We partition Einf into three pairwise disjoint sets, such that Einf := Finf ∪ Tval ∪ Ttest with a ratio of 3:1:1. Finf is a set of observed facts where it contains all entities and relations included in gGinf, Tval is a set of triplets for validation, and Ttest is a set of test triplets. The fact sets Ftr and Finf are also defined in (Zhang & Yao, 2022; Ali et al., 2021). Note that Vtr ∩ Vinf = ∅ by following a conventional inductive learning setting (Teru et al., 2020). Most existing methods assume Rinf ⊆ Rtr due to the constraint that relations cannot be new at inference time (Zhang & Yao, 2022). On the other hand, in our problem setting, Rinf is not necessarily a subset of Rtr since new relations are allowed to appear. At training time, we use Gtr := (Vtr, Rtr, Ftr) and a model is trained to predict Ttr. When tuning the hyperparameters of a model, we use Ginf := (Vinf, Rinf, Finf) to compute the embeddings; we check the model’s performance on Tval. At inference time, we evaluate the model’s performance using Ttest. The way we use Finf, Tval and Ttest is identical to (Ali et al., 2021; Galkin et al., 2022). For brevity, we do not explicitly mention Gtr or Ginf in the following sections; those should be distinguished in context. 4. Defining Relation Graphs Let us represent a knowledge graph as G = ( V, R, F) where V is a set of entities, R is a set of relations, and F is a set of triplets, i.e., F = {(vi, rk, vj)|vi ∈ V, rk ∈ R, vj ∈ V}. For every (vi, rk, vj) ∈ F, we add a reverse relation r−1 k to R and add a reverse triplet (vj, r−1 k , vi) to F (Schlichtkrull et al., 2018). Assume that |V| = n and |R| = m. Given a knowledge graph, we define a relation graph as a weighted graph where each node corresponds to a relation, and each edge weight indicates the affinity between two rela- tions. Figure 3 shows an example where we omit the reverse relations and self-loops in the relation graph for simplicity. 3INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Briefly speaking, we measure the affinity between two rela- tions by considering how many entities are shared between them and how frequently they share the same entity. To represent which relations are associated with which enti- ties, we create two matrices Eh ∈ Rn×m and Et ∈ Rn×m where the subscripts h and t indicate head and tail, re- spectively. Let Eh[i, j] denote the i-th row and the j-th column element of Eh, where Eh[i, j] is the frequency of vi appearing as a head entity of relation rj. Similarly, Et[i, j] is the frequency of vi appearing as a tail entity of relation rj. While some entities are frequently involved in relations, some entities are rarely involved in relations. To take this into account, we define the degree of an en- tity to be the sum of its frequencies. Formally, we de- fine Ah := Eh T Dh −2Eh where Dh ∈ Rn×n is the de- gree diagonal matrix of entities for head, i.e., Dh[i, i] :=P j Eh[i, j]. Similarly, we define At := Et T Dt −2Et where Dt ∈ Rn×n is the degree diagonal matrix of entities for tail. The degree normalization terms allow the sum of the affinity weights introduced by each entity in Ah and At to be normalized to one. Finally, we define the adjacency matrix of the relation graph to be A := Ah + At where A ∈ Rm×m and each el- ement aij ∈ A indicates the affinity between the rela- tions ri and rj. In Figure 3, we see that the relation graph identifies semantically close relations even though we use only the structure of a knowledge graph. However, there is a chance that we miss some semantically similar relation pairs in the relation graph if they do not share an entity in the knowledge graph. Note that the goal of the relation graph is not to identify the perfect set of similar relations but to define a relation’s reasonable neighborhood whose representation vectors can be used to create the embedding of the target relation, which will be discussed in Section 5.1. 5. INGRAM : Inductive Knowledge Graph Embedding Model We present INGRAM that consists of relation-level aggre- gation, entity-level aggregation, and modeling of relation- entity interactions. 5.1. Updating Relation Representation Vectors via Relation-Graph-Based Aggregation Suppose we have an initial feature vector for a relation ri, denoted by xi ∈ Rd (i = 1 , ··· , m), where d is the dimension of a relation vector. We initializexi using Glorot initialization (Glorot & Bengio, 2010). Let z(l) i ∈ Rd′ denote a hidden representation of ri where d′ is the hidden dimension, the superscript (l) indicates the l-th layer with l = 0, ··· , L−1, and L is the number of layers for relations. We compute z(0) i = Hxi where H ∈ Rd′×d is a trainable matrix that projects the initial feature vector to a hidden representation vector. All vectors are assumed to be column vectors unless specified. Since we define the relation graph A in Section 4, we can designate the neighboring relations of each relation using A. We update each relation’s representation by aggregating its own and neighbors’ representation vectors. Specifically, we define the forward propagation as follows: z(l+1) i = σ   X rj∈Ni α(l) ij W(l)z(l) j   (1) where σ(·) is an element-wise activation function such as LeakyReLU (Maas et al., 2013), z(l) j is a relation represen- tation vector of rj, W(l) ∈ Rd′×d′ is a weight matrix, Ni is the set of neighbors of ri on the relation graph A, and the attention value α(l) ij is defined by2 α(l) ij = exp \u0010 y(l)σ \u0010 P(l)[z(l) i ∥z(l) j ] \u0011 + c(l) s(i,j) \u0011 P rj′∈Ni exp \u0010 y(l)σ \u0010 P(l)[z(l) i ∥z(l) j′ ] \u0011 + c(l) s(i,j′) \u0011 (2) where ∥ denotes concatenating vectors vertically, P(l) ∈ Rd′×2d′ is a weight matrix, and y(l) ∈ R1×d′ is a row weight vector. We apply y(l) after σ(·) to resolve the static attention issue of (Veli ˇckovi´c et al., 2018) by fol- lowing (Brody et al., 2022). In our implementation, we use the residual connection (He et al., 2016) and the multi-head attention mechanism with K heads (Vaswani et al., 2017). In (2), c(l) s(i,j) is a learnable parameter indexed by s(i, j) which is defined by s(i, j) = \u0018rank(aij) × B nnz(A) \u0019 (3) where aij indicates the value corresponding to the i-th row and the j-th column in A, rank(aij) is the ranking of aij when the non-zero elements in A are sorted in descending order, nnz(A) is the number of non-zero elements inA, and B is a hyperparameter indicating the number of bins. We divide the relation pairs into B different bins according to their affinity scores, i.e., the aij values. Each relation pair has an index value of 1 ≤ s(i, j) ≤ B and we have the learnable parameters c(l) 1 , ··· , c(l) B . In (1) and (2), we update the relation representation vec- tors by using the attention mechanism, where we consider the relative importance of each neighboring relation and the affinity between the relations. While the former term is computed based on the local structure of the target rela- tion, the latter term, c(l) s(i,j), reflects a global level of affinity 2Note that Ni includes ri itself because A contains self-loops. 4INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs because we divide the affinity scores into B different lev- els globally. When representing a relation’s representation vector, it would be beneficial to take the vectors of similar relations to the target relation. Thus, c(l) s(i,j) is expected to have a high value for a small s(i, j) because the relation pairs belonging to a small s(i, j) indicate those having high affinity values. We empirically observed that c(l) s(i,j) values are learned as expected (details in Section 6.5). The way we incorporate the affinity into the attention mechanism is inspired by Graphormer (Ying et al., 2021) even though it is not designed for inductive knowledge graph embedding. By updating z(l) i for l = 0, ··· , L− 1 using (1), we have the final-level relation representation vectors z(L) i for i = 1, ··· , mwhich are utilized to update entity representation vectors. 5.2. Entity Representation by Entity-level Aggregation Suppose we have an initial feature vector for an entityvi, de- noted by bxi ∈ Rbd (i = 1, ··· , n), where bd is the dimension of an entity vector. We initialize bxi using Glorot initial- ization. Let h(l) i ∈ Rbd′ denote a hidden representation of vi where bd′ is the hidden dimension, the superscript (l) in- dicates the l-th layer with l = 0, ··· , bL − 1, and bL is the number of layers for entities. We compute h(0) i = cHbxi where cH ∈ Rbd′×bd is trainable. We update a representation vector of vi by aggregating the representation vectors of its neighbors, its own vector, and the representation vectors of the relations adjacent to vi. When we refer to the relation representation vectors here, we always use the final-level relation representation vectors z(L) k for k = 1, ··· , macquired in Section 5.1. We define the neighbors of vi ∈ V to be bNi = {vj|(vj, rk, vi) ∈ F, vj ∈ V, rk ∈ R}. To compute the attention weight for the self-loop of vi, we consider the mean vector of the representation vectors of the relations adjacent to vi: ¯z(L) i = X vj∈cNi X rk∈Rji z(L) kP vj′∈cNi |Rj′i| where Rji denotes the set of relations from vj to vi. We update an entity representation vector of vi by h(l+1) i = σ  β(l) ii cW (l) [h(l) i ∥¯z(L) i ] + X vj∈cNi X rk∈Rji β(l) ijk cW (l) [h(l) j ∥z(L) k ]   (4) where cW (l) ∈ Rbd′×( bd′+d′) is a weight matrix, and β(l) ii and β(l) ijk are the attention coefficients which are defined by β(l) ii = exp \u0010 by(l)σ \u0010 bP (l) b(l) ii \u0011\u0011 exp \u0010 by(l)σ \u0010 bP (l) b(l) ii \u0011\u0011 + P vj′∈cNi P rk′∈Rj′i exp \u0010 by(l)σ \u0010 bP (l) b(l) ij′k′ \u0011\u0011, β(l) ijk = exp \u0010 by(l)σ \u0010 bP (l) b(l) ijk \u0011\u0011 exp \u0010 by(l)σ \u0010 bP (l) b(l) ii \u0011\u0011 + P vj′∈cNi P rk′∈Rj′i exp \u0010 by(l)σ \u0010 bP (l) b(l) ij′k′ \u0011\u0011 where b(l) ii = [ h(l) i ∥h(l) i ∥¯z(L) i ], b(l) ijk = [ h(l) i ∥h(l) j ∥z(L) k ], bP (l) ∈ Rbd′×(2bd′+d′) is a linear transformation matrix, and by(l) ∈ R1×bd′ is a row weight vector. We also implement the residual connection and the multi-heads with bK heads. Our formulation in (4) seamlessly extends GATv2 (Brody et al., 2022) by incorporating the relation representation vectors in every aggregation step. Specifically, when we regard all relation vectors as constant, (4) is equivalent to GATv2. By updating h(l) i for l = 0, ··· , bL − 1 using (4), we have the final-level entity representation vectors h(bL) i (i = 1, ··· , n) which are utilized to model relation-entity interactions. 5.3. Modeling Relation-Entity Interactions Given the representation vectors provided in Section 5.1 and Section 5.2, we compute the final embedding vectors: zk := Mz(L) k (k = 1, ··· , m) for relations andhi := cMh(bL) i (i = 1, ··· , n) for entities, where M ∈ Rd×d′ and cM ∈ Rbd×bd′ are trainable projection matrices. A knowledge graph embedding scoring function, denoted by f(vi, rk, vj), returns a scalar value representing the plau- sibility of a given triplet (vi, rk, vj) (Schlichtkrull et al., 2018). To model the interactions between relation and en- tity embeddings, we use a variant of DistMult (Yang et al., 2015). We define our scoring function by f(vi, rk, vj) := hT i diag(Wzk)hj (5) where W ∈ Rbd×d is a weight matrix that converts the dimension of zk from d to bd and diag(Wzk) is the di- agonal matrix whose diagonal is defined by Wzk. Let (vi, rk, vj) ∈ Ttr be a positive triplet in a training set Ttr described in Section 3. We create negative triplets by cor- rupting a head or a tail entity of a positive triplet. Let (˚vi, rk, ˚vj) ∈ ˚Ttr denote the negative triplets. The margin- based ranking loss is defined by X (vi,rk,vj)∈Ttr X (˚vi,rk,˚vj)∈˚Ttr max(0, γ−f(vi, rk, vj)+f(˚vi, rk, ˚vj)) where γ is a margin separating the positive and negative triplets. The model parameters are learned by optimizing 5INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs the above loss using stochastic gradient descent with a mini- batch based on the Adam optimizer. 5.4. Training Regime Given fGtr = ( Vtr, Rtr, Etr), we divide Etr into Ftr and Ttr with a ratio of 3:1. For every epoch, we randomly re-split Ftr and Ttr with the minimal constraint that Ftr includes the minimum spanning tree of fGtr and Ftr covers all relations in Rtr so that all entity and relation embedding vectors are appropriately learned. At the beginning of each epoch, we initialize all feature vectors using Glorot initialization. This dynamic split and re-initialization strategy allow IN- GRAM to robustly learn the model parameters, which makes the model more easily generalizable to an inference graph. In Section 6.4, we empirically observe the importance of dynamic split by the ablation study of INGRAM . Since we randomly re-initialize all feature vectors per epoch during training, INGRAM learns how to compute embedding vec- tors using random feature vectors, and this is beneficial for computing embeddings with random features at inference time. This observation is consistent with recent studies in (Abboud et al., 2021; Sato et al., 2021) showing that the expressive power of GNNs can be enhanced with random- ized initial node features. However, (Abboud et al., 2021; Sato et al., 2021) have analyzed GNNs for standard graphs but not for knowledge graphs. We will further investigate the effects of the combination of dynamic split and random re-initialization strategy from a theoretical point of view. 5.5. Embedding of New Relations and Entities Given Ginf = (Vinf, Rinf, Finf), we create the relation graph discussed in Section 4 and compute the relation and entity embedding vectors using the learned model parameters of INGRAM . Algorithm 1 shows the overall procedure. Based on the generated embedding vectors of entities and relations on Ginf, we can predict missing triplets. For example, to solve (vi, rk, ?), we plug each entity vj ∈ Vinf into the given triplet and compute the score using (5) where W is already trained during training. The missing entity is predicted to be the one with the highest score. 6. Experimental Results We compare the performance of INGRAM with other induc- tive knowledge graph completion methods. 6.1. Datasets and Baseline Methods Since existing datasets do not contain new relations at gGinf, we create 12 datasets using three benchmarks, NELL- 995 (Xiong et al., 2017),Wikidata68K (Gesese et al., 2022), and FB15K237 (Toutanova & Chen, 2015). Let us call these Algorithm 1 Embeddings via INGRAM at Inference Time Input: Ginf = (Vinf, Rinf, Finf), the trained model parameters: H, W(l), P(l), y(l), c(l) 1 , ··· , c(l) B for l = 0, ··· , L− 1, cH, cW (bl) , bP (bl) , by(bl), bl = 0, ··· , bL − 1, M, cM Output: zi for all ri ∈ Rinf and hj for all vj ∈ Vinf 1: Create the relation graph A as discussed in Section 4. 2: Initialize xi for all ri ∈ Rinf and bxj for all vj ∈ Vinf using Glorot initialization. 3: Set z(0) i ← Hxi and h(0) j ← cHbxj. 4: for l = 0, ··· , L− 1 do 5: for ri ∈ Rinf do 6: Compute z(l+1) i according to (1) using W(l), P(l), y(l), and c(l) 1 , ··· , c(l) B . 7: end for 8: end for 9: for l = 0, ··· , bL − 1 do 10: for vj ∈ Vinf do 11: Compute h(l+1) j according to (4) using cW (l) , bP (l) , by(l), and z(L) i for ri ∈ Rinf. 12: end for 13: end for 14: zi ← Mz(L) i for ri ∈ Rinf and hj ← cMh(bL) j for vj ∈ Vinf. benchmarks NL, WK, and FB, respectively. For each bench- mark, we create four datasets by varying the percentage of triplets with new relations: 100%, 75%, 50% and 25%. For example, in NL-75, approximately 75% of triplets have new relations, and 25% of triplets have known relations, i.e., semi-inductive inference for relations. On the other hand, in NL-100, all triplets have new relations, i.e., inductive infer- ence for relations. In all 12 datasets, all entities in gGinf are new entities, as also assumed in (Teru et al., 2020). Details about these datasets are described in Appendix A. We compare the performance of INGRAM with 14 different methods: GraIL (Teru et al., 2020), CoMPILE (Mai et al., 2021), SNRI (Xu et al., 2022), INDIGO (Liu et al., 2021), RMPI (Geng et al., 2023), CompGCN (Vashishth et al., 2020), NodePiece (Galkin et al., 2022), NeuralLP (Yang et al., 2017), DRUM (Sadeghian et al., 2019), BLP (Daza et al., 2021), QBLP (Ali et al., 2021), NBFNet (Zhu et al., 2021), RED-GNN (Zhang & Yao, 2022), and RAILD (Gesese et al., 2022). Since the original implementations of GraIL, CoMPILE, SNRI, INDIGO, and RMPI were based on subgraph sam- pling, we extended them to consider all entities in Vinf to evaluate the performance more accurately. Due to the scal- ability issues of GraIL, CoMPILE, SNRI, INDIGO, and RMPI, they only run on NL datasets. BLP, QBLP and RAILD require BERT-based pre-trained vectors which are produced based on the names and textual descriptions of en- tities or relations. We provide BLP, QBLP and RAILD with the pre-trained vectors using available information. Since 6INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Table 1: Inductive link prediction performance on 12 different datasets, where all entities are new, whereas the last digits of each dataset (100, 75, 50, and 25) indicate the ratio of new relations. The best results are boldfaced and the second-best results are underlined. Our model, I NGRAM , significantly outperforms the baseline methods in most cases. NL-100 NL-75 NL-50 NL-25 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 GraIL 928.4 0.135 0.173 0.114 526.0 0.096 0.205 0.036 837.6 0.162 0.288 0.104 692.9 0.216 0.366 0.160 CoMPILE 743.1 0.123 0.209 0.071 519.6 0.178 0.361 0.093 466.6 0.194 0.330 0.125 438.9 0.189 0.324 0.115 SNRI 809.8 0.042 0.064 0.029 418.7 0.088 0.177 0.040 584.6 0.130 0.187 0.095 417.7 0.190 0.270 0.140 INDIGO 621.1 0.160 0.247 0.109 587.4 0.121 0.156 0.098 864.9 0.167 0.217 0.134 812.4 0.166 0.206 0.134 RMPI 143.9 0.220 0.376 0.136 244.5 0.138 0.275 0.061 479.1 0.185 0.307 0.109 385.7 0.213 0.329 0.130 CompGCN 877.9 0.008 0.014 0.001 750.5 0.014 0.025 0.003 1183.6 0.003 0.005 0.000 1052.5 0.006 0.010 0.000 NodePiece 755.1 0.012 0.018 0.004 565.8 0.042 0.081 0.020 832.2 0.037 0.079 0.013 620.9 0.098 0.166 0.057 NeuralLP 530.3 0.084 0.181 0.035 447.3 0.117 0.273 0.048 802.4 0.101 0.190 0.064 631.8 0.148 0.271 0.101 DRUM 532.6 0.076 0.138 0.044 445.4 0.152 0.313 0.072 803.8 0.107 0.193 0.070 637.1 0.161 0.264 0.119 BLP 564.8 0.019 0.037 0.006 242.5 0.051 0.120 0.012 426.5 0.041 0.093 0.011 332.9 0.049 0.095 0.024 QBLP 754.6 0.004 0.003 0.000 258.8 0.040 0.095 0.007 383.6 0.048 0.097 0.020 287.2 0.073 0.151 0.027 NBFNet 208.2 0.096 0.199 0.032 256.2 0.137 0.255 0.077 332.0 0.225 0.346 0.161 421.8 0.283 0.417 0.224 RED-GNN 201.7 0.212 0.385 0.114 470.1 0.203 0.353 0.129 622.5 0.179 0.280 0.115 403.0 0.214 0.266 0.166 RAILD 598.1 0.018 0.037 0.005 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A INGRAM 92.6 0.309 0.506 0.212 59.1 0.261 0.464 0.167 105.1 0.281 0.453 0.193 90.1 0.334 0.501 0.241 WK-100 WK-75 WK-50 WK-25 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 CompGCN 5861.5 0.003 0.009 0.000 1265.1 0.015 0.028 0.003 3297.4 0.003 0.002 0.001 1591.7 0.009 0.020 0.000 NodePiece 5334.1 0.007 0.018 0.002 800.3 0.021 0.052 0.003 3256.4 0.008 0.013 0.002 814.5 0.053 0.122 0.019 NeuralLP 5665.5 0.009 0.016 0.005 1191.5 0.020 0.054 0.004 4160.8 0.025 0.054 0.007 1384.1 0.068 0.104 0.046 DRUM 5668.0 0.010 0.019 0.004 1192.1 0.020 0.043 0.007 4163.0 0.017 0.046 0.002 1383.2 0.064 0.116 0.035 BLP 3888.1 0.012 0.025 0.003 523.9 0.043 0.089 0.016 1625.7 0.041 0.092 0.013 175.4 0.125 0.283 0.055 QBLP 2863.1 0.012 0.025 0.003 555.0 0.044 0.091 0.016 1371.4 0.035 0.080 0.011 342.0 0.116 0.294 0.042 NBFNet 4030.3 0.014 0.026 0.005 548.1 0.072 0.172 0.028 2874.0 0.062 0.105 0.036 790.5 0.154 0.301 0.092 RED-GNN 5382.4 0.096 0.136 0.070 906.2 0.172 0.290 0.110 3198.3 0.058 0.093 0.033 769.2 0.170 0.263 0.111 RAILD 2005.6 0.026 0.052 0.010 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A INGRAM 1515.7 0.107 0.169 0.072 315.5 0.247 0.362 0.179 1374.1 0.068 0.135 0.034 263.8 0.186 0.309 0.124 FB-100 FB-75 FB-50 FB-25 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 MR MRR Hit@10 Hit@1 CompGCN 1201.2 0.015 0.025 0.008 1211.6 0.013 0.026 0.000 2193.1 0.004 0.006 0.002 1957.4 0.003 0.004 0.000 NodePiece 1131.3 0.006 0.009 0.001 1162.7 0.016 0.029 0.007 1314.3 0.021 0.048 0.006 916.3 0.044 0.114 0.011 NeuralLP 988.2 0.026 0.057 0.007 855.0 0.056 0.099 0.030 1501.9 0.088 0.184 0.043 997.8 0.164 0.309 0.098 DRUM 984.0 0.034 0.077 0.011 853.8 0.065 0.121 0.034 1490.2 0.101 0.191 0.061 992.5 0.175 0.320 0.109 BLP 913.1 0.017 0.035 0.004 705.1 0.047 0.085 0.024 588.5 0.078 0.156 0.037 384.5 0.107 0.212 0.053 QBLP 842.8 0.013 0.026 0.003 798.3 0.041 0.084 0.017 564.9 0.071 0.147 0.030 352.6 0.104 0.226 0.043 NBFNet 451.5 0.072 0.154 0.026 550.8 0.089 0.166 0.048 758.5 0.130 0.259 0.071 571.4 0.224 0.410 0.137 RED-GNN 375.6 0.121 0.263 0.053 890.0 0.107 0.201 0.057 1169.3 0.129 0.251 0.072 1234.1 0.145 0.284 0.077 RAILD 686.0 0.031 0.048 0.016 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A INGRAM 171.5 0.223 0.371 0.146 217.4 0.189 0.325 0.119 580.2 0.117 0.218 0.067 330.3 0.133 0.271 0.067 RAILD is not implemented for the case where both known and new relations are present at inference time, we could not report the results of RAILD for the 75%, 50% and 25% settings. We provide the same training graph for all meth- ods. Given Etr, how to use Etr depends on each method. For example, GraIL uses the entire Etr for training without any split. In INGRAM , we use the dynamic split scheme as described in Section 5.4. For a fair comparison, we feed Finf, Tval and Ttest to all baselines exactly in the same way INGRAM uses. More details about how we run the baseline methods are described in Appendix B. We set d = 32 and bd = 32 for INGRAM and all the baseline methods. Note that we initialize the initial features of rela- tions and entities (xi in Section 5.1 and bxi in Section 5.2) using Glorot initialization in INGRAM . Details about how we tune the hyperparameters of INGRAM are described in Appendix C. 6.2. Inductive Link Prediction We measure the inductive link prediction performance of the methods using standard metrics (Wang et al., 2017): MR (↓), MRR ( ↑), Hit@10 ( ↑), and Hit@1 ( ↑). Table 1 shows the results on 12 different datasets, where all enti- ties are new, and each dataset has a different ratio of new relations. Among the 12 datasets, NL-100, WK-100, and FB-100 have entirely new relations (i.e., inductive infer- ence for relations), whereas the other 9 datasets contain a mixture of new and known relations (i.e., semi-inductive in- ference for relations). In Table 1, we first note thatINGRAM significantly outperforms all the baseline methods in NL- 100, WK-100, and FB-100, which are the most challenging datasets since relations and entities are all new. In these datasets, the performance gap between INGRAM and the best baseline method is considerable in terms of all metrics. This shows that INGRAM is the most effective method for inductive inference for relations. 7INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Table 2: Inductive link prediction performance of known relations and new relations on NL-50. Known Relations New Relations MR MRR Hit@10 MR MRR Hit@10 GraIL 711.8 0.264 0.389 936.0 0.082 0.209 CoMPILE 418.0 0.250 0.383 504.6 0.150 0.288 SNRI 515.8 0.206 0.240 638.4 0.071 0.146 INDIGO 943.6 0.185 0.264 803.4 0.152 0.180 RMPI 492.8 0.192 0.312 468.4 0.180 0.304 CompGCN 1195.6 0.004 0.005 1174.2 0.003 0.004 NodePiece 575.6 0.067 0.151 1033.5 0.016 0.039 NeuralLP 784.5 0.147 0.204 816.3 0.065 0.180 DRUM 787.8 0.146 0.196 816.4 0.076 0.191 BLP 357.1 0.056 0.131 480.8 0.030 0.062 QBLP 271.2 0.073 0.142 471.4 0.029 0.061 NBFNet 317.4 0.231 0.353 350.7 0.217 0.338 RED-GNN 565.3 0.210 0.300 667.2 0.154 0.265 INGRAM 100.7 0.330 0.481 108.5 0.244 0.430 Let us now consider the semi-inductive inference settings. In all NL and WK datasets as well as FB-75 datasets,INGRAM significantly outperforms the baseline methods. We also note that the performance gap between INGRAM and the baseline methods becomes more prominent when the ratio of new relations increases. While INGRAM shows clearly better performance than the baseline methods on 10 out of 12 datasets, some baseline methods such as NBFNet and RED-GNN show better per- formance than INGRAM on FB-25 and FB-50. Indeed, we notice that there exist simple rules between known rela- tions in these datasets, and thus, even a simple rule-based prediction works well on these datasets. This partly ex- plains the performance of NeuralLP, DRUM, RED-GNN, and NBFNet, which are designed to capture rules or patterns between known relations and directly apply them at infer- ence time. Different from these methods, INGRAM does not memorize particular patterns between known relations; instead, INGRAM focuses more on generalizability which is more beneficial for generating embeddings of new relations. In Table 2, we analyze the model performances on triplets with known relations and new relations on NL-50. All meth- ods perform better on known relations than new relations. Also, for the baseline methods, the performance gaps be- tween known and new relations are substantial. We see that the performance of INGRAM is much better than the best baseline methods in all metrics for both known and new relations. 6.3. Inductive Link Prediction with Known Relations Even though INGRAM is designed to consider the case where new relations appear at inference time, we also con- duct experiments on the conventional inductive link predic- tion scenario where all relations are known and all entities are new (Teru et al., 2020). We create a dataset NL-0 that satisfies this constraint, where |Vtr| = 1, 814, |Rtr| = 134, Table 3: Inductive link prediction on NL-0 and NELL-995- v1, where all relations are known and all entities are new. MR MRR Hit@10 Hit@1 NL-0 GraIL 508.2 0.192 0.332 0.114 CoMPILE 561.4 0.229 0.381 0.147 SNRI 561.3 0.117 0.176 0.081 INDIGO 705.6 0.201 0.263 0.166 RMPI 396.1 0.225 0.339 0.158 CompGCN 954.3 0.005 0.009 0.001 NodePiece 345.2 0.094 0.210 0.037 NeuralLP 566.3 0.175 0.326 0.102 DRUM 565.9 0.200 0.343 0.130 BLP 467.2 0.044 0.100 0.011 QBLP 346.2 0.060 0.144 0.013 NBFNet 160.2 0.263 0.430 0.177 RED-GNN 330.9 0.222 0.368 0.147 RAILD 468.4 0.050 0.109 0.014 INGRAM 152.4 0.269 0.431 0.189 NELL-995-v1 GraIL 18.7 0.499 0.595 0.405 CoMPILE 20.1 0.474 0.575 0.390 SNRI 21.2 0.419 0.520 0.330 INDIGO 20.4 0.521 0.595 0.495 RMPI 50.3 0.484 0.545 0.425 CompGCN 11.8 0.282 0.750 0.005 NodePiece 9.8 0.677 0.885 0.550 NeuralLP 33.0 0.547 0.785 0.400 DRUM 33.4 0.536 0.760 0.400 BLP 42.0 0.169 0.470 0.055 QBLP 18.8 0.326 0.545 0.230 NBFNet 7.1 0.613 0.875 0.500 RED-GNN 15.0 0.544 0.705 0.470 RAILD 113.6 0.052 0.205 0.000 INGRAM 6.0 0.739 0.895 0.660 |Etr| = 7 , 796 and |Vinf| = 2 , 026, |Rinf| = 112 , |Einf| = 3, 813. We call this dataset NL-0 since it includes 0% new relations. Also, we take an existing benchmark dataset, NELL-995-v1, from (Teru et al., 2020). We note that different papers did experiments under different settings, even though they used the same benchmark dataset. We conduct our experiments in a setting consistent with our other experiments. For the results on NELL-995-v1, our reproduced results can differ from those reported in the previous literature for the following reasons: (i) we use the validation set inside the “ind-test” set provided in NELL-995-v1, (ii) when measuring the link prediction performance, we use the test set of “ind- test”, (iii) for a fair comparison, we set the embedding dimension to 32 for all methods, and (iv) when conducting link prediction, we set the candidate set to be the entire entity set of the inference graph. Table 3 shows the inductive link prediction results on NL- 0 and NELL-995-v1, where all entities are new and all relations are known. We see that INGRAM outperforms all baselines in all metrics. Even though INGRAM does not learn relation-specific patterns as other baselines do, INGRAM shows reasonable performance on transductive inference for relations while having extra generalization capability to semi-inductive and inductive inferences for relations. 8INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs 1 2 3 4 5 6 7 8 9 10 s(i, j) −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 cs(i, j) NL Datasets NL-50 NL-75 NL-100 (a) Results on NL Datasets 1 2 3 4 5 6 7 8 9 10 s(i, j) −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 cs(i, j) WK Datasets WK-50 WK-75 WK-100 (b) Results on WK Datasets 1 2 3 4 5 6 7 8 9 10 s(i, j) −2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 cs(i, j) FB Datasets FB-50 FB-75 FB-100 (c) Results on FB Datasets Figure 4: Visualization of the cs(i,j) values learned by INGRAM with B = 10. The cs(i,j) value is expected to be large for a small s(i, j), and small for a large s(i, j). See Section 5.1 for more details. Table 4: Ablation Studies of INGRAM . NL-100 WK-100 FB-100 MRR Hit@10 MRR Hit@10 MRR Hit@10 w/ mean aggregator 0.259 0.421 0.047 0.106 0.110 0.184 w/ sum aggregator 0.049 0.082 0.000 0.000 0.001 0.000 w/ self-loop vector 0.133 0.292 0.007 0.015 0.014 0.027 w/o dynamic split 0.234 0.418 0.036 0.100 0.134 0.271 w/o relation update 0.235 0.415 0.057 0.138 0.183 0.310 w/o binning 0.209 0.443 0.070 0.138 0.142 0.292 INGRAM 0.309 0.506 0.107 0.169 0.223 0.371 6.4. Ablation Studies of INGRAM We conduct ablation studies for INGRAM to validate the importance of each module of INGRAM . Specifically, we compare the performance of INGRAM under the following settings: (i) we replace the attention-based aggregation in (1) and (4) with the mean aggregator or (ii) the sum aggre- gator; (iii) when we update an entity representation in (4), we replace ¯z(L) i with a separate learnable self-loop vector as in (Vashishth et al., 2020); (iv) we do not dynamically re-split Ftr and Ttr as described in Section 5.4, i.e., we fix the split during training; (v) we do not update a relation representation vector; (vi) we set B = 1 in (3), i.e., we do not utilize the affinity weights in updating relation represen- tations. The results of these settings are shown in Table 4 in order. Removing each module leads to a noticeable degra- dation in the performance of INGRAM . While some are more critical and others are less, INGRAM achieves the best performance when all modules come together. 6.5. Qualitative Analysis of INGRAM In Section 5.1, when updating the relation vectors using (2), we introduce the learnable parameters cs(i,j) for computing the attention coefficient between ri and rj. By definition of s(i, j) in (3), the value of s(i, j) is small if ri and rj are similar. Since we expect that two similar relations have a high attention value, we expect thatcs(i,j) is large for a small s(i, j), and cs(i,j) is small for a larges(i, j). Figure 4 shows the learned cs(i,j) values according to s(i, j). Even though some exceptions exist, overall, the plots are going down from left to right; the cs(i,j) values are learned as expected. Indeed, when we tune the number of bins B ∈ {1, 5, 10}, INGRAM achieves the best performance when B = 10 , showing the advantage of differentiating bins. 7. Conclusion and Future Work We consider challenging and realistic inductive learning sce- narios where new entities accompany new relations. Our method, INGRAM , can generate embeddings of new rela- tions and entities only appearing at inference time.INGRAM conducts inferences based only on the structure of a given knowledge graph without any extra information about the entities and relations or the aid of rich language models. While existing methods are biased toward learning the pat- terns of known relations, INGRAM focuses more on the generalization capability useful for modeling new relations. We will investigate the ways in which INGRAM can in- corporate known-relation-specific patterns into inferences when known relations are dominant. We also plan to do the theoretical analysis of INGRAM as done in (Xu et al., 2019), (Barcelo et al., 2022) and (Hamilton et al., 2017), as well as consider some extensions to hyper-relational facts (Chung et al., 2023) and bi-level or hierarchical struc- tures (Chung & Whang, 2023; Kwak et al., 2022). Finally, we will look into how we can make the predictions of INGRAM robust and reliable to possibly noisy informa- tion (Hong et al., 2023) in a given knowledge graph. Acknowledgements This research was supported by an NRF grant funded by MSIT 2022R1A2C4001594 (Extendable Graph Representa- tion Learning) and an IITP grant funded by MSIT 2022-0- 00369 (Development of AI Technology to support Expert Decision-making that can Explain the Reasons/Grounds for Judgment Results based on Expert Knowledge). 9INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs References Abboud, R., Ceylan, ˙I. ˙I., Grohe, M., and Lukasiewicz, T. The surprising power of graph neural networks with random node initialization. In Proceedings of the 30th International Joint Conference on Artificial Intelligence, pp. 2112–2118, 2021. Ali, M., Berrendorf, M., Galkin, M., Thost, V ., Ma, T., Tresp, V ., and Lehmann, J. Improving inductive link prediction using hyper-relational facts. In Proceedings of the 20th International Semantic Web Conference, pp. 74–92, 2021. Barcelo, P., Galkin, M., Morris, C., and Orth, M. R. Weis- feiler and Leman go relational. In Proceedings of the 1st Learning on Graphs Conference, 2022. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In Proceedings of the 10th Interna- tional Conference on Learning Representations, 2022. Chen, J., He, H., Wu, F., and Wang, J. Topology-aware cor- relations between relations for inductive link prediction in knowledge graphs. In Proceedings of the 35th AAAI Con- ference on Artificial Intelligence, pp. 6271–6278, 2021. Chung, C. and Whang, J. J. Learning representations of bi-level knowledge graphs for reasoning beyond link pre- diction. In Proceedings of the 37th AAAI Conference on Artificial Intelligence, pp. 4208–4216, 2023. Chung, C., Lee, J., and Whang, J. J. Representation learning on hyper-relational and numeric knowledge graphs with transformers. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 310–322, 2023. Cui, Y ., Wang, Y ., Sun, Z., Liu, W., Jiang, Y ., Han, K., and Hu, W. Inductive knowledge graph reasoning for multi- batch emerging entities. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management, pp. 335–344, 2022. Dai, D., Zheng, H., Luo, F., Yang, P., Liu, T., Sui, Z., and Chang, B. Inductively representing out-of-knowledge- graph entities by optimal estimation under translational assumptions. In Proceedings of the 6th Workshop on Representation Learning for NLP, pp. 83–89, 2021. Daza, D., Cochez, M., and Groth, P. Inductive entity repre- sentations from text via link prediction. In Proceedings of the Web Conference 2021, pp. 798–808, 2021. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pp. 4171–4186, 2019. Galkin, M., Denis, E., Wu, J., and Hamilton, W. L. Node- Piece: Compositional and parameter-efficient representa- tions of large knowledge graphs. In Proceedings of the 10th International Conference on Learning Representa- tions, 2022. Geng, Y ., Chen, J., Pan, J. Z., Chen, M., Jiang, S., Zhang, W., and Chen, H. Relational message passing for fully inductive knowledge graph completion. In Proceedings of the 39th IEEE International Conference on Data Engi- neering, 2023. Gesese, G. A., Sack, H., and Alam, M. RAILD: Towards leveraging relation features for inductive link prediction in knowledge graphs. In Proceedings of the 11th Interna- tional Joint Conference on Knowledge Graphs, 2022. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Proceed- ings of the 13th International Conference on Artificial Intelligence and Statistics, pp. 249–256, 2010. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base enti- ties: A graph neural network approach. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 1802–1808, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Proceedings of the 31st Conference on Neural Information Processing System, pp. 1025–1035, 2017. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770– 778, 2016. Hong, G., Kim, J., Kang, J., Myaeng, S.-H., and Whang, J. J. Discern and answer: Mitigating the impact of mis- information in retrieval-augmented models with discrim- inators. arXiv preprint arXiv:2305.01579 , 2023. doi: 10.48550/arXiv.2305.01579. Ji, S., Pan, S., Cambria, E., Marttinen, P., and Yu, P. S. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems, 33(2):494–514, 2022. Jin, J., Wang, Y ., Du, K., Zhang, W., Zhang, Z., Wipf, D., Yu, Y ., and Gan, Q. Inductive relation prediction using analogy subgraph embeddings. InProceedings of the 10th International Conference on Learning Representations, 2022. 10INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In Proceedings of the 5th International Conference on Learning Represen- tations, 2017. Kwak, J. H., Lee, J., Whang, J. J., and Jo, S. Semantic grasping via a knowledge graph of robotic manipulation: A graph representation learning approach. IEEE Robotics and Automation Letters, 7(4):9397–9404, 2022. Lin, Q., Liu, J., Xu, F., Pan, Y ., Zhu, Y ., Zhang, L., and Zhao, T. Incorporating context graph with logical reasoning for inductive relation prediction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 893–903, 2022. Liu, H., Wu, Y ., and Yang, Y . Analogical inference for multi-relational embeddings. In Proceedings of the 37th International Conference on Machine Learning, pp. 2168– 2178, 2017. Liu, S., Grau, B., Horrocks, I., and Kostylev, E. INDIGO: GNN-based inductive knowledge graph completion using pair-wise encoding. In Proceedings of the 35th Con- ference on Neural Information Processing Systems, pp. 2034–2045, 2021. Maas, A. L., Hannun, A. Y ., and Ng, A. Y . Rectifier nonlinearities improve neural network acoustic models. In ICML 2013 Workshop on Deep Learning for Audio, Speech and Language Processing, 2013. Mai, S., Zheng, S., Yang, Y ., and Hu, H. Communicative message passing for inductive relation reasoning. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, pp. 4294–4302, 2021. Markowitz, E., Balasubramanian, K., Mirtaheri, M., An- navaram, M., Galstyan, A., and Steeg, G. V . StATIK: Structure and text for inductive knowledge graph comple- tion. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 604–615, 2022. Nathani, D., Chauhan, J., Sharma, C., and Kaul, M. Learn- ing attention-based embeddings for relation prediction in knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4710–4723, 2019. Sadeghian, A., Armandpour, M., patrick Ding, and Wang, D. Z. DRUM: End-to-end differentiable rule mining on knowledge graphs. In Proceedings of the 33rd Conference on Neural Information Processing Systems, pp. 15347– 15357, 2019. Sato, R., Yamada, M., and Kashima, H. Random features strengthen graph neural networks. In Proceedings of the 2021 SIAM International Conference on Data Mining, pp. 333–341, 2021. Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., and Welling, M. Modeling relational data with graph convolutional networks. In Proceedings of the 15th International Semantic Web Conference , pp. 593–607, 2018. Sun, Z., Deng, Z.-H., Nie, J.-Y ., and Tang, J. RotatE: Knowl- edge graph embedding by relational rotation in complex space. In Proceedings of the 7th International Conference on Learning Representations, 2019. Teru, K., Denis, E., and Hamilton, W. Inductive relation prediction by subgraph reasoning. In Proceedings of the 37th International Conference on Machine Learning, pp. 9448–9457, 2020. Toutanova, K. and Chen, D. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pp. 57–66, 2015. Vashishth, S., Sanyal, S., Nitin, V ., and Talukdar, P. Composition-based multi-relational graph convolutional networks. In Proceedings of the 8th International Con- ference on Learning Representations, 2020. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Łukasz Kaiser, and Polosukhin, I. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems, pp. 5998–6008, 2017. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In Pro- ceedings of the 6th International Conference on Learning Representations, 2018. Wang, C., Zhou, X., Pan, S., Dong, L., Song, Z., and Sha, Y . Exploring relational semantics for inductive knowledge graph completion. In Proceedings of the 36th AAAI Con- ference on Artificial Intelligence, pp. 4184–4192, 2022. Wang, H., Ren, H., and Leskovec, J. Relational message passing for knowledge graph completion. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1697–1707, 2021. Wang, P., Han, J., Li, C., and Pan, R. Logic attention based neighborhood aggregation for inductive knowledge graph embedding. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence, pp. 7152–7159, 2019. 11INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Wang, Q., Mao, Z., Wang, B., and Guo, L. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724–2743, 2017. Xiong, W., Hoang, T., and Wang, W. Y . DeepPath: A rein- forcement learning method for knowledge graph reason- ing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 564–573, 2017. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In Proceedings of the 7th International Conference on Learning Representations, 2019. Xu, X., Zhang, P., He, Y ., Chao, C., and Yan, C. Subgraph neighboring relations infomax for inductive link predic- tion on knowledge graphs. In Proceedings of the 31st International Joint Conference on Artificial Intelligence, pp. 2341–2347, 2022. Yan, Z., Ma, T., Gao, L., Tang, Z., and Chen, C. Cycle representation learning for inductive relation prediction. In Proceedings of the 39th International Conference on Machine Learning, pp. 24895–24910, 2022. Yang, B., tau Yih, W., He, X., Gao, J., and Deng, L. Embed- ding entities and relations for learning and inference in knowledge bases. In Proceedings of the 3rd International Conference on Learning Representations, 2015. Yang, F., Yang, Z., and Cohen, W. W. Differentiable learn- ing of logical rules for knowledge base reasoning. In Proceedings of the 31st Conference on Neural Informa- tion Processing Systems, pp. 2319–2328, 2017. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y ., and Liu, T.-Y . Do transformers really perform badly for graph representation? In Proceedings of the 35th Conference on Neural Information Processing Systems, pp. 28877–28888, 2021. Zha, H., Chen, Z., and Yan, X. Inductive relation prediction by BERT. In Proceedings of the 36th AAAI Conference on Artificial Intelligence, pp. 5923–5931, 2022. Zhang, Y . and Yao, Q. Knowledge graph reasoning with relational digraph. In Proceedings of the ACM Web Con- ference 2022, pp. 912–924, 2022. Zhu, Z., Zhang, Z., Xhonneux, L.-P., and Tang, J. Neural Bellman-Ford networks: A general graph neural network framework for link prediction. In Proceedings of the 35th Conference on Neural Information Processing System, pp. 29476–29490, 2021. 12INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs A. Generating Datasets for Inductive Knowledge Graph Completion Algorithm 2 Generating Datasets for Inductive Knowledge Graph Completion Input: eG = (V, R, E), ntr, ninf, prel, ptri Output: fGtr = (Vtr, Rtr, Etr) and gGinf = (Vinf, Rinf, Einf) 1: eG ← Giant connected component of eG. 2: Randomly split R into Rtr and Rinf such that |Rtr| : |Rinf| = (1 − prel) : prel. 3: Uniformly sample ntr entities from V and form Vtr by taking the sampled entities and their two-hop neighbors. We select at most 50 neighbors per entity for each hop to prevent exponential growth. 4: Etr := {(vi, r, vj)|vi ∈ Vtr, vj ∈ Vtr, r∈ Rtr, (vi, r, vj) ∈ E}. 5: Etr ← Triplets in the giant connected component of Etr. 6: Vtr ← Entities involved in Etr. 7: Rtr ← Relations involved in Etr. 8: Let eG′ be the subgraph of eG where the entities in Vtr are removed. 9: In eG′, uniformly sample ninf entities and form Vinf by taking the sampled entities and their two-hop neighbors. We select at most 50 neighbors per entity for each hop to prevent exponential growth. 10: Einf := X ∪ Ysuch that |X| : |Y| = (1 − ptri) : ptri where X := {(vi, r, vj)|vi ∈ Vinf, vj ∈ Vinf, r∈ Rtr, (vi, r, vj) ∈ E}and Y := {(vi, r, vj)|vi ∈ Vinf, vj ∈ Vinf, r∈ Rinf, (vi, r, vj) ∈ E}. 11: Einf ← Triplets in the giant connected component of Einf. 12: Vinf ← Entities involved in Einf. 13: Rinf ← Relations involved in Einf. Algorithm 2 shows how we generate the datasets used in Section 6. Also, Table 5 and Table 6 show the statistic of the datasets and the hyperparameters used to create the datasets, respectively. As described in Section 3, Etr is divided into Ftr and Ttr. How to split and use Etr is a model-dependent design choice. On the other hand, Einf is divided into three pairwise disjoint sets, Finf, Tval, and Ttest with a ratio of 3:1:1. For a fair comparison, these three sets are fixed, and the same sets are provided to each model. In Section 4, we mentioned that we add reverse relations and triplets. While this addition is essential for GNN-based methods (Vashishth et al., 2020; Zhang & Yao, 2022), we should add the reverse relations and triplets after we splitFinf, Tval, and Ttest to prevent data leakage problems. Similarly, we add the reverse relations and triplets after we split Ftr and Ttr. B. Details about the Baseline Methods All experiments were conducted with GeForce RTX 2080 Ti, GeForce RTX 3090 or RTX A6000, depending on the implementations of each method. We modified all the baseline models except RAILD and RMPI so that the models accept new relations since they do not consider new relations at inference time. We used the original implementations provided by the authors of the models with minimal modification (if needed) and used the default setting except for the things described below. Since the original implementations of NeuralLP and DRUM include entities in Vtr as candidates for a prediction task at inference time, we excluded them from the candidates for a fair comparison. On the other hand, the implementations of BLP and RAILD restrict the candidates to be the entities involved in Ttest; so we modified this module to consider all entities in Vinf to be candidates. BLP and QBLP require pre-trained vectors for entities and RAILD requires pre-trained vectors for both entities and relations, where the pre-trained vectors are produced by feeding text descriptions or names into BERT (Devlin et al., 2019). Among our datasets, NL does not have text descriptions, FB has text descriptions only for entities, and WK has text descriptions for both entities and relations. We provided the pre-trained vectors with BLP, QBLP and RAILD using the available information. We tuned BLP with learning rate ∈ {0.00001, 0.00002, 0.00005} and L2 regularization coefficient ∈ {0, 0.001, 0.01}. QBLP was tuned with learning rate ∈ {0.0001, 0.0005}, the number of transformer layers ∈ {2, 3, 4} and the number of GCN layers ∈ {2, 3}. For GraIL, CoMPILE and SNRI, we set the early stop patience to be 10 validation trials and the number of total epochs to be 10. Following the original setting of RMPI, we used the Schema Enhanced RMPI for NL-25, NL-50, NL-75, and NL-100, and used the Randomly Initialized RMPI for NL-0 and NELL-995-v1. We tuned RED-GNN with weight decay ∈ {0.00001, 0.01}, dropout rate ∈ {0, 0.3} and the number of layers ∈ {3, 4}. We set the early stop patience to be 10 epochs. 13INGRAM : Inductive Knowledge Graph Embedding via Relation Graphs Since the implementation of RAILD does not contain the codes for obtaining node2vec representations of relations, we used the official C++ implementation of node2vec3 to calculate the representations of relations. The original implementation of CompGCN can only be applied to the transductive setting for both entities and relations. We modified CompGCN so that the model also uses randomly initialized embeddings for new entities appearing at inference time. CompGCN is tuned with the number of GCN layers ∈ {1, 2, 3}, learning rate ∈ {0.0001, 0.001} and the number of bases ∈ {−1, 20, 40}, where -1 denotes the case where each relation has its own embedding. We tuned NodePiece with the size of relational context ∈ {4, 12} and the margin ∈ {15, 20, 25}. Missing Baselines. We could not include PathCon (Wang et al., 2021) and TACT (Chen et al., 2021) in our experiments since their original source codes were written only for relation prediction but not for link prediction. ConGLR (Lin et al., 2022) and CBGNN (Yan et al., 2022) sample 50 negative candidates for each query, following the experimental setting of GraIL. Unlike GraIL, CoMPILE and SNRI, the original implementations of ConGLR and CBGNN do not provide the code for expanding the candidate set to Vinf, so we could not include them as baseline methods. Since the entity sets of training and inference graphs are disjoint in our setting, we could not include baselines assuming new entities should be attached to known entities, such as MEAN (Hamaguchi et al., 2017) and LAN (Wang et al., 2019). We could not include GraphANGEL (Jin et al., 2022) in our experiments because the results in (Jin et al., 2022) are not reproducible. C. Hyperparameters of INGRAM For INGRAM , we performed validation every 200 epochs for a total of 10,000 epochs. We tuned INGRAM with 10 negative samples, d′ ∈ {32, 64, 128, 256}, bd′ ∈ {128, 256}, L ∈ {1, 2, 3}, bL ∈ {2, 3, 4}, K ∈ {8, 16}, bK ∈ {8, 16}, γ ∈ {1.0, 1.5, 2.0, 2.5}, B ∈ {1, 5, 10} and the learning rate ∈ {0.0005, 0.001}. We observed that the best performance of INGRAM is achieved when B = 10, showing the effectiveness of our binning strategy used in (3) described in Section 5.1. Table 5: Datasets for Inductive Knowledge Graph Completion. NL-100 NL-75 NL-50 NL-25 |V| |R| |E| |V| |R| |E| |V| |R| |E| |V| |R| |E| fGtr 1,258 55 7,832 2,607 96 11,058 4,396 106 17,578 4,396 106 17,578 gGinf 1,709 53 3,964 1,578 116 3,031 2,335 119 4,294 2,146 120 3,717 WK-100 WK-75 WK-50 WK-25 |V| |R| |E| |V| |R| |E| |V| |R| |E| |V| |R| |E| fGtr 9,784 67 49,875 6,853 52 28,741 12,022 72 82,481 12,659 47 41,873 gGinf 12,136 37 22,479 2,722 65 5,717 9,328 93 16,121 3,228 74 5,652 FB-100 FB-75 FB-50 FB-25 |V| |R| |E| |V| |R| |E| |V| |R| |E| |V| |R| |E| fGtr 4,659 134 62,809 4,659 134 62,809 5,190 153 85,375 5,190 163 91,571 gGinf 2,624 77 11,645 2,792 186 15,528 4,445 205 19,394 4,097 216 28,579 Table 6: Hyperparameters Used to Create the Datasets. NL-100 NL-75 NL-50 NL-25 NL-0 WK-100 WK-75 WK-50 WK-25 FB-100 FB-75 FB-50 FB-25 ntr 15 60 50 50 20 20 20 30 30 10 10 10 10 ninf 80 50 80 80 80 250 15 80 50 20 20 50 50 prel 0.40 0.40 0.40 0.40 0.00 0.30 0.40 0.30 0.50 0.40 0.40 0.30 0.25 ptri 1.00 0.75 0.50 0.25 0.00 1.00 0.75 0.50 0.25 1.00 0.75 0.50 0.25 3https://github.com/snap-stanford/snap/tree/master/examples/node2vec 14",
      "references": [
        "The surprising power of graph neural networks with random node initialization",
        "Improving inductive link prediction using hyper-relational facts",
        "Weis-feiler and Leman go relational",
        "How attentive are graph attention networks?",
        "Topology-aware correlations between relations for inductive link prediction in knowledge graphs",
        "Learning representations of bi-level knowledge graphs for reasoning beyond link prediction",
        "Representation learning on hyper-relational and numeric knowledge graphs with transformers",
        "Inductive knowledge graph reasoning for multi-batch emerging entities",
        "Inductively representing out-of-knowledge-graph entities by optimal estimation under translational assumptions",
        "Inductive entity representations from text via link prediction",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "Node-Piece: Compositional and parameter-efficient representations of large knowledge graphs",
        "Relational message passing for fully inductive knowledge graph completion",
        "RAILD: Towards leveraging relation features for inductive link prediction in knowledge graphs",
        "Understanding the difficulty of training deep feedforward neural networks",
        "Knowledge transfer for out-of-knowledge-base entities: A graph neural network approach",
        "Inductive representation learning on large graphs",
        "Deep residual learning for image recognition",
        "Discern and answer: Mitigating the impact of misinformation in retrieval-augmented models with discriminators",
        "A survey on knowledge graphs: Representation, acquisition, and applications",
        "Inductive relation prediction using analogy subgraph embeddings",
        "Semi-supervised classification with graph convolutional networks",
        "Semantic grasping via a knowledge graph of robotic manipulation: A graph representation learning approach",
        "Incorporating context graph with logical reasoning for inductive relation prediction",
        "Analogical inference for multi-relational embeddings",
        "INDIGO: GNN-based inductive knowledge graph completion using pair-wise encoding",
        "Rectifier nonlinearities improve neural network acoustic models",
        "Communicative message passing for inductive relation reasoning",
        "StATIK: Structure and text for inductive knowledge graph completion",
        "Learning attention-based embeddings for relation prediction in knowledge graphs",
        "DRUM: End-to-end differentiable rule mining on knowledge graphs",
        "Random features strengthen graph neural networks",
        "Modeling relational data with graph convolutional networks",
        "RotatE: Knowledge graph embedding by relational rotation in complex space",
        "Inductive relation prediction by subgraph reasoning",
        "Observed versus latent features for knowledge base and text inference",
        "Composition-based multi-relational graph convolutional networks",
        "Attention is all you need",
        "Graph attention networks",
        "Exploring relational semantics for inductive knowledge graph completion",
        "Relational message passing for knowledge graph completion",
        "Logic attention based neighborhood aggregation for inductive knowledge graph embedding",
        "Knowledge graph embedding: A survey of approaches and applications",
        "DeepPath: A reinforcement learning method for knowledge graph reasoning",
        "How powerful are graph neural networks?",
        "Subgraph neighboring relations infomax for inductive link prediction on knowledge graphs",
        "Cycle representation learning for inductive relation prediction",
        "Embedding entities and relations for learning and inference in knowledge bases",
        "Differentiable learning of logical rules for knowledge base reasoning",
        "Do transformers really perform badly for graph representation?",
        "Inductive relation prediction by BERT",
        "Knowledge graph reasoning with relational digraph",
        "Neural Bellman-Ford networks: A general graph neural network framework for link prediction"
      ],
      "meta_data": {
        "arxiv_id": "2305.19987v3",
        "authors": [
          "Jaejun Lee",
          "Chanyoung Chung",
          "Joyce Jiyoung Whang"
        ],
        "published_date": "2023-05-31T16:10:42Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses fully inductive knowledge graph completion where both entities and relations can be unseen at inference time (semi-inductive and fully inductive for relations). Proposes INGRAM, a method that generates embeddings for new relations and new entities solely from KG structure by introducing a weighted relation graph and a relation-level aggregation mechanism. Demonstrates substantial gains over 14 baselines across 12 datasets, especially as the proportion of unseen relations increases, and shows scalability advantages over prior fully-inductive approaches (e.g., RMPI).",
        "methodology": "Defines a relation graph whose nodes are relations (including inverse relations) and weighted edges encode relation affinity computed from shared head/tail entities with degree-normalized co-occurrence: A = Eh^T Dh^{-2} Eh + Et^T Dt^{-2} Et. Learns relation embeddings via multi-layer attention aggregation on the relation graph (GATv2-style) with an additional global affinity bias term derived from binning edge weights into B ranks (learnable c_b per bin) to encourage attending to high-affinity relations. Learns entity embeddings via attention aggregation over neighboring entities and adjacent relation representations, incorporating a relation-conditioned message [h_j || z_k]. Uses a DistMult-variant scoring function f(h,r,t)=h_i^T diag(W z_k) h_j with margin ranking loss and negative sampling. Training regime uses per-epoch dynamic re-splitting of training edges into facts/targets (3:1) with constraints (facts contain MST and cover all relations) and re-initializes node/relation features each epoch to improve generalization to unseen graphs; inference constructs a new relation graph on the inference KG and computes embeddings with learned aggregation weights without retraining.",
        "experimental_setup": "Creates 12 inductive datasets from NELL-995, Wikidata68K, and FB15K-237 by splitting entities disjointly between train and inference graphs and controlling the fraction of inference triplets with unseen relations: {100%,75%,50%,25%} new-relation ratio per benchmark (e.g., NL-100 fully unseen relations; NL-75 semi-inductive). All inference entities are unseen (V_tr ∩ V_inf=∅). Inference edges are split into Finf:Tval:Ttest=3:1:1; reverse edges/relations are added after splitting to avoid leakage. Evaluates link prediction with MR (↓), MRR, Hits@10, Hits@1 using candidate set = all V_inf. Compares against 14 baselines (subgraph reasoning, GNN/rule-based, and text/BERT-based methods), with modifications for fair candidate sets; some heavy subgraph baselines only on NL due to scalability. Embedding dimension set to 32 across methods; INGRAM tuned over hidden sizes, layers, heads, margin, bins B, learning rate; validated every 200 epochs up to 10k epochs; ablations on aggregators, dynamic split, relation update, and binning.",
        "limitations": "Relation graph affinity relies on shared entities and frequency; semantically similar relations that do not share entities may not be connected, potentially limiting relation generalization. Uses a DistMult-style diagonal interaction model, which can be less expressive for asymmetric relations despite adding inverse relations. Training involves per-epoch re-initialization and dynamic splits, which may increase training variance and computational cost and lacks theoretical guarantees (acknowledged as future work). Performance can be inferior to rule/path-based baselines when known relations dominate and simple deterministic rules exist (observed on FB-25/FB-50). Requires constructing global relation graph matrices (Eh,Et) which may be memory-intensive for extremely large relation sets, and assumes access to sufficient observed facts Finf containing all entities/relations at inference. Datasets with new relations are synthetically generated; real-world distribution shift and noise patterns may differ.",
        "future_research_directions": "Incorporate known-relation-specific memorization/pattern learning alongside generalizable relation-graph aggregation to improve cases with many known relations. Provide theoretical analysis of INGRAM’s expressivity/generalization, particularly the role of dynamic splits and random feature re-initialization, akin to WL/GNN theory. Extend to hyper-relational facts and more complex KG schemas (n-ary statements, qualifiers) and to bi-level/hierarchical KG structures. Improve robustness and calibration under noisy or incomplete facts in Finf; develop uncertainty estimation and noise-aware relation affinity computation. Explore more expressive scoring functions (e.g., ComplEx/RotatE-style) and alternative relation affinity signals (text when available, temporal/evolving graphs) while keeping inductive capability and scalability.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Inductive Logical Query Answering in Knowledge Graphs",
      "full_text": "Inductive Logical Query Answering in Knowledge Graphs Mikhail Galkin Mila, McGill University mikhail.galkin@mila.quebec Zhaocheng Zhu Mila, Université de Montréal zhuzhaoc@mila.quebec Hongyu Ren Stanford University hyren@stanford.edu Jian Tang Mila, HEC Montréal, CIFAR AI Chair jian.tang@hec.ca Abstract Formulating and answering logical queries is a standard communication interface for knowledge graphs (KGs). Alleviating the notorious incompleteness of real- world KGs, neural methods achieved impressive results in link prediction and com- plex query answering tasks by learning representations of entities, relations, and queries. Still, most existing query answering methods rely on transductive entity embeddings and cannot generalize to KGs containing new entities without retrain- ing the entity embeddings. In this work, we study the inductive query answering task where inference is performed on a graph containing new entities with queries over both seen and unseen entities. To this end, we devise two mechanisms lever- aging inductive node and relational structurerepresentations powered by graph neural networks (GNNs). Experimentally, we show that inductive models are able to perform logical reasoning at inference time over unseen nodes generalizing to graphs up to 500% larger than training ones. Exploring the efﬁciency–effectiveness trade-off, we ﬁnd the inductiverelational structurerepresentation method generally achieves higher performance, while the inductive node representationmethod is able to answer complex queries in the inference-only regime without any train- ing on queries and scales to graphs of millions of nodes. Code is available at https://github.com/DeepGraphLearning/InductiveQE. 1 Introduction Traditionally, querying knowledge graphs (KGs) is performed via databases using structured query languages like SPARQL. Databases can answer complex queries relatively fast under the assumption of completeness, i.e., there is no missing information in the graph. In practice, however, KGs are notoriously incomplete [32]. Embedding-based methods that learn vector representations of entities and relations are known to be effective in simple link predictionpredicting heads or tails of query patterns (head, relation, ?), e.g., (Einstein, graduate, ?), as common in KG completion[1, 17]. Complex queries are graph patterns expressed in a subset of ﬁrst-order logic (FOL) with operators such as intersection (∧), union (∨), negation (¬) and existentially quantiﬁed (∃) variables1, e.g., ?U.∃V : Win(NobelPrize,V ) ∧Citizen(USA,V ) ∧Graduate(V,U ) (Fig. 1). Complex queries deﬁne a superset of KG completion. The conventional KG completion (link prediction) task can be viewed as a complex query with a single triplet pattern without logical operators, e.g., Citizen(USA,V ), which we also denote as a projection query. 1The universal quantiﬁer (∀) is often discarded as in real-world KGs there is no node connected to all others. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2210.08008v2  [cs.AI]  8 Nov 2022Where did US citizens with Nobel Prize graduate? 𝑞𝑞 = 𝑣𝑣. ∃𝑢𝑢: 𝑊𝑊𝑊𝑊𝑊𝑊 𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁 𝑃𝑃𝑃𝑃𝑊𝑊𝑃𝑃𝑁𝑁, 𝑢𝑢 ∧𝐶𝐶𝑊𝑊𝐶𝐶𝑊𝑊𝑃𝑃𝑁𝑁𝑊𝑊 𝑈𝑈𝑈𝑈𝑈𝑈, 𝑢𝑢 ∧𝐺𝐺𝑃𝑃𝐺𝐺𝐺𝐺𝑢𝑢𝐺𝐺𝐶𝐶𝑁𝑁(𝑢𝑢, 𝑣𝑣) 𝒢𝒢𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 Nobel  Prize USA Einstein University  of Zurich ETH Zurich 𝒢𝒢𝑣𝑣𝑡𝑡𝑣𝑣𝑡𝑡𝑣𝑣(𝒢𝒢𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡) (new nodes and edges) Nobel  Prize USA Einstein Feynman Princeton University  of Zurich ETH Zurich win citizen graduate answers Figure 1: Inductive query answering problem: at inference time, the graph is updated with new nodes Feynman and Princeton and edges such that the same query now has more answers. To tackle complex queries on incomplete knowledge graphs, query embeddingmethods are proposed to execute logic operations in the latent space, including variants that employ geometric [15, 23, 38], probabilistic [24, 9], neural-symbolic [ 26, 8, 5], neural [ 21, 4], and GNN [ 11, 3] approaches for learning entity, relation, and query representations. However, this very fact of learning a separate embedding for each entity makes those methods inherently transductive i.e., they are bound to the space of learned entities and cannot generalize to unseen entities without retraining the whole embedding matrix which can be prohibitively expensive in large graphs. The problem is illustrated in Fig. 1: given a graph about Einstein and a logical query Where did US citizens with Nobel Prize graduate?, transductive QE methods learn to execute logical operators and return the answer set {University of Zurich, ETH Zurich}. Then, the graph is updated with new nodes and edges about Feynman and Princeton, and the same query now has more correct answers {University of Zurich, ETH Zurich, Princeton}as new unseen entities satisfy the query as well. Such inductive inferenceis not possible for transductive models as they do not have representations for new Feynman and Princeton nodes. In the extreme case, inference graphs might be disconnected from the training one and only share the set of relations. Therefore, inductive capabilities are a key factor to transferring trained query answering models onto updated or entirely new KGs. In this work, we study answering complex queries in the inductive setting, where the model has to deal with unseen entities at inference time. Inspired by recent advancement in inductive Kg completion [42, 13], we devise two solutions for learning inductive representations for complex query: 1) The ﬁrst solution, NodePiece-QE, extends the inductive node representation model NodePiece [13] to complex query answering. NodePiece-QE learns inductive representations of each entity as a function of tokens from a ﬁxed-size vocabulary, and answers complex query with a non-parametric logical query executor [5]. The advantages of NodePiece-QE are that it only needs to be trained on simple link prediction data, answers complex queries in the inference-only mode, and can scale to large KGs. 2) The second solution, GNN-QE [40], extends the inductive KG completion model NBFNet [42] for complex query answering. Originally, GNN-QE was studied only in the transductive setting. Here, we analyze its inductive capabilities. GNN-QE learns inductive representations of the relational structure without entity embeddings, and uses the relational structure between the query constants and the answers to make the prediction. GNN-QE can be trained end-to-end on complex queries, achieves much better performance than NodePiece-QE, but struggles to scale to large KGs. To the best of our knowledge, this is the ﬁrst work to study complex logical query answering in the inductive setting without any additional features like entity types or textual descriptions. Conducting experiments on a novel benchmarking suite of 10 datasets, we ﬁnd that 1) both inductive solutions exhibit non-trivial performance answering logical queries over unseen entities and query patterns; 2) inductive models demonstrate out-of-distribution generalization capabilities to graphs up to 500% larger than training ones; 3) akin to updatable databases, inductive methods can successfully ﬁnd new correct answers to known training queries after adding new nodes and edges; 4) the inductive node representation method scales to answering logical queries over a graph of 2M nodes with 500k new unseen nodes; 5) GNN-based models still exhibit some difﬁculties [20, 35] generalizing to graphs larger than those they were originally trained on. 22 Related Work Knowledge Graph Completion. Knowledge graph completion, a.k.a. simple link prediction, has been widely studied in the transductive paradigm [6, 33, 27, 37], i.e., when training and inference are performed on the same graph with a ﬁxed set of entities. Generally, these methods learn a shallow embedding vector for each entity. We refer the audience to respective surveys [ 1, 17] covering dozens of transductive embedding methods. The emergence of message passing [ 14] and graph neural networks (GNNs) has led to more advanced, inductive representation learning approaches that model entity or triplet representations as a function of the graph structure in its neighborhood. GraIL [28] learns triplet representations based on the subgraph structure surrounding the two entities. NeuralLP [34], DRUM [25] and NBFNet [ 42] learn the pairwise entity representations based on the set of relation paths between two entities. NodePiece [13] learns entity representations from a ﬁxed-size vocabulary of tokens that can be anchor nodes in a graph or relation types. Complex Query Answering. In the complex (multi-hop) query answering setup with logical opera- tors, existing models employ different approaches, e.g., geometric [15, 23, 38], probabilistic [24, 9], neural-symbolic [26, 8, 5], neural [21, 4], and GNN [11, 3]. Still, all the approaches are created and evaluated exclusively in the transductive mode where the set of entities does not change at inference time. To the best of our knowledge, there is no related work in inductive logical query answering when the inference graph contains new entities. With our work, we aim to bridge this gap and extend inductive representation learning algorithms to logical query answering. In particular, we focus on the inductive setup where an inference graph is a superset of a training graph2 such that 1) inference queries require reasoning over both seen and new entities; 2) original training queries might have more correct answers at inference time with the addition of new entities. 3 Preliminaries and Problem Deﬁnition Knowledge Graph and Inductive Setup. Given a ﬁnite set of entities E, a ﬁnite set of relations R, and a set of triples (edges) T = (E×R×E ), a knowledge graph Gis deﬁned as G= (E,R,T). Accounting for the inductive setup, we deﬁne a training graph Gtrain = (Etrain,R,Ttrain) and an inference graph Ginf = (Einf,R,Tinf) such that Etrain ⊂Einf and Ttrain ⊂Tinf. That is, the inference graph extends the training graph with new entities and edges3.The inference graphGinf is an incomplete part of the not observable complete graph ˆGinf = (Einf,R, ˆTinf) with ˆTinf = Tinf ∪Tpred whose missing triples Tpred have to be predicted at inference time. First-Order Logic Queries. Applied to KGs, a ﬁrst-order logic (FOL) query Qis a formula that consists of constants C(C⊆E ), variables V(V⊆E , existentially quantiﬁed), relation projections R(a,b) denoting a binary function over constants or variables, and logic symbols (∃,∧,∨,¬). The answers AG(Q) to the query Qare assignments of variables in a formula such that the instantiated query formula is a subgraph of the complete graph ˆG. Fig. 1 illustrates the logical form of a query Where did US citizens with Nobel Prize graduate?as ?U.∃V : Win(NobelPrize,V )∧Citizen(USA,V )∧Graduate(V,U ) where NobelPrize and USA are constants; Win, Citizen, Graduate are relation projections(labeled edges); V,U - variables such that V is an existentially quantiﬁed free variable and U is the projected bound target variable of the query. Common for the literature, we aim at predicting assignments of the query target whereas assignments of intermediate variables might not always be explicitly interpreted depending on the model architecture. In the example, the answer set AG(Q) is a binding of a target variable U to constants University of Zurich and ETH Zurich. Inductive FOL Queries. In the standard transductive query answering setup, query constants and variables at both training and inference time belong to the same set of entities, i.e., Ctrain = Cinf ⊆ E,Vtrain = Vinf ⊆E. In the inductive setup covered in this work, query constants and variables at inference time belong to a different and larger set of entities Einf from the inference graph Ginf, i.e., Ctrain ⊆Etrain,Vtrain ⊆Etrain but Cinf ⊆Einf,Vinf ⊆Einf. This also leads to the fact that training queries executed over the inference graph might have more correct answers, i.e., AGtrain (Q) ⊆AGinf (Q). For example (cf. Fig. 1), the inference graph is updated with new nodes Feynman, Princeton and their 2The set of relation types is ﬁxed. 3Note that the set of relation types R remains the same. 3new respective edges. The same query now has a larger set of intermediate variables satisfying the formula (Feynman) and an additional correct answerPrinceton. Therefore, inductive generalization is essential for obtaining representations of such new nodes and enabling logical reasoning over both seen and new nodes, i.e., ﬁnding more answers to known queries in larger graphs or answering new queries with new constants. In the following section, we describe two approaches for achieving inductive generalization with different parameterization strategies. 4 Method Inductive Representations of Complex Queries. Given a complex query Q= (C,RQ,G), the goal is to rank all possible entities according to the query. From a representation learning perspective, this requires us to learn a conditional representation function f(e|C,RQ,G) for each entity e∈E. Transductive methods learn a shallow embedding for each answer entitye∈E, and, therefore, cannot generalize to unseen entities. For inductive methods, the function f(e|C,RQ,G) should generalize to some unseen answer entity e′(or unseen constant entity c′∈C′) at inference time. Here, we discuss two solutions for devising such an inductive function. The ﬁrst solution is to parameterize the representation of each entity e as a function of an invariant vocabulary of tokens that does not change at training and inference. Particularly, the vocabulary might consist of unique relation types Rthat are always the same for Gtrain and Ginf, and we are able to infer the representation of an unseen answer entity (or an unseen constant entity) as a function of its incident relations (cf. Fig. 2 left). The idea has been studied in NodePiece [ 13] for simple link prediction. Here, we adopt a similar idea to learn inductive entity representations for complex query answering. Once we obtain the representations for unseen entities, we can use any off-the-shelf decoding method (e.g., CQD-Beam [5]) for predicting the answer to the complex query. We denote this strategy as NodePiece-QE. The second solution is to parameterize f(e|C,RQ,G) as a function of the relational structure . Intuitively, an answer of a complex query can be decided solely based on the relational structure between the query constants and the answer (Fig. 1). Even after anonymizing entity names (and, hence, not learning any explicit entity embedding), we can still infer Princeton as an answer since it forms a distinctive relational structure  with the query constants and conforms to the query structure. Similarly, intermediate nodes will be deemed correct if they follow a relational structure . In other words, we do not need to know the answer node is Princeton, but only need to know the relative position of Princeton w.r.t. the constants like Nobel Prize and USA. Based on this idea, we design f(e|C,RQ,G) to be a relational structure search function. Such an idea has been studied in Neural Bellman-Ford Networks (NBFNet) [42] to search for a single relation in simple link prediction. Applied to complex queries, GNN-QE [40] chains several NBFNet instances with differentiable logic operations to learn inductive complex query in an end-to-end fashion. So far, GNN-QE was evaluated solely on transductive tasks. Here we extend it to the inductive setup. 4.1 NodePiece-QE: Inductive Node Representation Here we aim at reconstructing node representations for seen and unseen entities without learning shallow node embedding vectors. To this end, we employ NodePiece [13], a compositional tokeniza- tion approach that learns an invariant vocabulary of tokens shared between training and inference graphs. Formally, given a vocabulary of tokens ti ∈T, each entity ei is deterministically hashed into a set of representative tokens ei = [t1,...,t k]. An entity vector ei is then obtained as a function of token embeddings ei = fθ([ti,..., tk]),ti ∈Rd where the encoder function fθ : Rk×d →Rd is parameterized with a neural network θ. Since the set of relation types Ris invariant for training and inference graphs, we can learn relation embeddings R ∈R|R|×d and our vocabulary of learnable tokens T is comprised of distinct relation types such that entities are hashed into a set of unique incident relation types. For example (cf. Fig. 2 left), a middle node from a training graph Gtrain is hashed with a set of relations ei = [  ] that stands for two unique incoming relations  and one unique outgoing relation  . Passing the hashes through fθ, we can reconstruct the whole entity embedding matrix E ∈R|Etrain|×d. Additionally, it is possible to enrich entity and relation embeddings by passing them through a relational GNN encoder [31] over a target graph G: E′,R′= GNN(E,R,G). In both ways, the entity embedding matrix E encodes a joint probability distribution p(h,r,t) for all triples in a graph. 4𝒢𝒢𝑣𝑣𝑣𝑣𝑣𝑣𝑣𝑣𝑣𝑣 (𝒢𝒢𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡) NodePiece-QE node representations through relation context 𝒢𝒢𝑡𝑡𝑡𝑡𝑣𝑣𝑣𝑣𝑡𝑡 CQD Beam (non-parameteric decoder) trained with link prediction GNN-QE 𝒢𝒢𝑣𝑣𝑣𝑣𝑣𝑣𝑣𝑣𝑣𝑣 (𝒢𝒢𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡) relation structure w.r.t. anchors nodes in the query 𝒢𝒢𝑡𝑡𝑡𝑡𝑣𝑣𝑣𝑣𝑡𝑡 Binary  Classifier trained with complex query Figure 2: Inductive node representation (NodePiece-QE, left) and relational structure (GNN-QE, right) strategies for complex logical query answering. In NodePiece-QE, we obtain inductive node representations through the invariant set of tokens (here, through incident relation types). NodePiece- QE is an inference-only approach, pre-trained with simple 1p link prediction and can be directly applied to inductive complex queries with a non-parametric decoder (e.g., CQD Beam). In GNN-QE, we learn the the relative structure of each node w.r.t. the anchor nodes in the query. GNN-QE is trainable end-to-end on complex queries. Having a uniform featurization mechanism for both seen and unseen entities, it is now possible to apply any previously-transductive complex query answering model with learnable entity embeddings and logical operators [ 23, 11, 24, 8]. Moreover, it was recently shown [ 5] that a combination of simple link prediction pre-training and a non-parametric logical executor allows to effectively answer complex FOL queries in the inference-only regime without training on any complex query sample. We adopt this Continuous Query Decomposition algorithm with beam search (CQD-Beam) as the main query answering decoder. CQD-Beam relies only on entity and relation embeddings E,R pre-trained on a simple 1p link prediction task. Then, given a complex query, CQD-Beam applies t-norms and t-conorms [19] that execute conjunctions (∧) and disjunctions (∨) as non-parametric algebraic operations in the embedding space, respectively. In our inductive setup (Fig. 2), we train a NodePiece encoder fθ and relation embeddings R (and optionally a GNN) on the 1p link prediction task over the training graph Gtrain. We then apply the learned encoder to materialize entity representations of the inference graph E ∈R|Einf|×d and send them to CQD-Beam that performs a non-parametric decoding of complex FOL queries over new unseen entities. The inference-only nature of NodePiece-QE is designed to probe the abilities for zero-shot generalization in performing complex logical reasoning over larger graphs. 4.2 GNN-QE: Inductive Relational Structure Representation The second strategy relies on learning inductive relational structure representations instead of explicit node representations. Having the same set of relation types Rat training and inference time, we can parameterize each entity based on the relative relational structure between it and the anchor nodes in a given query. For instance (Fig. 2 right), given a query with a particular relational structure and a set of anchor nodes, the representation of each node captures its relational structure relative to the anchor nodes. Each neighborhood expansion step is equivalent to a projection step in a complex query. In our example, immediate neighboring nodes will capture the intersection pattern  , and further nodes, in turn, capture the extended intersection-projection structure  . Therefore, a node is likely to be an answer if its captured (or predicted) relational structure conforms with the query relational structure. As long as the set of relations is ﬁxed, relation projection is performed in the same way for training or new unseen nodes. The idea of a one-hop (1p) projection for simple link prediction has been proposed by Neural Bellman-Ford Networks (NBFNet) [42]. In particular, given a relation projection query (h,r,?), NBFNet assigns unique initial states h(0) to all nodes in a graph by applying an indicator functionh(0) = INDICATOR (h,v,r ), i.e., a head node h is initialized with a learnable relation embeddingr and all other nodes are initialized with zeros. Then, NBFNet applies Lrelational message passing GNN layers where each layer lhas its own learnable relation embedding matrix R(l) obtained as a projection (and reshaping) of thein initial relation 5R(l) = W(l)r + b(l). Final layer representations h(L) are passed through an MLP and the sigmoid function σto get a probability distribution over all nodes in a graph p(t|h,r) =σ(MLP(h(L))). As each projection query spawns a uniquely initialized graph and message passing procedure, NBFNet is seen to be applying a labeling trick[36] to model a conditional probability distribution p(t|h,r) that is provably more expressive than a joint distribution p(h,r,t) produced by standard graph encoders. Applied to complex queries, chaining k NBFNet instances allows us to answer k-hop projection queries, e.g., two instances for 2p queries. GNN-QE employs NBFNet as a trainable projection operator and endows it with differentiable, non-parametric product logic for modeling conjunction (∧), disjunction (∨), and negation (¬) over the fuzzy setsof all entities x ∈[0,1]E, i.e., after applying a logical operator (discussed in Appendix A), each entity’s degree of truth is associated with a scalar in range [0,1]. For the i-th hop projection, the indicator function initializes a node state h(0) e with a relation vector ri weighted by a scalar probability predicted in the previous hop xe: h(0) e = xeri. Differentiable logical operators allow training GNN-QE end-to-end on complex queries. 5 Experiments We designed the experimental agenda to demonstrate that inductive representation strategies are able to: 1) answer complex logical queries over new, unseen entities at inference time, i.e., when query anchors are new nodes (Section 5.2); 2) predict new correct answers for knowntraining queries when executed over larger inference graphs, i.e., when query anchors come from the training graph but variables and answers belong to the larger inference graph (Section 5.3); 3) generalize to inference graphs of up to 500% larger than training graphs; 4) scale to inductive query answering over graphs of millions of nodes when updated with 500k new nodes and 5M new edges (Section 5.5). 5.1 Setup & Dataset Dataset. Due to the absence of inductive logical query benchmarks, we create a novel suite of datasets based on FB15k-237 [ 29] (open license) and following the query generation process of BetaE [24]. Given a source graph with Eentities, we sample |Etrain|= r·|E|,r ∈[0.1,0.9] nodes to induce a training graph Gtrain. For validation and test graphs, we split the remaining set of entities into two non-overlapping sets each with 1−r 2 |E|nodes. We then merge training and unseen nodes into the inference set of nodes Einf and induce inference graphs for validation and test from those sets, respectively, i.e., Eval inf = Etrain ∪Eval and Etest inf = Etrain ∪Etest. That is, validation and test inference graphs both extend the training graph but their sets of new entities are disjoint. Finally, we sample and remove 15% of edges Tpred in the inference graphs as missing edges for sampling queries with those missing edges. Overall, we sample 9 such datasets based on different choices of r, which result in the ratios of inference graph size to the training graph Einf/Etrain from 105% to 550%. For each dataset, we employ the query sampler from BetaE [ 24] to extract 14 typical query types 1p/2p/3p/2i/3i/ip/pi/2u/up/2in/3in/inp/pin/pni. Training queries are sampled from the training graph Gtrain, validation and test queries are sampled from their respective inference graphs Ginf where at least one edge belongs to Tpred and has to be predicted at inference time. As inference graphs extend training graphs, training queries are very likely to have new answers when executed over Ginf with simple graph traversal and without any link prediction. We create an additional set of true answers for all training queries executed over the test inference graph Gtest inf to measure the entailment capabilities of query answering models. This is designed to be an inference task and extends the faithfullness evaluation of [26]. Dataset statistics can be found in Appendix B. Evaluation Protocol. Following the literature [24], query answers are separated into two sets: easy answers that only require graph traversal over existing edges, and hard answersthat require inferring missing links to achieve the answer node. For the main experiment, evaluation involves ranking of hard answers against all entities having easy ones ﬁltered out. For evaluating training queries on inference graphs, we only have easy answers and rank them against all entities. We report Hits@10 as the main performance metric on different query types. Implementation Details. All NodePiece-based models [ 13] were pre-trained until convergence on a simple 1p link prediction task with the relations-only vocabulary and entity tokenization, MLP encoder, and ComplEx [ 30] scoring function. We used a 2-layer CompGCN [ 31] as an 6100 200 300 400 500 Ratio inf/ train, % 0.1 0.2 0.3 0.4 0.5metric Metric = Hits@10 | query = EPFO avg 100 200 300 400 500 Ratio inf/ train, % 0.1 0.2 0.3 Metric = Hits@10 | query = neg avg Model NodePiece-QE NodePiece-QE w/ GNN GNN-QE Edge-type Heuristic Figure 3: Aggregated Hits@10 performance of test queries (involving unseen entities) executed on inference graphs of different ratios compared to training graphs. NodePiece-based models are inference-only and support EPFO queries, GNN-QE is trainable and supports negation queries. Table 1: Test Hits@10 results (%) on answering inductive FOL queries when Einf/Etrain = 175%. avgp is the average on EPFO queries (∧, ∨). avgn is the average on queries with negation. Model avg p avgn 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni Transductive BetaE 1.3 0.2 2.9 0.4 0.4 2.1 3.3 1.5 0.7 0.2 0.2 0.1 0.2 0.2 0.1 0.1 Inductive Inference-only Edge-type Heuristic 10.1 4.1 17.7 8.2 9.9 10.7 13.0 9.8 8.2 5.3 8.5 2.6 2.9 8.4 3.8 2.7NodePiece-QE 11.2 - 25.5 8.2 8.4 12.4 13.9 9.9 8.7 7.0 6.8 - - - - -NodePiece-QE w/ GNN 28.6 - 45.9 19.2 11.5 39.9 48.8 29.4 22.6 25.3 14.6 - - - - - Inductive Trainable GNN-QE 50.7 33.6 65.4 36.3 31.6 73.8 84.3 56.5 41.5 39.3 28.0 33.3 46.4 29.2 24.9 34.0 optional message passing encoder on top of NodePiece features. The non-parametric CQD-Beam [ 5] decoder for answering complex queries is tuned for each query type based on the validation set of queries, most of the setups employ a product t-norm, sigmoid entity score normalization, and beam size of 32. Following the literature, the GNN-QE models [ 40] were trained on 10 query patterns (1p/2p/3p/2i/3i/2in/3in/inp/pin/pni) where ip/pi/2u/up are only seen at inference time. Each model employs a 4-layer NBFNet [42] as a trainable projection operator with DistMult [33] composition function and PNA [10] aggregation. Other logical operators ( ∧,∨,¬) are executed with the non- parametric product t-normand t-conorm. Both NodePiece-QE and GNN-QE are implemented4 with PyTorch [22] and trained with the Adam [18] optimizer. NodePiece-QE models were pre-trained and evaluated on a single Tesla V100 32 GB GPU whereas GNN-QE models were trained and evaluated on 4 Tesla V100 16GB. All hyperparameters are listed in Appendix D. To show that the proposed models are non-trivial, we compare them with an Edge-type Heuristicbaseline (Appendix E), which selects all entities that satisfy the relations in the last hop of the query in Ginf. 5.2 Complex Query Answering over Unseen Entities on Differently Sized Inference Graphs First, we probe inference-only NodePiece-based embedding models and trainable GNN-QE in the inductive setup, i.e., query answering over unseen nodes requiring link prediction over unseen nodes. As a sanity check, we compare them to the Edge-type Heuristic and a transductive BetaE model [24] trained with standard hyperparameters (Appendix D) on the reference dataset (with ratio Einf/Etrain of 175%) with randomly initialized embeddings for unseen nodes at inference time. Table 1 summarizes the results on the reference dataset while Fig. 3 illustrates a bigger picture on all datasets (we provide a detailed breakdown by query type for all splits in Appendix C). The experiment on the tranductive BetaE conﬁrms that pure transductive models can not generalize to graphs with unseen nodes. With inductive models, however, we observe that even inference-only models pre-trained solely on simple 1p link prediction exhibit non-trivial performance in answering queries with unseen entities. Particularly, the inference-only NodePiece with GNNbaseline exhibits better performance over all query types and inference graphs up to 300% larger than training graphs. 4Code and data are available at https://github.com/DeepGraphLearning/InductiveQE 7100 200 300 400 500 Ratio inf/ train, % 0.2 0.4 0.6 0.8 1.0metric query = EPFO avg 100 200 300 400 500 Ratio inf/ train, % 0.2 0.4 0.6 0.8 1.0 query = neg avg Model NodePiece-QE NodePiece-QE w/ GNN GNN-QE Edge-type Heuristic Graph train test Figure 4: Aggregated Hits@10 performance oftraining queries on the original training and extended test inference graphs where queries have new correct answers. NodePiece-based models areinference- only and support EPFO queries, GNN-QE is trainable and supports negation queries. The trainable GNN-QE models expectedly outperform non-trainable baselines and can tackle queries with negation (¬). Here, we conﬁrm that the labeling trick[36] and conditional p(t|h,r) modeling better capture the relation projection problem than joint p(h,r,t) encoding approaches. Still, we notice that models with GNNs, i.e., inference-only NodePiece-QE with GNN and trainable GNN-QE, suffer from increasing the size of the inference graph and having more unseen entities. Reaching best results on Einf/Etrain ratios around 130%, both approaches steadily deteriorate up until ﬁnal 550% by 20 absolute Hits@10 points on EPFO queries and negation queries. We attribute this deterioration to the known generalization issues [20, 35] of message passing GNNs when performing inference over a larger graph than the network has seen during training. Recently, a few strategies have been proposed [ 7, 39] to alleviate this issue and we see it as a promising avenue for future work. On the other hand, a simple NodePiece-QE model without message passing retains similar performance independently of the inference graph size. Lastly, we observe that lower performance of inference-only NodePiece models can be also attributed to underﬁtting (cf. train graph charts in Fig. 4). Although 1p link predictors were trained until convergence (on the inductive validation set of missing triples), the performance of training queries on training graphs with easy answersthat require only relation traversal without predicting missing edges is not yet saturated. This fact suggests that better ﬁtting entity featurization (obtained by NodePiece or other strategies) could further improve the test performance in the inference-only regime. We leave the search of such strategies for future work. 5.3 Predicting New Answers for Training Queries on Larger Inference Graphs Simulating the incremental addition of new edges in graph databases, we evaluate the performance of our inference-only and trainable QE models on training queries on the original training graph and extended inference graph (with added test edges). As databases are able to immediately retrieve new answers to known queries after updating the graph, we aim at exploring and quantifying this behaviour of neural reasoning models. In this experiment, we probe training queries and their easy answers that require performing only graph traversal without predicting missing links in the inference graph. While execution of training queries over the training graph indicates how well the model could ﬁt training data, executing training queries over the bigger inference graph with new entities aims to capture basic reasoning capabilities of QE models in the inductive regime. Particular challenges arising when executing training queries over a bigger graph are: (1) the same queries can have more correct answers as more new nodes and edges satisfying the query pattern might have been added (as in Fig. 1); (2) more new entities create a “distractor” setting with more false positives. Generally, evaluation of training queries on the inference graph can be considered as an extended version of the faithfullness [26] evaluation that captures how well a trained model can answer original training queries, i.e., memorization capacity. In all 9 datasets, most of training queries have at least one new correct answer in the inference graph (more details in Appendix B). 8Fig. 4 illustrates the performance of the Edge-type Heuristic baseline, inference-only NodePiece-QE (without and with GNN) and trainable GNN-QE models. Generally, GNN-QE ﬁts the training query data almost perfectly conﬁrming the original ﬁnding [42] that NBFNet performs graph traversal akin to symbolic models. GNN-QE can also ﬁnd new correct answers on graphs up to 300% larger than training ones. Then, the performance deteriorates which we attribute to the distractor factor with more unseen entities and the mentioned generalization issue on larger inference graphs. The inference-only NodePiece-QE models, as expected, do not fully ﬁt the training data as they were never trained on complex queries. Still, the inference-only models exhibit non-trivial performance (compared to the Edge-type Heuristic) in ﬁnding more answers on graphs up to 200% larger than training ones with relatively small performance margins compared to training queries. The most surprising observation is that GNN-free NodePiece-QE models improve the performance on both training and inference graphs as the graphs (and the Einf/Etrain ratio) grow larger while GNN-enriched models steadily deteriorate. We attribute this growth to the relation-based NodePiece tokenization and its learned features that tend to be more discriminative in larger inference graphs where new nodes have smaller degree and thus can be better identiﬁed by their incident relation types. We provide more experimental results for each dataset ratio with breakdown by query type in Appendix C. 5.4 Ranking of Easy and Hard Answers In addition to evaluating faithfullness that measures whether a model could recover easy answers, it is also insightful to measure whether all easy answers can be ranked higher than hard answers. That is, a reliable query answering model would ﬁrst recover all possible easy answers and would enrich the answer set with highly-probable hard answers. To this end, we apply a ROC AUC metric over original unﬁltered scores. The ROC AUC score measures how many hard answers are ranked after easy answers. Note that the score does not depend on actual values of ranks, that is, the metric will be high when easy answers are ranked, e.g., in between 100-1000 as long as hard answers are ranked 1001 and lower. Therefore, ROC AUC still needs to be paired with MRR to see how easy and hard answers are ranked absolutely. Table 2: Macro-averaged ROC AUC score overunﬁltered predictions on the reference Einf/Etrain = 175% dataset to measure if all easy answers are ranked higher than hard answers. Higher is better. Model avg p avgn 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni Inductive Inference-only NodePiece-QE 0.692 0.623 0.710 0.711 0.657 0.654 0.692 0.731 0.723 0.729NodePiece-QE w/ GNN 0.776 0.783 0.783 0.739 0.758 0.733 0.760 0.801 0.841 0.787 Inductive Trainable GNN-QE 0.973 0.885 0.998 0.992 0.986 0.969 0.962 0.967 0.969 0.938 0.978 0.879 0.859 0.926 0.914 0.847 We compute ROC AUC for each query and average them over each query type thus making it macro-averaged ROC AUC. Our experimental results on all query types using the models reported in Table 1 on the reference 175% dataset are compiled in Table 2. GNN-QE has nearly perfect ROC AUC scores as it was trained on complex queries. NodePiece-QE models are acceptable for inference-only models that were only trained only on 1p simple link prediction and have never seen any complex query at training time. 5.5 Scaling to Millions of Nodes on WikiKG-QE Finally, we perform a scalability experiment evaluating complex query answering in the inductive mode on a new large dataset WikiKG-QE constructed from OGB WikiKG 2 [ 16] (CC0 license). While the original task is transductive link prediction, we split the graph into a training graph of 1.5M entities (5.8M edges, 512 unique relation types) and validation (test) graphs of 500k unseen nodes (5M known and 600k missing edges) each. The resulting validation (test) inference graphs are therefore of 2M entities and 11M edges with the Einf/Etrain ratio of 133% (details are in Appendix B). GNN-QE cannot scale to such sizes, so we only probe NodePiece-QE models. Due to the problem size, we only sample 10k EPFO queries of each type from the test inferencegraph to run in the inference-only regime. Each query has at least one missing edge to be predicted at inference. The answers are ranked against all 2M entities in the ﬁltered setting (in contrast to the OGB task that ranks against 1000 pre-computed negative samples) and Hits@100 as the target metric. 9We pre-train a NodePiece encoder (in addition to relation types, we tokenize nodes with a vocabulary of 20k nodes, total 3M parameters in the encoder) with the ComplEx decoder on 1p link prediction over the training graph for 1M steps (see Appendix D for hyperparameters). Then, the graph is extended with 500k new nodes and 5M new edges forming the inference graph. Then, using the pre-trained encoder, we materialize representations of entities (both seen and new) and relations from this inference graph. Finally, CQD-Beam executes the queries against the bigger inference graph extended with 500k new nodes and 5M new edges. Table 3: Test Hits@100 of NodePiece-QE on WikiKG-QE (2M nodes, 11M edges including 500k new nodes and 5M new edges) in the inference-only regime. avgp is the average on EPFO queries. Model avg p 1p 2p 3p 2i 3i pi ip 2u up Edge-type Heuristic 3.1 10.0 1.0 0.9 3.7 8.1 1.8 0.9 0.7 0.5 NodePiece-QE 9.2 22.6 5.2 3.9 11.6 17.4 7.0 4.5 7.4 3.2 NodePiece-QE w/ GNN 10.1 66.6 0.9 0.6 5.4 8.2 2.3 0.8 5.2 0.5 As shown in Table 3, we ﬁnd a non-trivial performance of the inference-only model on EPFO queries demonstrating that inductive node representationQE models are able to scale to graphs with hundreds of thousands of new nodes and millions of new edges in the zero-shot fashion. That is, answering complex queries over unseen entities is available right upon updating the graph without the need to retrain a model. This fact paves the way for the concept of neural graph databasescapable of performing zero-shot inference over updatable graphs without expensive retraining. 6 Limitations and Future Work Limitations. With the two proposed inductive query answering strategies, we observe a common trade-off between the performance and computational complexity. That is, inductive node repre- sentation models like NodePiece-QE are fast, scalable, and can be executed in the inference-only regime but underperform compared to the inductive relational structure representationmodels like GNN-QE. On the other hand, GNN-QE incurs high computational costs due to executing each query on a uniquely initialized graph instance. Alleviating this issue is a key to scalability. Societal Impact. The inductive setup assumes running inference on (partly) unseen data, that is, the nature of this unseen data might be out-of-distrbution, unknown and potentially malicious. This fact has to be taken into account when evaluating predictions and overall system trustworthiness. Conclusion and Future Work. In this work, we deﬁned the problem of inductive complex logical query answering and proposed two possible parameterization strategies based on node and relational structure representations to deal with new, unseen entities at inference time. Experiments demon- strated that both strategies are able to answer complex logical queries over unseen entities as well as identify new answers on larger inference graphs. In the future work, we plan to extend the inductive setup to completely disjoint training and inference graphs, expand the set of supported logical query patterns aligned with popular queries over real-world KGs, enable reasoning over continuous features like texts and numbers, support more KG modalities like hypergraphs and hyper-relational graphs, and further explore the concept of neural graph databases. Acknowledgments and Disclosure of Funding This project is supported by the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R&D Project (AI4D-CORE-06). This project was also partially funded by IV ADO Fundamental Research Project grant PRF-2019-3583139727. The computation resource of this project is supported by Calcul Québec5 and Compute Canada6. We would like to thank anonymous reviewers for the comments that helped to improve the manuscript. 5https://www.calculquebec.ca/ 6https://www.computecanada.ca/ 10References [1] Mehdi Ali, Max Berrendorf, Charles Tapley Hoyt, Laurent Vermue, Mikhail Galkin, Sahand Sharifzadeh, Asja Fischer, V olker Tresp, and Jens Lehmann. Bringing light into the dark: A large-scale evaluation of knowledge graph embedding models under a uniﬁed framework. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [2] Mehdi Ali, Max Berrendorf, Charles Tapley Hoyt, Laurent Vermue, Sahand Sharifzadeh, V olker Tresp, and Jens Lehmann. PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings. Journal of Machine Learning Research, 22(82):1–6, 2021. [3] Dimitrios Alivanistos, Max Berrendorf, Michael Cochez, and Mikhail Galkin. Query embedding on hyper-relational knowledge graphs. InInternational Conference on Learning Representations, 2022. [4] Alfonso Amayuelas, Shuai Zhang, Xi Susie Rao, and Ce Zhang. Neural methods for logical reasoning over knowledge graphs. In International Conference on Learning Representations, 2022. [5] Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. Complex query answering with neural link predictors. In International Conference on Learning Representations, 2021. [6] Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013, pages 2787–2795, 2013. [7] Davide Buffelli, Pietro Liò, and Fabio Vandin. SizeShiftReg: a regularization method for improving size-generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2022. [8] Xuelu Chen, Ziniu Hu, and Yizhou Sun. Fuzzy logic based logical query answering on knowledge graph. In International Conference on Machine Learning. PMLR, 2021. [9] Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chandan K. Reddy. Probabilistic entity representation model for reasoning over knowledge graphs. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. [10] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Veliˇckovi´c. Principal neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33:13260–13271, 2020. [11] Daniel Daza and Michael Cochez. Message passing query embedding. arXiv preprint arXiv:2002.02406, 2020. [12] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. [13] Mikhail Galkin, Etienne Denis, Jiapeng Wu, and William L. Hamilton. Nodepiece: Compo- sitional and parameter-efﬁcient representations of large knowledge graphs. In International Conference on Learning Representations, 2022. [14] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, volume 70 of Proceedings of Machine Learning Research, pages 1263–1272. PMLR, 2017. [15] Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding logical queries on knowledge graphs. Advances in Neural Information Processing Systems, 31, 2018. [16] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020. [17] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. A survey on knowledge graphs: Representation, acquisition and applications. IEEE Transactions on Neural Networks and Learning Systems, 33(2):494–514, 2022. 11[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. [19] Erich-Peter Klement, Radko Mesiar, and Endre Pap. Triangular norms. position paper I: basic analytical and algebraic properties. Fuzzy Sets Syst., 143(1):5–26, 2004. [20] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, pages 4204–4214, 2019. [21] Bhushan Kotnis, Carolin Lawrence, and Mathias Niepert. Answering complex queries in knowledge graphs with bidirectional sequence encoders. CoRR, abs/2004.02596, 2020. [22] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, pages 8024–8035, 2019. [23] Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in vector space using box embeddings. In International Conference on Learning Representations, 2019. [24] Hongyu Ren and Jure Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge graphs. Advances in Neural Information Processing Systems, 33, 2020. [25] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum: End-to-end differentiable rule mining on knowledge graphs. Advances in Neural Information Processing Systems, 32, 2019. [26] Haitian Sun, Andrew O. Arnold, Tania Bedrax-Weiss, Fernando Pereira, and William W. Cohen. Faithful embeddings for knowledge base queries. InAdvances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020. [27] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph em- bedding by relational rotation in complex space. In 7th International Conference on Learning Representations, ICLR 2019. OpenReview.net, 2019. [28] Komal K. Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, volume 119 of Proceedings of Machine Learning Research, pages 9448–9457. PMLR, 2020. [29] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pages 57–66, Beijing, China, July 2015. Association for Computational Linguistics. [30] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International conference on machine learning, pages 2071–2080. PMLR, 2016. [31] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based multi-relational graph convolutional networks. In International Conference on Learning Repre- sentations, 2020. [32] Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. Knowledge base completion via search-based question answering. In 23rd International World Wide Web Conference, WWW ’14, pages 515–526. ACM, 2014. [33] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In 3rd International Conference on Learning Representations, ICLR 2015, 2015. 12[34] Fan Yang, Zhilin Yang, and William W. Cohen. Differentiable learning of logical rules for knowledge base reasoning. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, pages 2319–2328, 2017. [35] Gilad Yehudai, Ethan Fetaya, Eli A. Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In Proceedings of the 38th Interna- tional Conference on Machine Learning, ICML 2021, volume 139 of Proceedings of Machine Learning Research, pages 11975–11986. PMLR, 2021. [36] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph neural networks for multi-node representation learning. In Advances in Neural Information Processing Systems, volume 34, pages 9061–9073. Curran Associates, Inc., 2021. [37] Shuai Zhang, Yi Tay, Lina Yao, and Qi Liu. Quaternion knowledge graph embeddings. In Ad- vances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, pages 2731–2741, 2019. [38] Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. Cone: Cone embeddings for multi-hop reasoning over knowledge graphs. Advances in Neural Information Processing Systems, 34, 2021. [39] Yangze Zhou, Gitta Kutyniok, and Bruno Ribeiro. Ood link prediction generalization capabilities of message-passing gnns in larger test graphs. In Advances in Neural Information Processing Systems, 2022. [40] Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. Neural-symbolic models for logical queries on knowledge graphs. In International Conference on Machine Learning, ICML 2022, Proceedings of Machine Learning Research. PMLR, 2022. [41] Zhaocheng Zhu, Chence Shi, Zuobai Zhang, Shengchao Liu, Minghao Xu, Xinyu Yuan, Yang- tian Zhang, Junkun Chen, Huiyu Cai, Jiarui Lu, Chang Ma, Runcheng Liu, Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Torchdrug: A powerful and ﬂexible machine learning platform for drug discovery, 2022. [42] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. Advances in Neural Information Processing Systems, 34, 2021. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 6 (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6 (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main exper- imental results (either in the supplemental material or as a URL)? [Yes] Code and sample data are included in the supplementary material (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Dataset creation process is described in Section 5.1 with more details in Appendix B. Hyperparameters are speciﬁed in Appendix D. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [No] We observe negligible variance w.r.t. random seeds (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Training details are speciﬁed in Section 5.1 and in Appendix D. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] Due to the overall size, we include a sample of the benchmarking suite in the supplemental material and will openly publish the whole dataset. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] No personal data involved (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [Yes] The datasets are anonymized, we discuss it in Appendix B. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14A Differentiable Logical Operators T-norms (⊤) and t-conorms (⊥) are fuzzy versions of conjunction (∧) and disjunction (∨), respectively. Fuzzy operators can be applied to vectors of continuous values within a certain range, e.g., [0,1]d, depending on the chosen fuzzy logic, and are executed as algebraic operations which makes them differentiable. Different fuzzy logics implement different t-norms and t-conorms. In this work, we experiment with two such logics: product logicand Gödel (min) logic. In the product logic, conjunction C, disjunction D, and negation Nare modeled as follows: C(x,y) =x ⊙y D(x,y) =x + y −x ⊙y N(x) =1 −x where inputs x,y ∈ [0,1]d are d-dimensional vectors with values in the range [0,1], ⊙is the element-wise multiplication, and 1 is the universe vector of all ones. In the Gödel logic, conjunction Cand disjunction Dare modeled as min and max, respectively: C(x,y) =min(x,y) D(x,y) =max(x,y) For GNN-QE we employ solely the product logic for end-to-end training on all types of complex queries. For NodePiece-QE and its inference-only mechanism based on CQD-Beam, we may select the best performing logic for each query type based on the validation set. The chosen operators for NodePiece-QE are reported in Table 13 in Appedix D. B Benchmarking Datasets Details We sampled 9 datasets (used in Section 5.2 and Section 5.3) from the original FB15k-237 [29] with already added inverse edges for ensuring reachability and connectedness of the underlying graph for the subsequent query sampling. Creation details are provided in the Section 5.1 and statistics on the sampled graphs are presented in Table 4. Varying the ratio of entities in the inference graph to the training graph Einf/Etrain, we aim at measuring inductive capabilities of proposed strategies in the out-of-distribution size generalization scenario. To measure scalability of inductive query answering approaches, we create WikiKG-QE, an inductive split of the originally transductive OGB WikiKG 2 [16], following the same sampling strategy as for 9 Freebase datasets. Table 4: Sampled graphs statistics for various ratios Einf/Etrain. Originally inverse triples are included in all graphs except WikiKG-QE. R- number of unique relation types, E- number of entities in various splits, T - number of triples. Validation and Test splits contain an inference graph (Einf,Tinf) which is a superset of the training graph with new nodes, and missing edges to predict Tpred. Ratio, % R E total Training Graph Validation Graph Test Graph Etrain Ttrain Evalinf Tvalinf Tvalpred Etestinf Ttestinf Ttestpred 106% 466 14,512 13,091 493,425 13,801 551,336 10,219 13,802 538,896 8,023 113% 468 14,442 11,601 401,677 13,022 491,518 15,849 13,021 486,068 14,893 122% 466 14,444 10,184 298,879 12,314 413,554 20,231 12,314 430,892 23,289 134% 466 14,305 8,634 228,729 11,468 373,262 25,477 11,471 367,810 24,529 150% 462 14,333 7,232 162,683 10,783 311,462 26,235 10,782 331,352 29,755 175% 436 14,022 5,560 102,521 9,801 265,412 28,691 9,781 266,494 28,891 217% 446 13,986 4,134 52,455 9,062 227,284 30,809 9,058 212,386 28,177 300% 412 13,868 2,650 24,439 8,252 178,680 27,135 8,266 187,156 28,657 550% 312 13,438 1,084 5,265 7,247 136,558 22,981 7,275 133,524 22,503 WikiKG-QE 133% 512 2,492,122 1,494,033 5,824,868 1,992,739 9,466,319 638,389 1,993,416 10,510,906 824,713 15In all datasets, entities and relations are anonymized and only have an integer ID. Furthermode, inference graphs at validation and test time are supersets of the respective training graph with new nodes and edges. The amount of new unique nodes is simply the difference Einf −Etrain between entities in those graphs, e.g., for the dataset of ratio 175%, the validation inference graph contains 4,241 new nodes and test inference graph contains 4,221 news nodes. Note that those 4,241 and 4,221 nodes are unique for each graph and do not overlap. That is, validation inference and test inference graphs are disconnected except sharing the same core training graph. Then, for each created inductive dataset, we sample queries of 14 query patterns following the BetaE [24] procedure. That is, training queries are sampled from the training graph Gtrain and have only easy answers reachable by simple edge traversal. Validation and test queries are sampled from the respective splits, e.g., validation queries are sampled from the validation graph Gval using entities from the validation inference graph Eval inf (which, in turn, are a union of training nodes and new, unseen validation nodes Etrain ∪Eval), and at least one edge in each query belongs to Tval pred and has to be predicted during query execution. Queries might have easy answers that are directly reachable by traversing edges Tval inf in the validation inference graph, whereas hard answers are only reachable after predicting missing edges from the set Tval pred. Final evaluation metrics are computed only based on the hard answers. Following the literature [24], we only retain queries that have less than 1000 answers. Table 5 summarizes the statistics on the sampled queries for each dataset ratio, each graph, and query type that we use in Section 5.2 for evaluating inductive query answering performance. In graphs with smaller inference graphs and smaller number of missing triples, we sample fewer queries with negation (2in, 3in, inp, pin, pni) for validation and test splits. For WikiKG-QE, due to its size, we only sample 10k EPFO queries to be executed in the inference-only regime without training (at the moment, CQD-Beam does not support queries with negation). We use those queries in Section 5.5 to evaluate scalability of NodePiece-QE and prediction quality in the inference-only mode. Table 5: Statistics on sampled queries for each dataset ratio and query type. For WikiKG-QE, we only sample EPFO queries without negation. Ratio Graph 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni 106% training 135,613 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 40,000 50,000 50,000 50,000validation 6,582 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 1,000 1,000 1,000 1,000 1,000test 5,446 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 1,000 1,000 1,000 1,000 1,000 113% training 115,523 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 40,000 50,000 50,000 50,000validation 10,256 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 1,000 1,000 1,000 1,000 1,000test 9,782 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 1,000 1,000 1,000 1,000 1,000 122% training 91,228 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 40,000 50,000 50,000 50,000validation 12,696 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 5,000 5,000 5,000 5,000 5,000test 14,458 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 5,000 5,000 5,000 5,000 5,000 134% training 75,326 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 40,000 50,000 50,000 50,000validation 15,541 50,000 50,000 50,000 50,000 50,000 50,000 20,000 20,000 5,000 5,000 5,000 5,000 5,000test 15,270 50,000 50,000 50,000 50,000 50,000 50,000 20,000 20,000 5,000 5,000 5,000 5,000 5,000 150% training 56,114 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 40,000 50,000 50,000 50,000validation 16,229 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 5,000 5,000 5,000 5,000 5,000test 17,683 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 5,000 5,000 5,000 5,000 5,000 175% training 38,851 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 40,000 50,000 50,000 50,000validation 17,235 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 10,000 10,000 10,000 10,000 10,000test 17,476 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 10,000 10,000 10,000 10,000 10,000 217% training 22,422 30,000 30,000 50,000 50,000 50,000 50,000 50,000 50,000 30,000 30,000 50,000 50,000 50,000validation 18,168 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 10,000 10,000 10,000 10,000 10,000test 16,902 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 10,000 10,000 10,000 10,000 10,000 300% training 11,699 15,000 15,000 40,000 40,000 50,000 50,000 50,000 50,000 15,000 15,000 50,000 40,000 50,000validation 16,189 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 10,000 10,000 10,000 10,000 10,000test 17,105 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 10,000 10,000 10,000 10,000 10,000 550% training 3,284 15,000 15,000 40,000 40,000 50,000 50,000 50,000 50,000 10,000 10,000 30,000 30,000 30,000validation 13,616 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 10,000 10,000 10,000 10,000 10,000test 13,670 50,000 50,000 50,000 50,000 50,000 50,000 50,000 50,000 10,000 10,000 10,000 10,000 10,000 WikiKG-QE 133% training 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 - - - - -validation 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 - - - - -test 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 - - - - - Furthermore, for the experiment in Section 5.3 to measure the abilities of inductive models to ﬁnd new answers of known queries, we take the created training queries and ﬁnd their easy answers in the validation inference Gval inf = (Eval inf ,Tval inf ) and test inference Gtest inf = (Etest inf ,Ttest inf ) graphs. That is, those new answers do not require predicting missing edges in the inference graphs and only require a model to execute edge traversal to ﬁnd (if any) new correct answers involving new, unseen entities and edges. For the validation (test) split, we only count such training queries qwhose answer set in 16Table 6: Statistics on training EPFO queries that have a different (often, larger) answer set when executed against validation and test inference graphs. We list the original number of training queries, number of those queries with new easy answers in the validation (In val) and test graphs (In test), as well as their percentage ratio to the total number. Most queries (except 2i,3i) have new answer sets. Ratio Graph1p 2p 3p 2i 3i pi ip 2u up #Q % #Q % #Q % #Q % #Q % #Q % #Q % #Q % #Q % 106%Train 135,613 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 14,079 10.4 32,220 64.4 40,860 81.7 7,598 15.2 4,416 8.8 16,485 33.0 29,290 58.6 33,507 67.0 41,671 83.3In test 11,560 8.5 31,894 63.8 40,547 81.1 7,313 14.6 4,175 8.4 16,204 32.4 28,778 57.6 32,978 66.0 41,167 82.3 113%Train 115,523 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 17,792 15.4 36,499 73.0 43,473 86.9 10,517 21.0 6,394 12.8 20,556 41.1 33,599 67.2 37,955 75.9 44,318 88.6In test 17,576 15.2 36,721 73.4 43,541 87.1 10,552 21.1 6,303 12.6 20,382 40.8 33,726 67.5 38,107 76.2 44,501 89.0 122%Train 91,228 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 20,281 22.2 38,642 77.3 44,654 89.3 11,695 23.4 5,851 11.7 22,662 45.3 35,935 71.9 40,356 80.7 45,672 91.3In test 20,418 22.4 38,706 77.4 44,688 89.4 11,847 23.7 6,185 12.4 22,524 45.0 35,768 71.5 40,395 80.8 45,684 91.4 134%Train 75,326 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 18,909 25.1 39,893 79.8 45,253 90.5 14,256 28.5 8,655 17.3 24,619 49.2 37,835 75.7 41,899 83.8 46,114 92.2In test 19,372 25.7 39,762 79.5 45,325 90.7 14,082 28.2 8,790 17.6 24,212 48.4 37,527 75.1 41,494 83.0 46,210 92.4 150%Train 56,114 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 17,434 31.1 40,666 81.3 45,832 91.7 14,103 28.2 8,011 16.0 25,106 50.2 38,499 77.0 42,587 85.2 46,754 93.5In test 18,566 33.1 41,202 82.4 46,092 92.2 14,575 29.2 8,193 16.4 25,782 51.6 38,932 77.9 43,101 86.2 46,791 93.6 175%Train 38,851 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 14,063 36.2 41,290 82.6 46,214 92.4 15,645 31.3 9,222 18.4 27,205 54.4 40,161 80.3 44,128 88.3 47,366 94.7In test 14,214 36.6 41,143 82.3 46,061 92.1 15,731 31.5 9,391 18.8 27,207 54.4 40,297 80.6 43,980 88.0 47,319 94.6 217%Train 22,422 100.0 30,000 100.0 30,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 10,437 46.5 24,659 82.2 26,760 89.2 13,784 27.6 7,807 15.6 24,884 49.8 39,107 78.2 43,496 87.0 46,112 92.2In test 10,257 45.7 24,344 81.1 26,579 88.6 14,055 28.1 7,962 15.9 24,962 49.9 38,966 77.9 43,092 86.2 45,850 91.7 300%Train 11,699 100.0 15,000 100.0 15,000 100.0 40,000 100.0 40,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 5,830 49.8 12,366 82.4 13,230 88.2 12,833 32.1 7,911 19.8 27,920 55.8 40,800 81.6 43,516 87.0 46,453 92.9In test 6,061 51.8 12,477 83.2 13,309 88.7 13,291 33.2 8,284 20.7 28,447 56.9 41,214 82.4 43,966 87.9 46,668 93.3 550%Train 3,284 100.0 15,000 100.0 15,000 100.0 40,000 100.0 40,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0In val 1,885 57.4 11,484 76.6 12,575 83.8 11,119 27.8 6,617 16.5 23,126 46.3 39,243 78.5 38,129 76.3 45,173 90.3In test 1,883 57.3 11,597 77.3 12,654 84.4 11,244 28.1 6,795 17.0 23,575 47.2 39,630 79.3 37,508 75.0 45,412 90.8 Table 7: Statistics on training negation queries that have a different (often, larger) answer set when executed against validation and test inference graphs. We list the original number of training queries, number of those queries with new easy answers in the validation (In val) and test graphs (In test), as well as their percentage ratio to the total number. Most queries have new answer sets. Ratio Graph 2in 3in pin pni inp #Q % #Q % #Q % #Q % #Q % 106% Train 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 In val 25,318 50.6 18,232 36.5 37,857 75.7 27,572 55.1 37,497 75.0 In test 25,111 50.2 18,237 36.5 37,441 74.9 27,535 55.1 37,176 74.4 113% Train 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 In val 31,216 62.4 24,620 49.2 42,015 84.0 33,011 66.0 41,980 84.0 In test 31,437 62.9 24,665 49.3 42,255 84.5 33,115 66.2 42,296 84.6 122% Train 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 In val 34,722 69.4 26,700 53.4 44,104 88.2 36,361 72.7 44,070 88.1 In test 35,028 70.1 27,105 54.2 44,089 88.2 36,398 72.8 44,074 88.1 134% Train 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 In val 38,096 76.2 31,631 63.3 45,672 91.3 39,641 79.3 45,491 91.0 In test 37,469 74.9 31,224 62.4 45,521 91.0 38,971 77.9 45,418 90.8 150% Train 50,000 100.0 40,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 In val 39,836 79.7 26,534 66.3 46,561 93.1 40,733 81.5 46,496 93.0 In test 40,127 80.3 26,968 67.4 46,832 93.7 41,100 82.2 46,811 93.6 175% Train 50,000 100.0 40,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 In val 42,418 84.8 29,083 72.7 47,666 95.3 42,987 86.0 47,606 95.2 In test 42,379 84.8 29,170 72.9 47,749 95.5 42,941 85.9 47,557 95.1 217% Train 30,000 100.0 30,000 100.0 50,000 100.0 50,000 100.0 50,000 100.0 In val 26,202 87.3 21,751 72.5 47,879 95.8 43,958 87.9 47,688 95.4 In test 26,080 86.9 21,591 72.0 47,655 95.3 43,837 87.7 47,417 94.8 300% Train 15,000 100.0 15,000 100.0 50,000 100.0 40,000 100.0 50,000 100.0 In val 13,595 90.6 11,996 80.0 48,693 97.4 36,427 91.1 48,279 96.6 In test 13,659 91.1 12,098 80.7 48,791 97.6 36,507 91.3 48,440 96.9 550% Train 10,000 100.0 10,000 100.0 30,000 100.0 30,000 100.0 30,000 100.0 In val 9,232 92.3 8,071 80.7 29,484 98.3 27,975 93.3 29,393 98.0 In test 9,137 91.4 8,053 80.5 29,510 98.4 27,839 92.8 29,218 97.4 17this split is different from the answer set in the training graph, e.g., Aval q ̸= Atrain q . We summarize the statistics of identiﬁed new answer sets in all datasets in Table 6 (for EPFO queries) and Table 7 (for queries with negations). We ﬁnd that in most query patterns across all dataset ratios, training queries indeed have new answer sets when executed against validation or test inference graphs. C More Experimental Results Here, we present a detailed breakdown of query answering performance measured in Sections 5.2 and 5.3 by query type. Fig. 5 and Table 8 contain detailed results from Section 5.2 of executing test queries with new, unseen entities over inference graphs of various ratios of new entities. 0.1 0.2 0.3 0.4 0.5Hits@10 query = EPFO avg 0.1 0.2 0.3 query = neg avg 0.2 0.3 0.4 0.5 0.6 0.7 query = 1p 0.1 0.2 0.3 0.4Hits@10 query = 2p 0.05 0.10 0.15 0.20 0.25 0.30 query = 3p 0.2 0.4 0.6 query = 2i 0.2 0.4 0.6 0.8Hits@10 query = 3i 0.1 0.2 0.3 0.4 0.5 0.6 query = pi 0.1 0.2 0.3 0.4 query = ip 0.1 0.2 0.3 0.4 0.5Hits@10 query = 2u 0.05 0.10 0.15 0.20 0.25 0.30 query = up 0.0 0.1 0.2 0.3 0.4 query = 2in 0.0 0.1 0.2 0.3 0.4Hits@10 query = 3in 100 200 300 400 500 Ratio inf/ train, % 0.05 0.10 0.15 0.20 0.25 0.30 query = inp 100 200 300 400 500 Ratio inf/ train, % 0.05 0.10 0.15 0.20 0.25 0.30 query = pin 100 200 300 400 500 Ratio inf/ train, % 0.0 0.1 0.2 0.3 0.4 0.5Hits@10 query = pni Model NodePiece-QE NodePiece-QE w/ GNN GNN-QE Edge-type Heuristic Figure 5: Hits@10 results on answering test inductive FOL queries on all ratios Einf/Etrain. 18Table 8: Test Hits@3 and Hits@10 results (%) on answering test inductive FOL queries on all ratios Einf/Etrain. avgp is the average on EPFO queries (∧, ∨). avgn is the average on queries with negation. Ratio Model Metric avgp avgn 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni 550% Edge-type HeuristicHits@3 5.0 2.3 5.9 4.7 5.2 5.3 7.0 5.1 4.3 2.8 4.5 1.5 1.7 4.9 2.0 1.4Hits@10 11.7 5.1 15.8 10.4 11.5 13.4 16.4 12.2 9.7 6.3 9.2 3.4 3.7 11.2 3.9 3.4 NodePiece-QE Hits@3 4.3 7.3 4.0 4.1 4.3 4.5 3.8 3.6 3.4 3.3Hits@10 9.6 16.3 8.4 8.8 10.8 11.5 8.8 7.7 6.8 7.0 NodePiece-QE w/ GNNHits@3 5.4 9.2 4.1 3.1 6.8 7.4 5.1 4.5 5.4 3.4Hits@10 11.1 20.1 8.3 6.2 14.0 15.5 10.5 8.7 10.3 6.6 GNN-QE Hits@3 24.2 9.7 28.3 15.8 10.1 37.7 60.9 31.3 14.4 10.1 8.8 9.0 16.5 9.8 7.7 5.6Hits@10 33.1 15.8 37.7 23.4 17.0 50.7 74.9 43.3 20.4 15.1 15.4 13.4 26.3 17.2 13.7 8.6 300% Edge-type HeuristicHits@3 5.5 2.7 10.3 5.1 5.4 5.0 6.3 5.0 4.8 2.7 5.1 1.6 1.9 4.8 2.5 2.5Hits@10 12.2 5.8 20.9 10.9 11.6 12.1 14.8 11.6 10.8 6.4 10.3 3.6 4.1 11.2 5.1 5.0 NodePiece-QE Hits@3 5.4 12.0 4.7 4.6 4.9 5.2 4.2 4.6 3.9 4.0Hits@10 10.7 19.6 9.5 9.5 11.0 12.0 9.7 9.5 7.5 8.3 NodePiece-QE w/ GNNHits@3 9.7 18.9 7.3 4.1 13.1 15.0 9.1 7.3 7.2 5.5Hits@10 17.9 31.5 13.7 8.2 24.3 27.0 18.0 13.7 13.8 10.6 GNN-QE Hits@3 31.8 13.5 41.5 21.0 16.1 51.7 66.7 37.2 25.2 13.5 13.3 11.7 21.3 14.0 12.7 7.7Hits@10 42.6 22.4 50.9 32.1 26.4 65.4 78.1 49.5 35.7 21.7 23.4 19.3 33.1 24.6 21.3 13.7 217% Edge-type HeuristicHits@3 5.5 2.4 10.3 4.8 4.8 5.1 7.2 5.4 4.7 2.1 4.6 1.2 2.3 4.3 2.2 2.2Hits@10 11.5 5.4 18.8 9.9 10.2 12.1 16.1 11.9 9.8 4.8 9.4 2.9 4.7 10.2 4.5 4.6 NodePiece-QE Hits@3 5.9 13.9 4.8 4.4 5.8 6.7 5.3 5.1 3.4 4.0Hits@10 11.7 22.3 9.3 8.8 12.9 15.5 11.5 10.0 7.1 7.8 NodePiece-QE w/ GNNHits@3 13.6 25.7 8.8 5.3 18.7 24.8 12.9 10.3 8.9 6.9Hits@10 23.5 41.0 16.0 10.4 32.5 40.8 23.7 17.7 16.9 12.6 GNN-QE Hits@3 37.9 19.2 50.6 24.4 19.3 58.6 76.2 45.1 31.4 19.7 16.0 17.6 32.6 18.3 14.0 13.6Hits@10 49.2 30.1 61.3 36.1 29.8 72.6 86.8 58.4 42.5 29.3 25.8 26.7 47.4 30.3 22.7 23.3 175% Edge-type HeuristicHits@3 4.7 1.7 8.4 3.8 4.8 4.5 5.6 4.3 4.0 2.5 4.2 1.0 1.2 3.3 1.8 1.0Hits@10 10.1 4.1 17.7 8.2 9.9 10.7 13.0 9.8 8.2 5.3 8.5 2.6 2.9 8.4 3.8 2.7 NodePiece-QE Hits@3 5.6 14.2 4.1 4.1 5.6 6.2 4.5 4.6 3.5 3.3Hits@10 11.2 25.5 8.2 8.4 12.4 13.9 9.9 8.7 7.0 6.8 NodePiece-QE w/ GNNHits@3 17.2 30.7 10.7 5.9 24.4 31.2 17.2 13.1 14.2 7.3Hits@10 28.6 45.9 19.2 11.5 39.9 48.8 29.4 22.6 25.3 14.6 GNN-QE Hits@3 38.5 20.5 52.8 24.1 20.6 59.8 73.3 43.2 30.0 24.4 17.9 18.9 32.2 17.8 15.3 18.2Hits@10 50.7 33.6 65.4 36.3 31.6 73.8 84.3 56.5 41.5 39.3 28.0 33.3 46.4 29.2 24.9 34.0 150% Edge-type HeuristicHits@3 4.4 1.9 9.2 3.6 4.0 4.3 5.3 3.9 3.5 1.8 3.8 1.3 1.5 3.5 2.1 1.1Hits@10 9.6 4.4 17.4 7.9 8.7 10.4 12.7 9.0 7.7 4.5 8.0 2.8 3.6 8.7 4.2 2.5 NodePiece-QE Hits@3 5.4 14.0 4.5 4.1 5.0 5.5 4.0 4.6 3.0 3.8Hits@10 10.8 22.8 8.9 8.5 11.7 12.9 9.1 9.1 6.3 7.7 NodePiece-QE w/ GNNHits@3 15.7 33.1 9.7 4.6 22.3 26.9 14.8 11.4 12.3 6.5Hits@10 25.9 48.9 17.2 9.1 36.6 43.1 25.1 19.7 21.1 12.2 GNN-QE Hits@3 37.3 18.1 56.6 23.6 18.9 58.6 69.8 39.6 27.3 23.2 18.0 16.9 25.7 16.6 16.2 15.4Hits@10 49.3 30.3 69.1 35.7 29.7 73.1 81.3 52.9 38.7 34.3 28.7 28.3 40.1 27.8 27.7 27.7 133% Edge-type HeuristicHits@3 4.0 1.9 8.6 3.5 3.2 4.3 4.9 3.4 3.5 1.8 3.2 1.2 1.6 2.7 2.0 1.1Hits@10 9.0 4.2 17.7 7.3 7.1 10.1 11.8 8.0 7.5 4.3 7.0 2.6 2.9 7.4 3.8 2.5 NodePiece-QE Hits@3 5.1 15.4 4.8 3.5 4.4 4.1 2.9 4.8 2.6 3.4Hits@10 10.2 24.8 9.3 7.7 10.1 9.9 7.4 9.3 5.6 7.5 NodePiece-QE w/ GNNHits@3 19.4 38.0 12.6 5.2 27.0 32.3 17.9 16.0 16.7 8.7Hits@10 29.6 52.1 20.6 9.4 42.3 49.2 29.3 24.3 25.5 14.2 GNN-QE Hits@3 38.8 21.4 56.3 25.6 19.8 59.3 68.5 40.6 30.6 28.4 19.8 23.0 25.9 16.4 18.3 23.6Hits@10 51.4 34.1 69.2 38.7 31.1 73.4 79.9 53.8 43.7 42.2 30.4 35.6 40.3 27.8 28.6 38.1 121% Edge-type HeuristicHits@3 4.3 1.5 14.7 3.0 3.2 3.0 3.9 2.8 2.8 1.5 3.3 0.9 1.0 2.6 1.7 1.2Hits@10 8.6 3.7 23.3 6.5 6.9 7.6 9.5 6.7 6.4 3.7 6.9 2.2 2.4 7.2 3.5 3.0 NodePiece-QE Hits@3 4.6 16.0 3.2 2.7 3.7 4.3 3.1 3.5 2.1 2.8Hits@10 9.6 28.0 6.5 6.1 9.2 10.7 7.8 7.1 4.9 6.0 NodePiece-QE w/ GNNHits@3 18.4 39.7 10.6 4.8 24.8 30.6 16.4 13.9 16.8 7.8Hits@10 27.9 53.6 17.1 8.4 38.0 45.9 26.7 21.9 26.1 13.9 GNN-QE Hits@3 35.3 18.9 62.0 21.1 17.9 50.0 59.5 36.8 26.7 27.4 16.6 20.2 23.4 15.4 13.8 21.5Hits@10 46.2 29.1 74.2 31.1 27.2 63.4 70.5 48.1 37.3 38.3 25.4 30.5 35.2 25.5 21.8 32.6 113% Edge-type HeuristicHits@3 3.1 1.0 8.5 1.8 2.3 3.3 4.5 2.5 1.9 1.3 2.1 0.5 1.3 1.5 1.2 0.4Hits@10 7.0 2.4 15.2 4.3 5.5 8.2 10.4 6.4 4.4 3.4 5.1 1.1 2.4 4.2 2.9 1.6 NodePiece-QE Hits@3 4.0 13.6 3.2 3.2 3.4 3.8 2.4 2.6 1.7 2.7Hits@10 8.1 21.5 6.4 6.4 7.8 8.6 5.9 5.9 3.8 6.3 NodePiece-QE w/ GNNHits@3 18.1 39.1 10.2 3.8 25.9 31.0 17.2 13.8 15.8 6.4Hits@10 27.2 53.6 16.0 7.3 39.2 46.7 26.2 21.1 24.0 11.0 GNN-QE Hits@3 38.1 22.7 58.6 24.5 22.3 53.0 62.1 39.0 28.4 33.4 21.7 26.4 24.5 13.6 20.2 28.5Hits@10 49.4 33.9 71.7 34.9 31.3 67.5 74.7 50.7 38.9 44.3 30.9 36.8 39.5 23.7 29.6 39.8 106% Edge-type HeuristicHits@3 2.8 1.3 7.1 1.9 1.7 3.5 4.2 1.8 1.4 1.4 1.9 1.6 1.2 1.2 1.1 1.4Hits@10 6.4 3.0 14.5 4.3 4.4 8.1 9.7 4.8 3.7 3.1 4.7 2.8 2.2 4.4 2.6 2.9 NodePiece-QE Hits@3 4.0 11.9 3.6 4.0 3.6 4.0 1.8 2.1 1.5 3.3Hits@10 7.7 19.2 5.9 7.6 8.1 9.2 4.3 4.5 4.1 6.1 NodePiece-QE w/ GNNHits@3 22.1 39.6 14.8 5.1 30.1 35.6 19.6 19.6 24.5 9.9Hits@10 31.7 54.3 21.6 7.8 44.2 51.6 28.4 27.7 35.2 14.0 GNN-QE Hits@3 40.6 28.3 58.1 28.5 24.1 54.7 62.3 38.7 33.1 40.3 25.6 31.7 30.4 17.0 22.4 40.1Hits@10 50.4 37.7 71.9 37.0 32.4 67.9 73.7 48.2 41.8 48.0 32.7 39.6 43.2 25.3 31.1 49.4 19Fig. 6 and Table 9 contain detailed results from the experiment in Section 5.3 about executing training queries over the original training and extended test inferencegraphs. 0.2 0.4 0.6 0.8 1.0Hits@10 query = EPFO avg 0.2 0.4 0.6 0.8 1.0 query = neg avg 0.2 0.4 0.6 0.8 1.0 query = 1p 0.2 0.4 0.6 0.8 1.0Hits@10 query = 2p 0.2 0.4 0.6 0.8 1.0 query = 3p 0.2 0.4 0.6 0.8 1.0 query = 2i 0.2 0.4 0.6 0.8 1.0Hits@10 query = 3i 0.2 0.4 0.6 0.8 1.0 query = pi 0.2 0.4 0.6 0.8 1.0 query = ip 0.2 0.4 0.6 0.8 1.0Hits@10 query = 2u 0.4 0.6 0.8 1.0 query = up 0.2 0.4 0.6 0.8 1.0 query = 2in 0.0 0.2 0.4 0.6 0.8 1.0Hits@10 query = 3in 100 200 300 400 500 Ratio inf/ train, % 0.2 0.4 0.6 0.8 1.0 query = inp 100 200 300 400 500 Ratio inf/ train, % 0.2 0.4 0.6 0.8 1.0 query = pin 100 200 300 400 500 Ratio inf/ train, % 0.2 0.4 0.6 0.8 1.0Hits@10 query = pni Model NodePiece-QE NodePiece-QE w/ GNN GNN-QE Edge-type Heuristic Graph train test Figure 6: Hits@10 results on answering training queries executed over the original train (solid line) and test inference (dashed line) graphs. NodePiece-QE models are inference-only and were trained on 1p queries, GNN-QE is end-to-end trainable on all complex queries. 20Table 9: Hits@10 results (%) of training queries executed over the original training graph and extended test inferencegraph. All ratios Einf/Etrain. avgp is the average on EPFO queries (∧, ∨). avgn is the average on queries with negation. NodePiece-QE models are inference-only and were trained on 1p queries, GNN-QE is end-to-end trainable on all complex queries. Ratio Model Graph avgp avgn 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni 550% Edge-type Heuristic test 25.4 10.5 19.2 21.3 23.4 36.1 29.4 18.9 23.6 27.3 29.2 7.0 4.6 23.1 10.4 7.4 NodePiece-QE train 79.1 76.9 70.7 65.2 93.8 94.4 80.0 70.2 89.6 70.8test 48.2 49.8 36.1 28.7 72.7 81.7 56.4 34.0 46.9 27.8 NodePiece-QE w/ GNNtrain 80.0 84.9 68.9 45.9 96.7 96.3 85.5 77.6 93.1 71.4test 55.7 60.8 37.3 23.0 84.8 86.1 60.1 46.6 66.2 36.7 GNN-QE train 99.9 99.7 100.0 99.9 99.8 100.0 100.0 100.0 100.0 100.0 99.8 99.7 100.0 99.6 99.6 99.8test 95.4 92.1 99.5 94.8 83.9 99.9 100.0 99.1 93.0 99.3 89.5 95.8 98.1 87.4 87.3 92.1 300% Edge-type Heuristic test 25.4 13.0 27.1 25.1 30.8 21.3 27.6 20.6 27.7 12.2 36.0 10.2 5.5 20.2 14.0 15.2 NodePiece-QE train 62.6 68.3 55.3 54.6 68.3 76.0 62.2 58.5 58.2 62.0test 41.0 51.8 30.1 28.4 56.3 66.5 42.1 30.7 31.7 31.1 NodePiece-QE w/ GNNtrain 85.5 96.9 81.3 49.9 97.5 98.2 89.5 86.0 91.4 78.7test 64.3 82.2 51.9 33.1 86.4 90.5 67.8 55.9 62.8 47.9 GNN-QE train 99.9 99.4 100.0 100.0 99.8 100.0 100.0 100.0 100.0 100.0 99.8 99.1 99.6 99.4 99.2 99.6test 96.2 89.7 100.0 95.9 94.6 100.0 100.0 97.3 91.3 96.7 90.3 90.8 95.5 95.0 84.6 82.8 217% Edge-type Heuristic test 21.9 11.1 24.3 22.2 24.6 20.8 30.0 21.4 18.9 9.3 25.6 7.4 9.9 16.0 10.3 12.1 NodePiece-QE train 59.7 68.6 51.4 43.3 70.4 79.6 62.0 52.8 55.8 53.3test 45.0 55.3 34.3 28.0 61.6 73.3 48.4 35.0 35.6 33.3 NodePiece-QE w/ GNNtrain 83.9 96.9 77.9 46.6 97.8 98.9 90.1 83.6 90.0 72.9test 71.0 87.6 59.0 37.5 91.0 95.2 76.4 65.6 72.2 54.9 GNN-QE train 99.9 98.7 100.0 99.9 99.7 100.0 100.0 100.0 99.9 100.0 99.8 98.6 99.2 98.7 98.3 98.5test 98.3 93.9 100.0 97.6 96.9 100.0 100.0 99.0 96.8 98.5 96.2 95.5 95.6 96.1 91.8 90.5 175% Edge-type Heuristic test 20.0 9.8 23.4 18.4 25.5 17.7 23.6 18.7 17.6 9.6 25.5 6.7 6.5 16.0 9.3 10.3 NodePiece-QE train 50.1 65.8 42.0 40.7 54.2 62.1 47.1 43.1 49.0 47.0test 40.2 57.6 30.8 29.1 47.9 57.4 38.6 31.8 35.5 33.0 NodePiece-QE w/ GNNtrain 91.0 99.9 94.1 51.7 99.9 99.9 96.3 96.8 98.4 82.2test 81.8 95.8 76.2 43.4 97.3 98.7 87.4 84.3 88.5 64.6 GNN-QE train 100.0 99.6 100.0 100.0 99.9 100.0 100.0 100.0 100.0 100.0 99.9 99.6 99.6 99.5 99.4 99.6test 99.1 93.9 100.0 99.8 98.9 100.0 99.9 98.6 98.7 98.1 97.6 95.3 94.1 95.5 93.5 91.0 150% Edge-type Heuristic test 19.7 9.8 27.0 19.1 24.8 16.6 23.6 17.6 15.9 8.4 24.7 6.2 7.7 15.1 9.2 10.6 NodePiece-QE train 41.3 57.8 37.3 36.3 40.7 48.7 36.8 34.0 39.1 41.3test 34.8 52.5 29.1 28.1 36.9 46.0 31.4 27.2 30.2 31.6 NodePiece-QE w/ GNNtrain 88.5 99.7 89.7 44.9 99.6 99.8 93.9 94.1 97.7 76.6test 79.6 95.0 74.3 38.3 96.2 98.3 86.0 81.7 85.6 61.1 GNN-QE train 99.9 98.8 100.0 99.9 99.8 100.0 100.0 99.9 99.9 100.0 99.9 98.6 98.8 98.9 98.7 99.1test 98.8 94.1 100.0 99.6 98.5 99.9 99.9 99.2 98.4 96.5 97.4 94.6 95.5 94.9 92.9 92.3 133% Edge-type Heuristic test 19.8 10.2 25.0 19.8 25.5 15.6 21.3 17.8 17.3 8.7 26.8 6.8 7.5 15.0 10.4 11.5 NodePiece-QE train 32.9 52.0 32.0 34.1 25.0 27.7 26.0 29.9 31.9 37.4test 29.2 49.0 27.5 29.4 22.9 26.0 23.0 25.8 27.0 32.1 NodePiece-QE w/ GNNtrain 89.7 99.9 92.0 47.6 99.9 99.9 94.1 96.0 99.2 78.8test 84.2 97.3 82.0 43.0 97.8 98.9 89.3 88.5 92.4 69.0 GNN-QE train 100.0 99.7 100.0 100.0 99.9 100.0 100.0 100.0 100.0 100.0 99.9 99.8 99.8 99.6 99.5 99.8test 99.2 96.8 100.0 99.1 98.9 99.9 99.8 99.0 98.5 98.6 98.7 97.5 97.0 97.7 95.8 95.8 121% Edge-type Heuristic test 17.9 8.6 22.9 16.6 23.9 14.6 20.7 15.8 14.0 7.9 24.3 5.4 5.9 13.8 8.6 9.2 NodePiece-QE train 38.4 56.4 33.8 32.5 37.0 43.6 34.5 32.3 36.6 38.6test 35.0 53.5 29.7 28.7 35.1 42.2 31.2 28.4 32.4 33.8 NodePiece-QE w/ GNNtrain 87.8 100.0 89.2 42.1 99.9 99.9 92.3 94.1 98.5 74.4test 84.8 98.0 83.9 39.6 98.6 99.4 89.2 90.3 94.3 69.7 GNN-QE train 100.0 99.4 100.0 100.0 99.9 100.0 100.0 100.0 100.0 100.0 99.9 99.4 99.5 99.1 99.3 99.7test 99.3 97.3 100.0 99.2 99.2 100.0 100.0 99.3 98.3 99.2 98.6 97.9 98.3 97.8 96.5 96.2 113% Edge-type Heuristic test 18.3 9.0 26.8 17.9 23.9 13.9 18.8 15.2 14.6 8.2 25.0 5.7 6.7 13.6 9.1 9.8 NodePiece-QE train 31.8 52.5 29.4 30.3 27.0 30.5 25.5 26.7 30.8 33.9test 30.2 51.0 27.6 28.4 25.9 29.6 24.1 25.0 28.6 31.8 NodePiece-QE w/ GNNtrain 87.4 100.0 88.4 40.4 99.9 99.9 91.9 94.0 99.6 72.4test 85.1 98.8 83.9 38.6 99.0 99.5 89.9 90.7 97.2 67.9 GNN-QE train 100.0 98.8 100.0 100.0 99.9 100.0 100.0 100.0 100.0 100.0 100.0 98.7 99.0 98.7 98.5 99.2test 99.9 97.8 100.0 100.0 99.7 100.0 100.0 99.8 99.9 100.0 99.9 98.2 98.4 98.1 97.3 97.1 106% Edge-type Heuristic test 17.1 8.3 24.1 16.1 23.6 13.2 17.4 14.4 13.5 7.8 23.9 5.4 5.5 13.5 8.2 9.0 NodePiece-QE train 24.1 45.4 22.9 27.6 16.1 16.8 17.0 20.1 22.1 28.7test 23.6 44.9 22.3 27.0 15.8 16.6 16.6 19.5 21.5 27.9 NodePiece-QE w/ GNNtrain 86.0 99.9 84.9 38.0 99.9 99.9 90.8 92.3 99.2 68.8test 84.9 99.4 82.8 37.1 99.5 99.8 89.8 90.7 98.1 66.6 GNN-QE train 100.0 99.0 100.0 100.0 99.9 100.0 100.0 100.0 99.9 100.0 99.9 98.8 99.0 98.9 98.9 99.4test 99.9 98.4 100.0 100.0 99.8 100.0 100.0 100.0 99.8 100.0 99.7 98.4 98.5 98.3 98.1 98.8 21D Hyperparameters Both NodePiece-QE and GNN-QE models are implemented with PyTorch [ 22] (MIT License). In particular, NodePiece-QE models employ PyG [ 12] (MIT License) and PyKEEN [ 2] (MIT License) for training link prediction models. GNN-QE is implemented based on the ofﬁcial NBFNet repository 7 (MIT License) and TorchDrug [41] library (Apache 2.0). For all inductive experiments in Sections 5.2 and 5.3, Table 10 lists best hyperparameters for NodePiece-QE models without GNN encoder, Table 11 contains hyperparameters for GNN-enabled NodePiece-QE models. The GNN-free models use only relation-based tokenization where each entity eis represented with two ﬁxed-size sets: a set of kunique incoming ri and a set of kunique outgoing ro relation types. Looking up their d-dimensional vectors, we obtain: e= [ [ri1,ri2,..., rik][ro1,ro2,..., rok] ] ∈R2×k×d If, for some entity, the number of unique relations of a certain kind is less than k, we pad the set with auxiliary [PAD] tokens. Entity representations are built as a function of the two sets f(e) :R2×k×d →Rd: he = MLP ( RANDOM PROJ( k∑ j=0 rij) +RANDOM PROJ( k∑ j=0 roj) ) Particularly, we ﬁrst sum up tokens of the same direction, pass them through a random projection layer RANDOM PROJ (we found that making this projection learnable does not improve results), sum up representations of incoming and outgoing parts, and pass the resulting vector through a learnable MLP. This way, the number of learnable encoder parameters does not depend on the sequence length k, i.e., the number of chosen tokens per node. The GNN-enabled models employ a slightly different Concat + MLPencoder where each node is tokenized with a sample of kincident relations. Then, we concatenate d-dimensional embeddings of those tokens ti into a single long vector Rkd, and then use a 2-layer MLP to project it to a model dimension d, i.e., f(e) :Rkd →Rd: he = MLP ( [t0; t1; ... ; tk] ) For the large-scale experiment on WikiKG-QE in Section 5.5, we employ theConcat + MLPencoder. Instead of separating incoming and outgoing relation types, we ﬁrst tokenize each node with 20 nearest anchors (pre-selected in advance using the default NodePiece strategy [13]) and add a sample of k unique incident relations. NodePiece-QE w/ GNN employs a 3-layer CompGCN with the RotatE interaction function during message computation and sum aggregator. Due to high memory consumption, we train the 50d model for 2 epochs on 2 x RTX 8000 (48 GB) GPUs. The overall tokens vocabulary consists of 20,000 anchor nodes, 1,024 relation types (including inverse relations) and one [PAD] token. All hyperparameters for this experiments are listed in Table 12. Having trained the link predictors, we tune CQD-Beam hyperparameters on the validation set varying the t-norms, t-conorms, and scores normalization. Table 13 lists best options for each EPFO query type. For all experiments, we used a beam size k= 32except for queries on WikiKG-QE where we used k= 8due to the memory-expensive need of maintaining a beam over 2M entities. Table 14 lists hyperparameters for GNN-QE models for all inductive splits. We found this architecture is quite stable under various conﬁgurations and eventually employed the same set of hyperparameters across all datasets. BetaE (as a transductive baseline for the reference 175% dataset) was conﬁgured with400dembedding dimension, batch size 512, 32 negative samples, learning rate 0.0005, margin γ60, and trained on 10 query patterns {1p,2p,3p,2i,3i,2in,3in,inp,pin,pni} for 200ksteps. 7https://github.com/DeepGraphLearning/NBFNet 22Table 10: NodePiece-QE hyperparameters for all inductive splits. Hyperparameter Dataset Einf/Etrain Ratios 106 113 133 134 150 175 217 300 550 V ocab size 472 466 460 458 450 438 442 402 346 Tokens per node 20 20 20 20 20 20 20 20 10 V ocab dim 400 400 400 400 400 400 400 400 1000 Scoring function ComplEx [30] Encoder RandomProj + MLP Encoder dim 400 400 400 400 400 400 400 400 1000 Encoder layers 2 Batch size 256 Epochs 400 400 400 1000 1000 2000 2000 2000 3000 Learning rate 1e-4 Optimizer Adam Loss function BCE Adv. temperature 1.0 1.0 1.0 1.0 0.5 0.5 0.5 0.2 1.0 # negatives 128 128 128 128 128 128 128 128 128 # parameters 699k 694k 689k 688k 681k 671k 675k 643k 2.7M Training time (hrs) 15 12 10 9 6 9 9 5 1 Table 11: NodePiece-QE with GNN hyperparameters for all inductive splits. Hyperparameter Dataset Einf/Etrain Ratios 106 113 133 134 150 175 217 300 550 V ocab size 472 466 460 458 450 438 442 402 346 Tokens per node 20 10 V ocab dim 200 400 Scoring function ComplEx [30] Encoder Concat + MLP Encoder dim 200 400 Encoder layers 2 GNN encoder CompGCN [31] + RotatE [27] message function GNN layers 5 5 5 5 5 3 3 3 3 GNN dim 200 400 Batch size 256 Epochs 600 1000 1000 1000 1000 1000 3000 4000 4000 Learning rate 5e-4 Optimizer Adam Loss function BCE Adv. temperature 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.5 1.0 # negatives 128 128 128 128 128 128 128 128 128 # parameters 2.8M 2.8M 2.8M 2.8M 2.8M 2.4M 2.4M 2.3M 5.7M Training time (hrs) 30 30 19 20 14 15 8 5 1 23Table 12: NodePiece-QE hyperparameters for WikiKG-QE (133%). Hyperparameter NodePiece-QE NodePiece-QE w/ GNN V ocab size 20,000 anchors + 1024 relation types Anchor tokens per node 20 15 Relation tokens per node 20 10 V ocab dim 100 50 Scoring function ComplEx [30] Encoder Concat + MLP Encoder dim 200 100 Encoder layers 2 2 GNN - CompGCN GNN dim - 50 GNN layers - 3 Batch size 512 1024 Epochs 40 ( ≈1M steps) 2 Learning rate 1e-4 Optimizer Adam Loss function BCE Adversarial temp. 1.0 # negatives 64 512 # parameters 2,922,900 1,211,950 Training time (hrs) 40 16 Table 13: CQD-Beam t-norm hyperparameters for all splits and both link predictors, NodePiece-QE and NodePiece-QE w/ GNN, when answering EPFO queries. The default beam size k= 32, prod + σis product t-norm with sigmoid score normalization. Details on t-norms are in Appendix A. Ratio Link predictor 1p 2p 3p 2i 3i pi ip 2u up 106% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 113% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 122% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 134% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 150% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 175% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 217% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 300% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 550% NodePiece-QE prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σ prod +σNodePiece-QE w/ GNN 133% NodePiece-QE (WikiKG-QE) prod + σfor all query types 24Table 14: Hyperparameters of GNN-QE on different datasets. All the hyperparameters are selected by the performance on the validation set. Hyperparameter All splits GNN #layers 4 hidden dim. 32 composition DistMult [33] aggregation PNA [10] MLP #layer 2 hidden dim. 64 Traversal Dropout probability 0.5 Logical Operator t-norm product Learning batch size 64 sample weight uniform across queries loss BCE # negatives 32 optimizer Adam learning rate 5e-3 iterations (#batch) 10,000 adv. temperature 0.1 E Edge-type Heuristic We consider Edge-type Heuristicas a trivial baseline for inductive complex query. Given a query Q= (C,RQ,G), Edge-type Heuristic ﬁnds all entities e ∈E that satisfy the relations in the last hop of RQ on the inference graph Ginf. In other words, this baseline ﬁlters out entities that are not consistent with the query according to the edge types, which is a necessary condition for the answers when the inference graph is reasonably dense. Since Edge-type Heuristic only distinguishes the entities into two classes, we randomly shufﬂe the entities in each class to create a ranking. Edge-type Heuristic can be easily implemented as GNN-QE with a special relation projection. Given an inference graph Ginf, we ﬁrst preprocess a relation-to-entity mapping M, where Mr,t means there exists a head entity hand an edge (h,r,t) for tail entity t. Then the relation projection of relation r can be implemented by looking up the row Mr. Note that the Edge-type Heuristic only ﬁlters entities according to the edge type, and hence the head entity (or the distribution of head entity) is ignored in the relation projection. 25",
      "references": [
        "Bringing light into the dark: A large-scale evaluation of knowledge graph embedding models under a uniﬁed framework.",
        "PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings.",
        "Query embedding on hyper-relational knowledge graphs.",
        "Neural methods for logical reasoning over knowledge graphs.",
        "Complex query answering with neural link predictors.",
        "Translating embeddings for modeling multi-relational data.",
        "SizeShiftReg: a regularization method for improving size-generalization in graph neural networks.",
        "Fuzzy logic based logical query answering on knowledge graph.",
        "Probabilistic entity representation model for reasoning over knowledge graphs.",
        "Principal neighbourhood aggregation for graph nets.",
        "Message passing query embedding.",
        "Fast graph representation learning with PyTorch Geometric.",
        "Nodepiece: Compo- sitional and parameter-efﬁcient representations of large knowledge graphs.",
        "Neural message passing for quantum chemistry.",
        "Embedding logical queries on knowledge graphs.",
        "Open graph benchmark: Datasets for machine learning on graphs.",
        "A survey on knowledge graphs: Representation, acquisition and applications.",
        "Adam: A method for stochastic optimization.",
        "Triangular norms. position paper I: basic analytical and algebraic properties.",
        "Understanding attention and generalization in graph neural networks.",
        "Answering complex queries in knowledge graphs with bidirectional sequence encoders.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Query2box: Reasoning over knowledge graphs in vector space using box embeddings.",
        "Beta embeddings for multi-hop logical reasoning in knowledge graphs.",
        "Drum: End-to-end differentiable rule mining on knowledge graphs.",
        "Faithful embeddings for knowledge base queries.",
        "Rotate: Knowledge graph em- bedding by relational rotation in complex space.",
        "Inductive relation prediction by subgraph reasoning.",
        "Observed versus latent features for knowledge base and text inference.",
        "Complex embeddings for simple link prediction.",
        "Composition-based multi-relational graph convolutional networks.",
        "Knowledge base completion via search-based question answering.",
        "Embedding entities and relations for learning and inference in knowledge bases.",
        "Differentiable learning of logical rules for knowledge base reasoning.",
        "From local structures to size generalization in graph neural networks.",
        "Labeling trick: A theory of using graph neural networks for multi-node representation learning.",
        "Quaternion knowledge graph embeddings.",
        "Cone: Cone embeddings for multi-hop reasoning over knowledge graphs.",
        "Ood link prediction generalization capabilities of message-passing gnns in larger test graphs.",
        "Neural-symbolic models for logical queries on knowledge graphs.",
        "Torchdrug: A powerful and ﬂexible machine learning platform for drug discovery, 2022.",
        "Neural bellman-ford networks: A general graph neural network framework for link prediction."
      ],
      "meta_data": {
        "arxiv_id": "2210.08008v2",
        "authors": [
          "Mikhail Galkin",
          "Zhaocheng Zhu",
          "Hongyu Ren",
          "Jian Tang"
        ],
        "published_date": "2022-10-13T03:53:34Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces and formalizes the problem of inductive complex logical query answering (FOL/EPFO) on knowledge graphs where inference-time graphs contain new entities (E_train ⊂ E_inf) and even training queries may gain new correct answers after graph updates. Proposes and empirically analyzes two inductive mechanisms that avoid transductive entity embedding matrices: (1) NodePiece-QE—inductive node representations built from an invariant token vocabulary (primarily relation-type context), enabling inference-only complex query answering via a non-parametric executor (CQD-Beam) trained only on 1-hop link prediction; (2) GNN-QE—inductive relational-structure representations based on NBFNet-style conditional message passing from query anchors, enabling end-to-end training on complex queries (including negation). Provides a new inductive benchmark suite (10 datasets) derived from FB15k-237 and OGB WikiKG2, and shows inductive generalization to inference graphs up to ~500% larger than training graphs, ability to recover new answers after graph updates, and scalability of NodePiece-QE to multi-million-node graphs in a zero-shot regime.",
        "methodology": "Defines inductive FOL query answering as learning a conditional scoring/ranking function f(e | C, R_Q, G) that generalizes to unseen entities. NodePiece-QE: deterministically tokenizes each entity into a fixed-size set of tokens from an invariant vocabulary (primarily unique incident relation types; optionally anchor nodes for large-scale), learns token embeddings and an MLP encoder to compose entity embeddings, optionally refines with a relational GNN encoder (CompGCN). Pretrains only on 1p link prediction using ComplEx scoring and negative sampling; answers complex EPFO queries at inference with CQD-Beam, which decomposes queries and executes ∧/∨ via fuzzy logic t-norms/t-conorms (product/Gödel), using beam search over intermediate variables. GNN-QE: extends GNN-based query embedding by using NBFNet as a trainable projection operator modeling p(t|h,r) via relation-conditioned message passing (labeling trick); chains multiple projections for multi-hop, and applies differentiable fuzzy logic operators (product t-norm/t-conorm and 1−x negation) over entity truth-value vectors, allowing end-to-end training on complex query patterns and supporting negation.",
        "experimental_setup": "Benchmarks: constructs 9 inductive datasets from FB15k-237 (with inverse edges added) by sampling a training entity subset (ratio r∈[0.1,0.9]) to form G_train and forming disjoint validation/test unseen entity sets; validation/test inference graphs are supersets of training with new nodes, and 15% of inference edges are withheld as missing links (T_pred) to ensure queries require link prediction. Inference graph size ratios E_inf/E_train range ~105% to ~550%. Queries: uses BetaE query sampler to generate 14 standard query types (1p/2p/3p/2i/3i/ip/pi/2u/up/2in/3in/inp/pin/pni); GNN-QE is trained on 10 patterns (excluding ip/pi/2u/up) and evaluated on all; NodePiece-QE is inference-only for EPFO patterns (no negation). Metrics: primary Hits@10 (filtered—easy answers filtered out; evaluate hard answers requiring missing-link inference); additional entailment/faithfulness evaluation by re-answering training queries on enlarged inference graphs and measuring retrieval of new easy answers; ROC-AUC on unfiltered scores to assess ranking easy vs hard answers. Baselines: transductive BetaE with random embeddings for unseen nodes (to show failure), and an Edge-type Heuristic (last-hop relation filtering). Scalability: WikiKG-QE from OGB WikiKG2 split into 1.5M-node training graph and 0.5M unseen nodes for val/test (E_inf≈2M, 11M edges); evaluates NodePiece-QE with CQD-Beam on 10k EPFO queries/type using Hits@100 against all 2M entities. Compute: NodePiece-QE trained on single V100; GNN-QE trained/evaluated on 4×V100; large-scale NodePiece experiments on RTX 8000.",
        "limitations": "Performance–efficiency trade-off: NodePiece-QE is scalable and can run inference-only but underperforms end-to-end GNN-QE on accuracy, especially for harder multi-hop patterns; also cannot natively handle negation in the CQD-Beam setup used here. GNN-QE achieves higher performance and supports negation but is computationally expensive because each query induces a uniquely initialized message-passing run over the graph, limiting scalability to very large KGs. Both GNN-based variants (GNN-QE and NodePiece+GNN) exhibit size-generalization degradation as inference graphs grow much larger than training graphs (up to ~20 Hits@10 drop by 550%), consistent with known GNN generalization issues; distractor entities in larger graphs further hurt ranking. Benchmark assumptions include fixed relation vocabulary across train/inference and primarily “superset” inference graphs (not fully disjoint), and use of anonymized graphs without side information (types/text), which may limit applicability or ignore helpful signals. Limited reporting of variance (no error bars) and partial coverage of logical operators for some models.",
        "future_research_directions": "Improve size/out-of-distribution generalization of message-passing models for larger inference graphs (e.g., size-shift regularization, architectural changes, sampling/normalization strategies) and mitigate distractor effects. Develop more scalable relational-structure query answering (reduce per-query GNN cost via caching, amortization, subgraph extraction, or compiled operators) to bring GNN-QE to million-node KGs. Extend inductive setting to fully disjoint training/inference graphs (no shared entities) and broaden supported real-world query patterns beyond the benchmark set, including richer negation/union handling for inference-only approaches. Incorporate continuous/auxiliary modalities (text, numerical literals, types) while retaining inductive guarantees. Generalize to other KG formalisms (hypergraphs, hyper-relational statements) and investigate “neural graph database” systems that support incremental updates and immediate zero-shot query answering.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Learning from Both Structural and Textual Knowledge for Inductive Knowledge Graph Completion",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "INDIGO: GNN-Based Inductive Knowledge Graph Completion Using Pair-Wise Encoding",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Conformal Inductive Graph Neural Networks",
      "full_text": "Published as a conference paper at ICLR 2024 CONFORMAL INDUCTIVE GRAPH NEURAL NETWORKS Soroush H. Zargarbashi CISPA Helmholtz Center for Information Security zargarbashi@cs.uni-koeln.de Aleksandar Bojchevski University of Cologne bojchevski@cs.uni-koeln.de ABSTRACT Conformal prediction (CP) transforms any model’s output into prediction sets guaranteed to include (cover) the true label. CP requires exchangeability, a re- laxation of the i.i.d. assumption, to obtain a valid distribution-free coverage guar- antee. This makes it directly applicable to transductive node-classification. How- ever, conventional CP cannot be applied in inductive settings due to the implicit shift in the (calibration) scores caused by message passing with the new nodes. We fix this issue for both cases of node and edge-exchangeable graphs, recover- ing the standard coverage guarantee without sacrificing statistical efficiency. We further prove that the guarantee holds independently of the prediction time, e.g. upon arrival of a new node/edge or at any subsequent moment. 1 I NTRODUCTION Graph Neural Networks (GNNs) are used in many applications however without a reliable estimation of their output’s uncertainty. We can not rely solely on the predicted distribution of labels π(y | x) (e.g. from the softmax) as it is often uncalibrated. Therefore, it is crucial to find confidence estimates aligned with the true p(y | x). There is a rich literature on uncertainty quantification methods many of which require retraining or modifications to the model architecture. Among them, almost none come with a guarantee, and many rely on the i.i.d. assumption. Given the interdependency structure (adjacency) we cannot easily adopt these methods for node classification. As a result, uncertainty quantification methods for GNNs are very limited (Stadler et al., 2021). Conformal prediction (CP) is an alternative approach that uses the model as a black box and returns prediction sets guaranteed to cover the true label without any assumptions on the model’s architec- ture or the data generating process. This guarantee is probabilistic and works for any user-specified probability 1 − α. To apply CP, we need a conformity score function s : X × Y 7→R that quan- tifies the agreement between the input and the candidate label. Additionally, we need a held-out calibration set. The only assumption for a valid coverage guarantee is exchangeability between the calibration set and the test set. This makes CP applicable to non-i.i.d settings like transductive node classification – Zargarbashi et al. (2023) and Huang et al. (2023) showed that with a permutation- equivariant model (like almost all GNNs) CP obtains a valid coverage guarantee. Given the dynamic nature of real-world graphs which evolve over time,inductive node classification is more reflective of actual scenarios than transductive. In the inductive setting, we are given an initial graph (used for training and calibration) which is progressively expanded by set of nodes and edges introduced to the graph over time. Importantly test nodes are not present at the training and calibration stage. For GNNs, updates in the graph cause an implicit shift in the embeddings of existing nodes and consequently the softmax outputs and conformity scores. Therefore, in the inductive setting, even assuming node/edge exchangeability (see § B for a formal definition), the coverage guarantee is no longer valid. Intuitively, calibration scores are no longer exchangeable with test scores, as soon as new nodes or edges are introduced (see Fig. 1 right). To address this issue, Clarkson (2023) adapt thebeyond exchangeabilityapproach applying weighted CP for inductive node-classification aiming to recover the guarantee up to a bounded (but unknown) error (Barber et al., 2023). Unfortunately, this approach has an extremely limited applicability when applied with a realistic (sparse) calibration set. In sparse networks and limited labels, the method fails to predict any prediction set for a large number of nodes; and for the rest, the statistical effi- ciency is significantly low (see § C for details). 1 arXiv:2407.09173v1  [cs.LG]  12 Jul 2024Published as a conference paper at ICLR 2024 −1.0 −0.8 −0.6 −0.4 −0.2 0.0 Score 0 2Density Standard CP 0 1000 2000 3000 4000 Timestep 0.05 0.10 EMD Dist. Cal Test 0 1000 2000 3000 Timestep 0 1000 2000 3000 4000 NodeEx CP Naive CP Ground Truth −0.8 −0.6 −0.4 −0.2 Figure 1: [Left] Each vertical line on the heatmap shows sorted true test scores at each timestep. The dashed line shows the true (unknown)α-quantile and the quantile from each approach is also shown alongside. NodeEx CP (ours) closely tracks the true quantile, while naive CP deviates over time. [Upper right] Distributions from selected timesteps marked by the same color on the heatmap. The distribution shift is observable over time with new nodes appearing. [Lower right] The earth mover distance (EMD) between naive CP calibration scores and shifted true scores, denoted as “Test”; and EMD between naive and NodeEx CP scores, denoted as “Cal”. Details in § D.5. We show that for a node or edge exchangeable graph sequence (without distribution shift), we can adapt CP w.r.t. the implicit shift in score space, recovering the desired coverage guarantee. Node- exchangeability assumes that the joint distribution of nodes is invariant to any permutation. In the inductive setting, this means that any node is equally likely to appear at any timestep. We show that under node-exchangeability, the effect of new coming nodes is symmetric to the calibration set and the existing nodes. Hence, computing the conformity scores conditional to the subgraph at any timestep recovers the guarantee. Previous works under the transductive setting (Zargarbashi et al., 2023) and (Huang et al., 2023) also assume node-exchangeability. While real-world graphs are sparse, all node-exchangeable generative processes produce either empty or dense graphs (see § 3). Therefore, we extend our study to the edge-exchangeable set- ting which can generate sparse graphs. In § 4.2 we show that w.r.t. scores, any edge-exchangeable sequence is equivalent to a weighted node-exchangeable sequence. Therefore, through weighted CP (weighted quantile lemma from Tibshirani et al. (2019)) we recover the 1 − α coverage. Impor- tantly, the improvement of our method is orthogonal to whether there is a distribution shift over time. Therefore, to address shift, we can still apply the weighting scheme from Barber et al. (2023) on top. We focus on inductive node-classification. We define node-exchangeable (NodeEx) CP, and we show that (i) for node-exchangeable sequences, NodeEx CP obtains 1 − α guarantee by calibrating over shifted conformal scores conditional to subgraphs in each timestep (Fig. 1 left), (ii) the 1 − α guar- antee is achievable via weighted CP on edge-exchangeable sequences, and (iii) The guarantee holds independent of the prediction time – the prediction set for a node at any timestep after its appearance benefits from the same coverage guarantee (e.g. prediction upon arrival or all nodes at once both result in the same coverage). We justify our approach with both theoretical and empirical results. 2 B ACKGROUND : C ONFORMAL PREDICTION AND TRANSDUCTIVE GNN S Here we recall the general guarantee of CP (e.g. for image dataset). Through this paper, we assume a continuous score function without ties. This can apply to any function via adding noise. With weights wi ∈ [0, 1], and the sorting permutationτ⋆, we define the weighted quantileQuant (·; ·; ·) as Quant Ä α; {si}N i=1 ; {wi}N i=1 ä = inf ( sτ⋆(i) : Pi j=1 wτ⋆(j) PN j=1 wj + 1 ≥ α ) (1) 2Published as a conference paper at ICLR 2024 Theorem 1 (V ovk et al. (2005)). Let {(xi, yi)}n i=1, and (xn+1, yn+1) be exchangeable. With any continuous function s : X × Y 7→R measuring the agreement between x, and y, and user-specified significance level α ∈ (0, 1), with prediction sets defined as Cα(xn+1) = {y : s(xn+1, y) ≥ ˆq} where ˆq := Quant (˜α; {s(xi, yi)}n i=1; 1). We have Prob [yn+1 ∈ C(xn+1)] ≥ 1 − α. The marginal coverage probability is upper-bounded by 1 − α + 1/(n + 1). With infinite samples, the coverage is distributed as a Beta(n + 1 − l, l) with l = ⌊(n + 1)α⌋ (V ovk, 2012), while with a fixed population |D| = n + m, and an exchangeable calibration set |Dcal| = n, the probability of coverage Cov(D \\ Dcal) := (1 /m)1[yi ∈ C(xi)]i∈D−Dcal derives from a collection of hyper- geometric distributions (Huang et al., 2023). Conformity scores. The guarantee from Theorem 1 holds regardless of how the conformity score function s : X × Y →R is defined. A wise choice of s(·, ·) is reflected in other metrics like the prediction set size. We use adaptive prediction sets (APS) with a score function defined as s(x, y) := −(ρ(x, y) + u · π(x)y). Here ρ(x, y) := PK c=1 π(x)c1 [π(x)c > π(x)y] is the sum of all classes predicted as more likely than y, and u ∈ [0, 1] is a uniform random value that breaks the ties between different scores to allow exact 1 − α coverage (Stutz et al., 2022). Our method is independent of the choice of score function and we show its applicability on other (network-agnostic and network-aware) scores in § D.3. CP beyond exchangeability. For cases where the calibration and test points are not exchangeable, there is a coverage gap∆α, i.e. Prob [yn+1 ∈ Cα(xn+1)] ≥ 1−α−∆α. Barber et al. (2023) shows that ∆α has an upper bound that depends on how far the data deviates from exchangeability. This upper bound can be further controlled via weighted conformal prediction. For related works see § F. 2.1 C ONFORMAL PREDICTION FOR TRANSDUCTIVE NODE -CLASSIFICATION Consider a graph G(V, E, A, y) with X ∈ Rn×d as node-features matrix, A ∈ Rn×n as adjacency matrix, and y ∈ Rn as labels. V = Vtr ∪ Vcal ∪ Vu is the set of vertices (training, calibration, and test). Let f be a black-box permutation-equivariant model (e.g. a GNN) trained on labels of Vtr, and s be a continuous score function with access to the outputs of f. The score function s may or may not use information about the graph structure (see § D.3). In the transductive setup the graph G is fixed, and training and calibration is performed with all observed nodes (including features and structure). The labels of Vtr and Vcal are used respectively for training and calibration, while all other labels remain unseen. Here when Vcal and Vu are exchangeable, standard CP can be applied (Zargarbashi et al., 2023; Huang et al., 2023). For a set with a fixed number of nodes V′, we define the discrete coverage asCov(V′) = 1 |V′| P vj∈V′ 1 î yj ∈ ˆC(vj) ó . The following theorem shows that under transductive calibration, CP yields valid prediction sets. Theorem 2 (Rephrasing of Theorem 3 by Huang et al. (2023)) . For fixed graph G, and a permutation-equivariant score function s(·, ·), with Vcal = {(vi, yi)}N i=1 exchangeably sampled from V \\ Vtr, and prediction sets ˆC(v) = ¶ y : s(v, y) ≥ Quant Ä α; {s(vi, yi)}N i=1 ; 1 ä© we have Prob [Cov(Vu) ≤ t] = 1 − ΦHG(iα − 1; M + N, N,⌈Mt⌉ + iα) (2) where iα = ⌈(N + 1)(1 − α)⌉ is the unweighted α-quantile index of the calibration scores, and ΦHG(·; N, n, K) is the c.d.f. of a hyper-geometric distribution with parameters N (population), n number of samples, and K number of successful samples among the population. The issue with inductive node-classification. The above approach does not directly translate to the inductive setting. As soon as the graph is updated, the implicit shift in the nodes embeddings results in a distribution shift w.r.t. the calibration scores that are computed before the update. This breaks the exchangeability as shown by Zargarbashi et al. (2023) and in Fig. 1. To address this issue, Clark- son (2023) adopts weighted CP (Barber et al., 2023) with neighborhood-dependent weights (NAPS), limiting the calibration nodes to those inside an immediate neighborhood of a test node. Applying NAPS on sparse graphs or with small (realistic) calibration sets, leaves a significant proportion of test nodes with an “empty” calibration set. Hence, its applicability is limited to very special cases (see § C for details). Moreover, NAPS does not quantify the coverage gap∆α. In contrast, when assum- ing either node or edge-exchangeability, the gap for our approach is zero regardless of the weights. 3Published as a conference paper at ICLR 2024 3 I NDUCTIVE GNN S UNDER NODE AND EDGE EXCHANGEABILITY In the inductive scenario, the graph changes after training and calibration. Therefore, the model f is trained, and CP is calibrated only on a subgraph before the changes. We track these updates in a sequence of graphs G1, G2, . . .where Gt = (Vt, Et) is a graph with a finite set of vertices and edges. We focus on progressive updates meaning ∀t : Vt ⊆ Vt+1 and Et ⊆ Et+1. At timestep t in a node- inductive sequence, the update adds a nodevt with all its connections to existing vertices. In an edge- inductive sequence, updates add an edge, which may or may not bring unseen nodes with it. W.l.o.g update sets are singular. Node-inductive and edge-inductive sequences are formally defined in § B. We call a node-inductive sequence to be node-exchangeable if the generative process is invariant to the order of nodes, meaning that any permutation of nodes has the same probability. Analogously, a sequence is edge-exchangeable if all permutations of edges are equiprobable. For both cases, the problem is node-classification. Since split CP is independent of model training w.l.o.g. we assume that we train on an initial subgraph G0. We do not need to assume that G0 is sampled exchangeably. Formal definitions of node-exchangeability and edge-exchangeability are provided in § B. For transductive GNNs, both Zargarbashi et al. (2023) and Huang et al. (2023) assume that the calibration set is sampled node-exchangeably w.r.t. the test set, however the entire graph is fixed and given. In contract, we assume that the sequence of graphsG1, G2, . . .itself, including calibration and test nodes, is either node- or edge-exchangeable. Transductive setting can be seen as a special case. Sparsity. Any node-exchangeable graph is equivalent to mixture of graphons (Aldous, 1981; Hoover, 1979; Cai et al., 2016). A graphon (or graph limit) is a symmetric measurable function W : [0, 1]2 7→ [0, 1] and the graph is sampled by drawing ui ∼ Uniform[0, 1] for each vertex vi, and an edge between vi, and vj with probability W(ui, uj). We know that graphs sampled from a graphon are almost surely either empty or dense (Orbanz & Roy, 2014; Cai et al., 2016) – the number of edges grows quandratically w.r.t. the number of vertices. However, sparse graphs (with edges sub- quadratical in the number of vertices) are more representative to real-world networks. Therefore, we also consider edge-exchangeable graph sequences (Cai et al., 2016) that can achieve sparsity. 4 C ONFORMAL PREDICTION FOR EXCHANGEABLE GRAPH SEQUENCES The first N nodes of the sequence (and their labels) are taken as the calibration set, leaving the rest of the sequence to be evaluated. 1 For easier analysis, we record the coverage of each node at each timestep. Let T0 be the time when the last calibration node arrived. Up to timestepT, let matrix C ∈ {0, 1}VT ×T with C[i, t] indicate whether the prediction set for test node vi at timestep t covers the true label yi. The time index inC is relative toT0 and w.l.o.g. we index nodes upon their appearance. Hence, in each column C[·, t], the first |Vt| elements are in {0, 1} and the rest are N/A. Formally: C[i, t] = ® 1 î yi ∈ ¶ y : s(vi, y) ≥ Quant Ä α; {s(vi, yi|Gt)}vi∈Vcal ; 1 ä©ó i ≥ t N/A o .w. (3) 0 1000 2000 3000 Timestep 0100020003000 Nodes Figure 2: 1−C for Cora. Details in § B Here we assume that one can evaluate a node at any timestep after its appearance. We define V(t) eval ⊆ Vt as the set of nodes evaluated at timestep t. For an edge- exchangeable sequence, a node v is considered as active (existing) upon the arrival of the first edge connected to it. After T timesteps, we call the (recorded) sub-partition ∪T i=1V(i) eval as an evaluation maskIT . Any node appears at most once inIT (we do not re-evaluate a node). We define the Cov(V(t) eval) = (1/|V(t) eval|) P vi∈V(t) eval C[i, t] as the em- pirical coverage over the mask V(t) eval. Similarly, Cov(IT ) is the empirical coverage forIT (average over all records). We visualize1−C under node exchangeability on Fig. 2. 1All results can be trivially generalized to the case where the calibration nodes are scattered in the sequence. This only affects the width of the coverage interval (the variance) as it is directly a function of the number of available calibration nodes when computing prediction sets. 4Published as a conference paper at ICLR 2024 4.1 N ODE -EXCHANGEABLE SEQUENCES A node-inductive graph sequence is node-exchangeable if any permutation of node appearance is equally likely – at each step, any unseen node has the same probability of appearing (see § B). With the arrival of new nodes, the distribution of embeddings (and consequently scores) shifts (as shown in Fig. 1). This is why calibrating prior to the update fails to guarantee coverage for new nodes. However, with node-exchangeability, calibration and evaluation scores computed conditional to a specific subgraph are still exchangeable. We extend Theorem 2 to inductive GNNs on node- exchangeable graph sequences for any subset of evaluated vertices. Proposition 1. At time step t, with graph Gt = (Vt, Et) from a node-exchangeable sequence, and a calibration set Vcal consisting of the first n nodes in Gt, for any vj ∈ V(t) eval ⊆ Vt \\ Vcal define C(t)(vj) = ¶ y : s(vj, y| Gt) ≥ Quant Ä α; {s(vi, yi | Gt)}i∈Vcal ; 1 ä© (4) Then Prob î yj ∈ C(t)(vj) | vj ∈ V(t) eval ó ≥ 1 − α. Moreover with m := |V(t) eval|, Prob î Cov(V(t) eval) ≤ β ó = 1 − ΦHG(iα − 1; m + n, n, iα + ⌈βt⌉) (5) We deffer proofs to § B. Here we provide an intuitive justification of the theorem. First, if V(t) eval includes all vertices the problem reduces to the transductive case, for which Theorem 2 applies with scores conditional on the current graph. Otherwise, V′ := Vt \\ V(t) eval is not empty. Here the effect of V′ is symmetric to Vcal and V(t) eval. For instance, consider a linear message passing Z = AXW with weight matrix W (our results hold in general). From standard CP we know that for row- exchangeable matrix X, XW is also row-exchangeable (a linear layer is permutation equivariant). Hence, we just question the row-exchangeability of AX. Split X into [X⊤ cal | X⊤ eval | X⊤ V′ ]⊤, and similarly split A into nine blocks based on the endpoints of each edge. We have that AX equals \"Acal·cal Acal·eval Acal·V′ Aeval·cal Aeval·eval Aeval·V′ AV′·cal AV′·eval AV′·V′ # X = \" Acal·calXcal + Acal·evalXeval AevalcalXcal + Aeval·evalXeval ··· # | {z } Mat(1) + \"Acal·V′ XV′ Aeval·V′ XV′ ··· # | {z } Mat(2) For the guarantee to be valid, the first |Vcal| + |Veval| rows should be exchangeable. For Mat(1), this already holds due to node-exchangeability and Theorem 2. With XV′ being common in both blocks of Mat(2), we only need Acal·V′ and Aeval·V′ to be row-exchangeable which again holds due to node exchangeability. Hence, the effect of V′ is symmetric to calibration and evaluation sets. In other words, the shifted embedding still preserves the guarantee conditional to the graph at timestep t. The only requirement is to compute the conformal scores for all nodes including the calibration set and dynamically update the quantile threshold – the threshold depends on t. Another way to understand the theorem is that for any column inC, the expectation of any subset of elements ≥ 1 − α. Next, we generalize this guarantee to any evaluation mask independent of time. Theorem 3. On a node-exchangeable graph sequence, with exchangeably sampled Vcal consisting of the first n nodes in Gt, for any valid partition IT = ∪T i=1V(i) eval we have Prob î yi ∈ C(t)(vi) | vi ∈ V(t) eval ó ≥ 1 − α (6) Moreover, it holds thatProb [Cov(IT ) ≤ β] = 1 − ΦHG(iα; |IT | + n, n, iα + ⌈|IT | ·β⌉) Theorem 3 indicates that CP applies to inductive GNNs conditional to the subgraph in which each node is evaluated. We will leverage this insight in § 5. In conclusion, with a node-exchangeable graph sequence, for any node vi appearing at timestep t, and evaluated at any timestep t′ ≥ t, it holds that Prob î yi ∈ C(t′)(vi) ó ≥ 1 − α. Two special cases of this result are evaluation upon appearance (the diagonal of C) and evaluation all at once (the last column of C) which follows from Theorem 2. The latter is also equivalent to the simultaneous-inductive setting in Zargarbashi et al. (2023). The user is flexible to delay the prediction to any time, still preserving the guarantee. 5Published as a conference paper at ICLR 2024 4.2 E DGE -EXCHANGEABLE SEQUENCES In an edge-inductive sequence at each timestep an edge is added,Gt+1 = (Vt ∪V (et+1), Et ∪et+1).2 This edge may introduce new nodes or connect existing ones. When all permutations µ of the edge-sequence are equally likely, Prob [(e1, . . . em)] = Prob [(µ(e1), . . . µ(em))], the sequence is edge-exchangeable (see § B for a formal definition). We address this setting using a special case of weighted quantile lemma (Tibshirani et al., 2019) with weights defined by the frequency of elements. Lemma 1. Let X = {x1, ··· , xm} be exchangeable random variables and f : 2 X 7→ R be a mapping defined on subsets of X. For any partitioning ∪n+1 i=1 Xi = X and zi := f(Xi) we have: Prob ñ zn+1 ≤ Quant Ç β; {zi}n i=1 ∪ {∞}; ß 1 |Xi| ™n+1 i=1 åô ≥ β We introduce the edge-exchangeable (EdgeEx) CP and prove its guarantee by showing that at any timestep t, an edge exchangeable sequence, is decomposed into weighted node exchangeable subse- quences with weights equal to 1/deg(v). Then, on each subsequence weighted CP maintains a valid guarantee. In this setup, there are no isolated nodes, any node can be evaluated upon appearance. Theorem 4. At each timestep t, given graph Gt = (Vt, Et) from an edge-exchangeable sequence and with a calibration set Vcal, define q = Quant Ä α; {si}vi∈Vcal ; ¶ 1/deg(vi)vi∈Vcal ©ä , and for any vj ∈ V(t) eval define C(t)(vj) := {y : s(vj, y) ≥ q}. We have Prob \u0002yj ∈ C(t)(vj)\u0003 ≥ 1 − α. Via Theorem 4, EdgeEx CP is guaranteed to provide 1 − α coverage. The result holds for each timestep t, conditional to that timestep. 5 N ODE EXCHANGEABLE AND EDGE EXCHANGEABLE CP Building upon the theory in § 4, for node-exchangeable graph sequences we define node- exchangeable (NodeEx) CP with coverage guarantee conditional to the subgraph at each timestep. Recall that the shift in scores upon changes in the graph is symmetric for calibration and evalua- tion nodes. Hence, NodeEx recomputes the calibration scores with respect to the additional con- text. In an edge-exchangeable sequence via Theorem 4, EdgeEx CP – weighted NodeEx CP with wi = 1/deg(vi), results in a similar valid guarantee. Any further details about NodeEx also gener- alizes to the EdgeEx via Theorem 4. With both settings at each timestep t we have seen a set of nodes Vt. Let V(t) eval ⊂ Vt be the set of nodes evaluated at timestep t. This set can contain any node from the past as long as they are not already evaluated. Specifically, we cannot use the prediction sets of a particular node multiple times to compare and pick a specific one (see § D.1 for a discussion). We always assume that the calibration set is the union of all nodes appeared up to timestep t = T0 and the test set contains the remaining nodes. For a sequence of timesteps and corresponding evaluation sets\b(ti, Vti eval)\tT i=T0+1 where ∪T i=T0+1Vti eval = VT − Vcal, NodeEx CP (see § A for algorithm) returns valid prediction sets. We use EdgeEx CP for edge-exchangeable sequences. CP on node-conditional random subgraphs . Another application of Proposition 1 is to produce subgraph conditional prediction sets even for transductive node-classification. For each node vtest, we define K subgraphs each including {vtest}, Vcal and randomly sampled V′ ⊂ VG. Then, we average the scores and return prediction sets. In another approach, we can run K concurrent CPs, each resulting in a prediction set. Then we combine them with a randomized voting mechanism to a final prediction set. The resulting prediction sets include true labels with 1 − α probability. This approach works for any number of subgraphs. See § D.1 for further explanations. Relation to full conformal prediction . In our paper, we technically adopted the split conformal 3 approach where model training is separate from calibration due to its computational efficiency. An alternative is full conformal prediction, where to obtain a score for a (calibration) nodevi ∈ Vcal and any candidate labely′ w.r.t. a test nodevtest we train a model from scratch onVcal ∪{(vtest, y′)} and 2Similar results apply for updates with more than one edge. 3Sometimes called inductive conformal prediction. However, this is an orthogonal use of the word inductive. 6Published as a conference paper at ICLR 2024 consider its prediction for vi. This involves training |Vcal|+1 models for each y′ and each test node, which is extremely expensive but can use all4 available labels. Transductive node classification can be seen as a middle ground between split and full conformal as explained by Huang et al. (2023). Our approach is similarly in between – instead of retraining the model from scratch we simply update the embeddings (and thus scores) by performing message-passing with the newly arrived nodes/edges. 6 E XPERIMENTAL EVALUATION 6.1 E XPERIMENTAL SETUP AND DISCUSSION OF METRICS We evaluate our approach for both node and edge exchangeable sequences where we have three options to make predictions: (i) upon node arrival, (ii) at a given fixed timestep shared for all nodes, (iii) at an arbitrary node-specific timestep (we choose at random). We compare our approach with naive CP where the coverage guarantee will not hold. Later we discuss why both over- and under- coverage are invalid. For completeness, we compare NodeEx CP with NAPS in § C in addition to a detailed overview of its drawbacks. However, as discussed in § 2.1, NAPS is not a suitable baseline. We use APS (Romano et al., 2020) as the base score function while other scores are tested in § D. Models and datasets. We consider 9 different datasets and 4 models: GCN Kipf & Welling (2017), GAT Veliˇckovi´c et al. (2018), and APPNPKlicpera et al. (2019) as structure-aware and MLP as a structure-independent model. We evaluate our NodeEx, and EdgeEx CP on the common citation graphs CoraML McCallum et al. (2004), CiteSeer Sen et al. (2008), PubMed Namata et al. (2012), Coauthor Physics and Coauthor CS Shchur et al. (2018), and co-purchase graphs Amazon Photos and Computers McAuley et al. (2015); Shchur et al. (2018) (details in § E). Results for Flickr (Young et al., 2014), and Reddit2 (Zeng et al., 2019) (with GCN model and their original split) datasets are in § D.5. Here, the model only affects the efficiency and not the validity. Evaluation procedure. For any of the mentioned datasets, we sample 20 nodes per class for training and 20 nodes for validation with stratified sampling. For the node-exchangeable setup, the calibra- tion set has the same size as the training. For the edge-exchangeable sequence, the same number of edges are sampled. Therefore, each round of simulation has a potentially different number of cali- bration nodes for the edge-exchangeable setup. First, we take a random sample of nodes as train/val set and train the model on the resulting subgraph G0. Then, for the node-exchangeable sequence, we first sample a calibration set randomly from the remaining nodes. Then, at each timestep, we add a random unseen node to the graph (with all edges to the existing nodes) and predict the class probability given the updated subgraph. We use the updated conformal scores to create prediction sets for all existing test nodes until time t and record the empirical coverage results in column t of the coverage matrix C. Similarly, for the edge-exchangeable sampling, we sample calibration edges, and take both ends as calibration nodes. The remaining edges are sampled one at a time. The rest of the procedure is the same as node-exchangeable setting and results in an analogous C. Challenges with small calibration sets . With a limited number of samples in the calibration set, there will be an additional error in the coverage due to the discrete quantile function with a low sample rate. In practice, we take the ⌊n/(n + 1) · α⌋ index of a discrete array as the conformal threshold which is often not exactly equal to 1 − α quantile in the continuous domain. Therefore, we expect 1/(|Vcal| + 1) error around 1 − α. This error is in addition to the variance of the Beta or Hyper-geometric distribution and will converge to 0 with increasing calibration set size. Distance from target coverage. Our main evaluation criteria is the distance of empirical coverage w.r.t. the target 1 − α. For all datasets we set the desired coverage to 90%, and we report the (absolute) distance w.r.t. this value (see § E). Each reported result is an average of 10 different random node-exchangeable sequences and 15 different random edge-exchangeable sequences. Note that due to homophily, we observe a higher empirical coverage in the non-exchangeable case which can wrongly be interpreted as being better. To address any potential confusion, first the guarantee is invalid if it breaks either the lower or the upper-bound. In real-world deployment we do not have ground-truth labels to calculate empirical coverage, and if the guarantee is broken (in either direction) the output is unreliable – nullifying the main goal of CP. Second, a higher empirical coverage also results in larger set sizes making the prediction sets less useful (see also Fig. 4). 4This is unlike split conformal that needs to split the available labels into training and calibration sets. 7Published as a conference paper at ICLR 2024 0 1000 2000 3000 Timestep 0.85 0.90 0.95 1.00Coverage Guarantee NodeEx CP Naive CP 0 1000 2000 3000 Timestep 0.85 0.90 0.95 1.00Coverage 0 1000 2000 3000 Timestep 0.85 0.90 0.95 1.00Coverage EdgeEx CP 0.80 0.85 0.90 0.95 1.00 Coverage 0 10 20Density Theoretical Distribution Figure 3: [Upper left] Coverage over time under node exchangeability when predicting upon node arrival (diagonals ofC). [Upper right] Coverage when we instead predict at a fixed time (columns of C). [Lower right] Same as upper right but under edge-exchangeability. [Lower left] The empirical distribution of coverage when we predict at node-specific times (fixed entries of C), compared to the theoretical distribution. Sample size results in a slightexpected shift. The transparent lines show a particular sequence, and the thick solid lines shows the average over 10 (15) sequences. Efficiency and singletons . In addition to coverage, which is the main goal of our work, we also briefly study the efficiency (average set size) and the singleton hit – fraction of correct sets of size 1. As we will see in § 6.2, our NodeEx and EdgeEx CP improve these metrics as a byproduct. Some scoring functions such as RAPS (Angelopoulos et al., 2021) and DAPS (Zargarbashi et al., 2023) directly target these metrics and can be used on top of our approach. We explore DAPS in § D.3. 6.2 E VALUATION OF EMPIRICAL COVERAGE , SET SIZE AND SINGLETON HIT Table 1 shows the deviation from the coverage guarantee for different datasets when the label of each node is predicted upon its arrival. As shown in the table, while naive CP shows a significant shift from the 1 − α = 0.9 coverage, NodeEx CP maintains the empirical coverage close to the desired value. In Fig. 3 (upper left) we show the temporal evolution of coverage. We show the coverage for all nodes that arrived until time t and see that the guarantee is preserved for each timestep t. As mentioned before, the goal is to achieve near 1 − α empirical coverage, the over-coverage of standard CP might misleadingly appear as a better result. Not only does over-coverage come at the cost of less efficiency as shown in Fig. 4 (see § D.2 for details), but the empirical coverage of non-guaranteed prediction sets is not predictable apriori. Fig. 3(lower left) shows the same experiment for edge-exchangeability. While NodeEx CP guaran- tees coverage under node-exchangeability, the weights specified in our EdgeEx CP are necessary for the guarantee to hold on an edge-exchangeable sequence. Standard CP fails again. As explained in § 4 any node can be evaluated at any timestep after its appearance. Fig. 3 (upper right) shows the result when predicting at a fixed time (instead of upon arrival). Additionally, Fig. 3 (lower right) shows the empirical distribution of coverage for node-specific times (random subsets of nodes, each node predicted at a random time after appearance). For NodeEx CP, the empiri- cal distribution matches the theoretical one, while the coverage of naive CP substantially diverges. 8Published as a conference paper at ICLR 2024 Node Exch. Sequence Edge Exch. Sequence Dataset Acc NodeEx CP Naive CP EdgeEx CP NodeEx CP Naive CP Cora-ML 0.800 0.280 5.860 1.929 3.883 6.860 PubMed 0.777 1.254 4.649 1.241 3.405 5.315 CiteSeer 0.816 0.039 4.150 0.335 1.572 3.460 Coauth-CS 0.914 0.397 4.082 3.024 4.662 7.835 Coauth-Phy. 0.940 0.555 2.689 2.240 4.378 6.123 Amz-Comp. 0.788 0.263 6.373 2.687 5.727 7.036 Amz-Photo 0.868 0.127 3.483 2.546 4.130 6.613 Table 1: Average absolute deviation from guarantee (in %, forGCN model and 1 train/val split). Unsurprisingly, under node exchangeability, in Fig. 4 we see that our NodeEx CP improves both additional metrics – has smaller sets and larger singleton hit ratio. The same holds for other settings. In all experiments, we consider a sparse (and thus realistic) calibration set size, e.g. for PubMed we sample 60 nodes for calibration, which is a significantly lower number compared to other tasks like image classification with 1000 datapoints for same purpose (Angelopoulos et al., 2021). The calibration size controls the concentration (variance) around 1 − α as reflected by the transparent lines on Fig. 3. Increasing the set size reduces the variance but the mean is always 1 − α. Limitations. We identified three main limitations. First, the guarantee is marginal. Second, real- world graphs may not satisfy node- or edge-exchagability. This can be partially mitigated by the beyond exchangeability framework. Finally, the guarantee does not hold for adversarially chosen evaluation sets Veval. In other words, the choice of which nodes is evaluated at which timesteps must be prior to observing the prediction set. We provide a longer discussion on each of these limitations in § A. Our main focus is on the validity. However, we also reported other metrics such as the average set size, singleton hit ratio and other score functions in § D. 7 C ONCLUSION We adapt conformal prediction to inductive node-classification for both node and edge exchangeable graph sequences. We show that although introducing new nodes/edges causes a distribution shift in the conformity scores, this shift is symmetric. By recomputing the scores conditional to the evaluation subgraph, we recover the coverage guarantee. Under edge-exchangeability, we need to also account for the node degrees to maintain validity. Importantly, our approach affords flexibility – the guarantee holds regardless of prediction time which can be chosen differently for each node. 0 1000 2000 3000 Timestep 2.0 2.5 3.0 3.5 4.0 4.5Average Set Size NodeEx CP. Naive CP. 0 1000 2000 3000 Timestep 0.1 0.2 0.3Singleton Hit Ratio NodeEx CP. Naive CP. Figure 4: [Left] Average set size (lower is better) and [Right] singleton hits ratio (higher is better) of naive CP vs. NodeEx CP for CiteSeer and GCN. Our approach improves both metrics. 9Published as a conference paper at ICLR 2024 REPRODUCIBILITY STATEMENT All datasets of our experiments are publicly available and referenced in § 6.1 in addition to details on experimental setups. The pseudocode of our approach is provided in § A. The hyperparameters of models are given in § E. The code of experiments is also uploaded as supplementary material. REFERENCES David J Aldous. Representations for partially exchangeable arrays of random variables. Journal of Multivariate Analysis, 1981. Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021a. Anastasios Nikolas Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. ArXiv, abs/2107.07511, 2021b. Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, and Jitendra Malik. Uncertainty sets for image classifiers using conformal prediction. In International Conference on Learning Representations, 2021. Anastasios Nikolas Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Con- formal risk control. In The Twelfth International Conference on Learning Representations, 2024. Rina Foygel Barber. Is distribution-free inference possible for binary regression? arXiv: Statistics Theory, 2020. Rina Foygel Barber, Emmanuel J. Cand `es, Aaditya Ramdas, and Ryan J. Tibshirani. Predictive inference with the jackknife+. arXiv: Methodology, 2019a. Rina Foygel Barber, Emmanuel J. Cand `es, Aaditya Ramdas, and Ryan J. Tibshirani. The limits of distribution-free conditional predictive inference. Information and Inference: A Journal of the IMA, 2019b. Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Conformal prediction beyond exchangeability. The Annals of Statistics, 51, 2023. Diana Cai, Trevor Campbell, and Tamara Broderick. Edge-exchangeable graphs and sparsity. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Informa- tion Processing Systems. Curran Associates, Inc., 2016. Jase Clarkson. Distribution free prediction sets for node classification. In Proceedings of the 40th International Conference on Machine Learning, 2023. Adam Fisch, Tal Schuster, T. Jaakkola, and Regina Barzilay. Conformal prediction sets with limited false positives. Proceedings of the 39th International Conference on Machine Learning, 2022. Douglas N. Hoover. Relations on probability spaces and arrays of random variables. Preprint, Institute for Advanced Study, Princeton, NJ, 1979. Kexin Huang, Ying Jin, Emmanuel Candes, and Jure Leskovec. Uncertainty quantification over graph with conformalized graph neural networks. In Thirty-seventh Conference on Neural Infor- mation Processing Systems, 2023. Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net- works. In International Conference on Learning Representations, 2017. Johannes Klicpera, Aleksandar Bojchevski, and Stephan G ¨unnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In International Conference on Learning Representations, 2019. Jing Lei and Larry A. Wasserman. Distribution-free prediction bands for non-parametric regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76, 2014. 10Published as a conference paper at ICLR 2024 Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. Image-based recom- mendations on styles and substitutes. Proceedings of the 38th International ACM SIGIR Confer- ence on Research and Development in Information Retrieval, 2015. Andrew McCallum, Kamal Nigam, Jason D. M. Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. Information Retrieval, 2004. Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for collective classification. In 10th international workshop on mining and learning with graphs, 2012. Peter Orbanz and Daniel M Roy. Bayesian models of graphs, arrays and other exchangeable random structures. IEEE transactions on pattern analysis and machine intelligence, 37, 2014. Yaniv Romano, Matteo Sesia, and Emmanuel J. Cand `es. Classification with valid and adaptive coverage. In NeurIPS, 2020. Mauricio Sadinle, Jing Lei, and Larry A. Wasserman. Least ambiguous set-valued classifiers with bounded error levels. Journal of the American Statistical Association, 2018. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI Magazine, Sep. 2008. doi: 10.1609/aimag.v29i3. 2157. Glenn Shafer and Vladimir V ovk. A tutorial on conformal prediction.Journal of Machine Learning Research, 2008. Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G¨unnemann. Pitfalls of graph neural network evaluation. ArXiv, abs/1811.05868, 2018. Maximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Z¨ugner, and Stephan G¨unnemann. Graph posterior network: Bayesian predictive uncertainty for node classification. Advances in Neural Information Processing Systems, 2021. David Stutz, Krishnamurthy Dj Dvijotham, Ali Taylan Cemgil, and Arnaud Doucet. Learning opti- mal conformal classifiers. In International Conference on Learning Representations, 2022. Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal pre- diction under covariate shift. Advances in neural information processing systems, 2019. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li `o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations , 2018. Vladimir V ovk. Conditional validity of inductive conformal predictors. In Steven C. H. Hoi and Wray Buntine (eds.), Proceedings of the Asian Conference on Machine Learning , volume 25 of Proceedings of Machine Learning Research , pp. 475–490, Singapore Management University, Singapore, 04–06 Nov 2012. PMLR. Vladimir V ovk, Alexander Gammerman, and Glenn Shafer.Algorithmic learning in a random world, volume 29. Springer, 2005. Pivithuru Wijegunawardana, Ralucca Gera, and Sucheta Soundarajan. Node classification with bounded error rates. 2020. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2, 2014. Soroush H. Zargarbashi, Simone Antonelli, and Aleksandar Bojchevski. Conformal prediction sets for graph neural networks. In Proceedings of the 40th International Conference on Machine Learning, 2023. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Graphsaint: Graph sampling based inductive learning method. CoRR, abs/1907.04931, 2019. 11Published as a conference paper at ICLR 2024 A A LGORITHM FOR NODE EX CP AND WEIGHTED EDGE EX CP At each timestep t with node-exchangeable graph Gt, we first run the GNN model to extract con- formity scores for all nodes in Vt. We compute the α-quantile of the conformity scores conditional to the current graph, and for each new test node vtest ∈ V(t) eval we return any candidate label with a score larger than the conditional threshold. For an edge-exchangeable graph, the algorithm is still the same unless we compute the conditional weighted quantile with weights equal to 1/d(v) for all calibration vertices v. Algorithm 1 shows the pseudocode for NodeEx CP. The code is accessible at github.com/soroushzargar/conformal-node-classification. Algorithm 1: NodeEx and EdgeEx CP for inductive node classification Data: Graph Gt at timestep t. Calibration vertex set Vcal, Permutation equivariant score function s (e.g. a function of GNN), Evaluation vertex set V(t) eval Result: Ct(vj) : vj ∈ V(t) eval 1 St ← s(Gt) ; // Get score functions for all vertices 2 if node-exchangeable sequence then 3 Set τt ← Quant Ä α; {St[i, yi]}vi∈Vcal ; 1 ä ; 4 else if edge-exchangeable sequence then 5 Set τt ← Quant Ä α; {St[i, yi]}vi∈Vcal ; 1/deg(vi | Gt) ä ; 6 ∀vj ∈ V(t) eval, set Ct(vj) = {y : St[j, y] ≥ τt} ; Naive CP. In naive CP we compute calibration scores before the arrival of any new node, on the inductive subgraph over Vtr ∪ Vcal. Prior to the evaluation of any node, we compute the calibration quantile q from calibration scores. For any test node, the conformity score of all its classes is compared to q. Clarkson (2023) shows that this approach fails to provide valid prediction sets (the guarantee is broken due to the shift caused by the arrival of new nodes). Computational complexity. Conformal prediction has two main computational routines in addition to model’s inference: computing scores and quantiles. For the calibration set we only need the scores of the true class, but for test nodes, we should compute scores for all classes. Depending on the score function the complexity can be different. For standard CP we compute calibration scores once, but for NodeEx CP for each evaluation (timestep) we need to recompute the predictions and the conformity scores. Hence, Node(Edge)Ex CP has an overhead of O(n × ts × t) for t timesteps, n calibration nodes, and ts the time for computing a conformity score for one class and one node. At each step we should also evaluate the model once if the model’s prediction changes across time. Computing the quantile threshold takes O(n) steps. For standard CP we have to compute this value once, but in NodeEx CP this values is updated upon each evaluation. The total wall-clock overhead is less the a second. Additionally all mentioned complexities are for serial computation and some of CP procedures can run significantly faster via parallel computation. Limitations of NodeEx and EdgeEx CP . Our study mainly focuses on approaches that can pro- vide a valid CP for changing subgraphs under certain assumptions of node-or edge-exchangeability. There are three main limitations for using NodeEx (or EdgeEx) CP (i) The guarantee provided by NodeEx (and EdgeEx) CP is marginal. However, it is shown that exact conditional coverage Prob [y ∈ C(x) | x] is not achievable. Still, there are methods that tend to get better approximations of conditional coverage. (ii) In real-life graph sequences, it is hard to determine whether the graph sequence is node- or edge-exchangeable or neither. Here a follow-up works is to use the beyond ex- changeability approach (Barber et al., 2023) to achieve valid coverage guarantees without the need for node- or edge-exchangeability assumption. (iii) Our result maintains validity as long as the se- lection evaluation timestep for each node is done without any knowledge of the prediction set. An adversarial selection can cause a significant deviation from the guaranteed coverage. For instance, an adversary can observe prediction sets for a particular node during various timesteps and pick the timestep of the smallest prediction set similar to the two false examples in § D.1. This results can lead to a significant miscoverage rate compared to the guarantee. 12Published as a conference paper at ICLR 2024 B A DDITION TO THE THEORY Matric C. For timestep t and index i, C[i, t] indicates that whether node vi if evaluated at timestep t is going to be covered or not. Hence this value can be either 1, indicating CP covers node vi at timestep t, 0 showing that CP does not cover this node, or N/A showing that this node has not appeared at this timestep. This matrix is constructed using ground-truth labels and is shown only for the purpose of sanity check and evaluation. The matrixC is a very dense matrix since1 −α entities at each column should be 1. That is why in Fig. 2 we showed1 −C where each column is expected to have α-percentage of ones. A valid CP approach should result in a similar percentage of ones in each column. For an invalid CP, if the trend is gradually moving toward over-coverage, the matrix 1 − C becomes more sparse in later columns. Node- and edge- exchangeability . A sequence Z = ( z1, . . . , zn) is called exchangeable if for any permutation µ, the joint probability of the permuted sequence remains the same, i.e. Prob [(z1, . . . , zn)] = Prob [( µ(z1), . . . , µ(zn))]. Before considering inductive graph sequences, we first define node- and edge-exchangeability on a fixed given graph G. Definition 1 (Node-exchangeability). Let G(V, E, X, y) be a graph where i-th row of X is the feature vector for node vi, and similarly, i-th element of y is the label of that node. Let µV : {1, . . . , n} × {1, . . . , n} with n = |V| be a permutation. We define µV (G) to be a relabeling of vertices V with µV (E) = {(µV (vi), µV (vj)) : ∀(vi, vj) ∈ E}, µV (X)[i, ·] = X[µV (i), ·], and µV (y)[i] = y[µV (i)]. The permuted graph is µV (G)(µV (V), µV (G), µV (X), µV (y)). G is called node exchangeable if Prob [G = G] = Prob [G = µV (G)], where G is sampled from a generative graph distribution. Definition 2 (Edge-exchangeability). Let G(V, E, X, y) where E = {e1, . . . em} is the set of edges. Each edge ek is defined as a pair ek = ((vi, xi, yi), (vj, xj, yj)). For each node vi ∈ Vlet X be the feature matrix with rows xi, and y be the vector of labels with entries yi. Let µ : {1, . . . , m} × {1, . . . , m} with m = |E| be a permutation. We define µ(G) to be a relabeling of edges E with µE(E) = \beµE(i) : ∀ei ∈ E\t. The permuted graph is µE(G)(V, µE(E), X, y). G is called edge exchangeable if Prob [G = G] = Prob [G = µE(G)], where G is sampled from a generative graph distribution. By an inductive sequence, we refer to a progressive sequence of graphs meaning that for each timestep t, the graph Gt−1 is a subgraph of Gt. Node- and edge-inductive sequences are defined as follows: Definition 3 (Node-inductive sequence). A node-inductive sequence G0, G1, . . .is a sequence start- ing from an empty graph G0 = ( ∅, ∅). For each timestep t, the graph Gt is defined by adding a vertex vt with all its connections to the vertices in the previous timestep. The vertex set is then Vt = Vt−1 ∪ {vt} and the edge set is the union of all previous edges Et−1 and any edge between vt and Vt−1; Et = Et−1 ∪ (∩∞ i=1 {e = (vt, vi) : e ∈ Ei, vi ∈ Vi−1}). Similarly an edge-inductive sequence is defined as follows: Definition 4 (Edge-inductive sequence). An edge inductive sequence G0, G1, . . .starts from an empty graph G0 = (∅, ∅). For each timestep t, the graph Gt is defined by adding an edge et = (vi, vj) to the graph at the previous timestep. Hence Et = Et−1 ∪ {et}, and Vt = Vt−1 ∪ {vi, vj}. A node-inductive sequence is called node-exchangeable if for all graphs Gt in the sequence, Gt is node-exchangeable. Similarly, if all graphs in the sequence are edge-exchangeable, the sequence is also edge-exchangeable. Note, an inductive sequence could be equivalentlty defined with respect to a “final” graphGn, where n can be infinity, by starting from an empty graph and adding a random node/edge at each timestep. If the final graph is node-exchangeable any node-inductive subgraph of itG′ ⊆ Gt, G′ is also node ex- changeable. A similar argument holds for edge-inductive subgraphs of a “final” edge-exchangeable graph. Permutation invariance and equivariance. A permutation invariant function f returns the same output for any permutation applied on its inputs, f(z1, . . . , zn) = f(zµ(1), . . . zµ(n)) for any per- mutation µ. A function f is permutation-equivariant if permuting the input results in the same permutation on the output, i.e. for any f(x1, . . . , xn) = (y1, . . . , yn) we have f(xµ(1), . . . xµ(n)) = 13Published as a conference paper at ICLR 2024 (yµ(1), . . . yµ(n)) for any µ. A permutation-equivariant GNN assigns the same predictions (and thus the same scores) to each node even when nodes/edges are relabeled. Proof for Proposition 1. First assume V(t) eval = Vt meaning that any node appeared at timestep t is either in the calibration or the evaluation set. Due to node-exchangeability, Vt and Vcal are exchangeable. Hence, we adapt the proof of Theorem 2. Now consider the general case where V′ := Vt \\ Vcal ̸= ∅. Any subset A′ of an exchangeable set A is still exchangeable. For any permu- tation of A′ there are k permutations in A all having the same ordering over elements of A′. Since k is a constant for all permutations and function of|A| and |A′|, any permutation in A′ has the same probability. Which implies the exchangeability of its elements. This implies that with non-empty set V′, Vcal and Vt \\V ′ are still exchangeable. As a result, again we adapt Theorem 2 with the effect of V′ being symmetric to the calibration and evaluation sets. Proof for Theorem 3. We break the proof to two parts. Part 1. For a fixed node vi at timestep t1 let αi := E[ci,t1 ]. For any timestep t2 with which vi is existing in, we have E[ci,t2 ] = αi. Note that since vi is fixed its expected coverage probability is not exactly 1 − α. As in definition, αi = Prob î Quant Ä α; {s(vj, yj | Gt1 )}vj∈Vcal ; 1 ä ≤ s(vi, yi | Gt1 ) ó Let V′ j = Vtj \\ (Vcal ∪ {vi}). Node-exchangeability implies that V′ 1 and V′ 2 have same and also symmetric effect on all scores. Hence, with either of sets as context, the expected order of elements remains similar and E[ci,t2 ] = αi. Moreover, for each node vi and all t, ci,t ∼ Bernoulli(αi). Part 2. The coverage of any sub-partitioning IT can be written as an average over a set of elements in CT . In other words define ι(vi) = j ⇔ (vi ∈ Vj eval) we have Cov(IT ) = 1 |IT | |IT |X i=1 C[i, ι(vi)] Part 1 = |IT |X i=1 C[i, T] Following Proposition 1 we have Prob [Cov(IT ) ≤ β] = 1 |IT | |IT |X i=1 C[i, ι(vi)] = 1 − ΦHG(iα; |IT | + n, n, iα + ⌈|IT | ·β⌉) Proof for Theorem 4. We divide the proof into two cases: Graph where all nodes have degree 1 . In this case Gt is a union of disjoint edges (e1, . . . em) We divide Vt = V1 ∪ V2 including one and only one endpoint of each edge decided at random. The vertex set in this case has the same size as the edge set, and there is a one-to-one mapping between any permutation µ over the edge set to vertices in V1 or V2 since only one endpoint is present in each set. Hence Prob [E] = Prob [Eµ] implies Prob [Vi] = Prob [Vi,µ]i∈{1,2}. Similarly, selection Vcal decomposes to Vcal,1 ∪ Vcal,2 and again Vcal,i, and Vi \\ Vcal,i are exchangeable due to node-exchangeability in Vi. Any test node belongs to either of the subsequences which means is guaranteed via the CP applied to that subset. The vertex set is divided into two subsets, and the intersection of the calibration set to each subset is exchangeable. Hence calibrating on each subset result in 1 − α coverage for all the nodes in the subset. Both calibrations have the same expected quantile threshold q. This is because each calibration node has 1/2 probability of being included in each of the sets. General graphs. We follow the same approach. We divide the endpoints of edges into two multi- sets. For each edge we run this division and this decision is made for each vertex deg(vi) times. In each partition, each node vi has the expected frequency deg(vi)/2. Via Lemma 1 we show that weighted CP is valid for each subsequence Vi and hence for Vt. 14Published as a conference paper at ICLR 2024 0 1000 2000 Timestep 0 1000 2000Applicable Nodes 1-hop NAPS 2-hop NAPS Arrived Nodes 0 1000 2000 Timestep 101 102 Cal. Set Size No. Cal. Nodes Figure 5: [Left] The proportion of nodes without a prediction set due to empty calibration on CoraML dataset. The plot shows the inapplicability of k-hop NAPS with k ∈ {1, 2}. [Right] The average size of each node’s filtered calibration set. Note that this plot excludes non-applicable nodes. 0 1000 2000 Timestep 0.6 0.7 0.8 0.9 1.0Emp. Coverage Guarantee 0 1000 2000 Timestep 0 1000 2000Inapplicable Nodes NAPS NodeEx CP Figure 6: [Left] Comparison of NAPS and NodeEx CP in empirical coverage for CoraML dataset and GCN model across different permutations of nodes. [Right] The number of nodes without pre- diction set for different permutations on the same dataset/model. C L IMITATIONS OF NAPS For the inductive scenario, NAPS (Clarkson, 2023) adapts the beyond exchangeability approach (Barber et al., 2023) without any computed bounds on the coverage gap (the distance between the coverage of the given non-exchangeable sequence and1−α). For each test node, calibration weights are assigned equal to 1 in case they are in the immediate neighborhood of the test node and 0 other- wise. In other words, this approach filters out the non-neighbor calibration nodes for each test node. This weight assignment is generalized to any k-hop neighbor while it is originally suggested to use k = 1 or 2. In a sparse graph, or with a limited calibration set, the probability of having calibra- tion points in the immediate neighborhood is significantly low which leaves many of the test nodes with empty calibration sets. Fig. 5 shows that for our experimental setup, until the end of the graph sequence, a notable proportion of the nodes are left without a prediction set. For the same reason, even for nodes in the neighborhood of the calibration set, the statistical efficiency is very low. Since this inapplicability is reported on a node-exchangeable sequence, it is independent of the prediction timestep – a node disconnected from the calibration set will remain disconnected as the graph grows. Setting aside non-applicable nodes we compared NAPS with NodeEx CP in empirical coverage. Fig. 6 shows that NAPS is significantly far from the coverage guarantee compared to NodeEx CP. Note that this experiment is very biased in favor of NAPS as it excludes nodes without a prediction set while evaluating the same nodes for NodeEx CP. 15Published as a conference paper at ICLR 2024 D A DDITIONAL EXPERIMENTS D.1 CP WITH SUBGRAPH SAMPLING Standard CPSub CP (Bern)Sub CP (Max)Sub CP (Min) 0.7 0.8 0.9Coverage Guarantee Figure 7: Comparison between standard CP, Bernoulli prediction sets from NodeEx CP on random subgraphs, the union, and intersection of CPs. Although the theory in § 4 concerns induc- tive graph sequences, we can leverage its results to transductive node-classification via subgraph sampling. For each test node vtest, we sample K inductive subgraphs all including {vtest} ∪ Vcal. Considering Theorem 3, in each subgraph, calibration scores and test scores are exchangeable. This leaves K prediction sets all with a coverage guarantee of 1 − α. We consider each prediction set as a vote for all its ele- ments. Each label y′ will be selected in the final prediction set with a Bernoulli exper- iment with parameter p = PK i=1 1[yj ∈ Ci(vtest)]/K. Again the resulting predic- tion set contains the true label with 1 − α probability. It is important to note that Theorem 3 does not imply any result about the probability of a node being covered in all of K prediction sets. Specifically, the probability of a node being covered in all prediction sets simultaneously is less than (or equal to) the probability of the same event in each single CP. As shown in Fig. 7 the sub- graph Bernoulli prediction sets result in the same coverage as standard CP for transductive node- classification. Additionally, the union and intersection of CPs respectively result in over-coverage and under-coverage. D.2 S ET SIZE AND SINGLETON HIT RATIO In § 6 we focused on empirical coverage as the main evaluation criteria since our goal is to recover the conformal guarantee. When the guarantee holds, this metric is often reported as a sanity check. In that case, the average prediction set size (efficiency) is considered as a direct indicator of how efficient (and therefore useful) prediction sets are. Another important metric called singleton hit ratio is the frequency of singleton sets covering the true label. In Fig. 8 we show the coverage matrix alongside the prediction set size and singleton hits indicator for each node at each timestep. As shown in the figure, since the standard CP breaks the guarantee toward over-coverage, it results in larger prediction sets on average and hence, a lower number of singleton hits. The same result is also shown in Fig. 4 over different timesteps. D.3 D IFFERENT SCORE FUNCTIONS The threshold prediction sets (TPS) approach (Sadinle et al., 2018) directly takes the softmax output as a conformity score s(x, y) = π(x)y, where π(x)y is the predicted probability for class y. Al- though TPS produces small prediction sets, its coverage is biased toward easier examples leaving the hard examples under-covered (Angelopoulos & Bates, 2021b). Romano et al. (2020) define adap- tive prediction sets (APS) with a score function defined ass(x, y) := −(ρ(x, y) + u · π(x)y). Here ρ(x, y) := PK c=1 π(x)c1 [π(x)c > π(x)y] is the sum of all classes predicted as more likely than y, and u ∈ [0, 1] is a uniform random value that breaks the ties between different scores to allow exact 1 − α coverage (Stutz et al., 2022). APS returns the smallest prediction sets satisfying conditional coverage if the model returns the ground truth conditional probability p(y | x), otherwise Barber et al. (2019b) show that achieving conditional coverage is impossible without strong unrealistic as- sumptions. Here we use APS as baseline scoring function, but our method is orthogonal to this choice and works with any score. 16Published as a conference paper at ICLR 2024 0 1000 2000 3000 0 1000 2000 3000 Nodes 0 100020003000 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 3000 Timestep 0 1000 2000 3000 Nodes 0 100020003000 Timestep 0 1000 2000 3000 0 1000 2000 3000 Timestep 0 1000 2000 3000 0 2 4 0 2 4 Figure 8: [Left column] The coverage matrix 1 − C (colored points show miscoverage). [Middle column] The prediction set size for each point at each timestep. [Right column The matrix of singleton hits. A cell is colored if the node at the timestep is predicted with a singleton set covering the true label. [Upper row] The NodeEx CP approach. [Lower row] The standard CP. The result is shown for CiteSeer dataset and GCN model. Although we used APS as the base score function for evaluation, our results are general to any other scoring function. Here we evaluate our approach with two other score functions called threshold prediction sets (TPS) (Sadinle et al., 2018), and diffused adaptive prediction sets DAPS Zargarbashi et al. (2023). TPS (Fig. 9 - left) simply applies the softmax function on model’s results (logits) and uses it as the conformity score. Although it produces small prediction sets its coverage is biased toward easier examples (Angelopoulos & Bates, 2021b). DAPS (Fig. 9 - right) works as a structure-aware extension scoring over APS. It diffuses conformity scores over the network structure 0 1000 2000 3000 Timestep 0.70 0.75 0.80 0.85 0.90 0.95 1.00Coverage Guarantee NodeEx CP (TPS) Naive CP (TPS) 0 1000 2000 3000 Timestep 0.70 0.75 0.80 0.85 0.90 0.95 1.00Coverage Guarantee NodeEx CP (DAPS) Naive CP (DAPS) Figure 9: [Left] Comparison of naive CP and NodeEx CP with TPS score function and [Right] DAPS score function. Results are for CiteSeer dataset and GCN model. 17Published as a conference paper at ICLR 2024 to leverage the uncertainty information in the neighborhood and produce a more efficient prediction set. Since DAPS also incorporates the structure in the scores’ space (in addition to the implicit effect via message passing) it is even more influenced by the changes in the graph structure while using standard CP. However, as shown in the Fig. 9, we can recover the coverage guarantee via NodeEx CP regardless of the utilized score function. D.4 D IFFERENT MODELS AND INITIAL SPLITS The coverage guarantee in CP holds regardless of the model structure. CP uses the model as a black-box and just performs the quantile calibration over the output of the model which is the input conformity score function. However, better model structure, training procedure, etc are reflected in other metrics like set size and singleton hits. As shown in Fig. 10, NodeEx CP results in similar coverage values for all the models while standard CP results in different coverage for each model (note the significant distance between structure-aware models and MLP). As MLP does not take the adjacency structure into account, its empirical coverage is still guaranteed even with the standard CP. Note that different models result in different efficiencies in the prediction sets. Fig. 11 shows the same experiment conducted on edge-exchangeable sampling. Again similar results are observed for edge-exchangeable sequences. As pointed out in (Shchur et al., 2018), GNNs are sensitive to the initial train/validation sampling. This however does not impact the coverage of NodeEx (and EdgeEx) CP as it is guaranteed agnostic to the model and the initial split. However similar to different model architectures, different initial splits also affect the model’s accuracy which is reflected in the efficiency of the prediction set. 1000 2000 0.8 0.9 1.0Coverage 1000 2000 2 4Ave. Set Size APPNPNet 1000 2000 0.8 0.9 1.0Coverage 1000 2000 2 3Ave. Set Size GAT 1000 2000 0.8 0.9 1.0Coverage 1000 2000 2 4Ave. Set Size GCN 1000 2000 No. Nodes 0.8 0.9 1.0Coverage NodeEx Naive 1000 2000 No. Nodes 3.0 3.5Ave. Set Size MLP Figure 10: [Left] The empirical coverage for different models. [Right] The average set size. The results are shown for the CoraML dataset with node-exchangeable sampling. 18Published as a conference paper at ICLR 2024 500 1000 1500 2000 0.8 0.9 1.0Coverage 2000 4000 6000 3 4Ave. Set Size APPNPNet 500 1000 1500 2000 0.8 0.9 1.0Coverage 2000 4000 6000 2 3Ave. Set Size GAT 500 1000 1500 2000 0.8 0.9 1.0Coverage 2000 4000 6000 2 3Ave. Set Size GCN 500 1000 1500 2000 No. Nodes 0.8 0.9 1.0Coverage NodeEx Naive EdgeEx 2000 4000 6000 No. Nodes 2.5 3.0 3.5Ave. Set Size MLP Figure 11: [Left] The empirical coverage for different models. [Right] the average set size. The results are shown for the CoraML dataset with edge-exchangeable sampling. Fig. 12 verifies this fir the node-exchangeable sampling. We also evaluated our method compared to standard CP for edge-exchangeable sequences over different initial sampling. The result is in Fig. 13. D.5 O THER EXPERIMENTS Experiment corresponding to Fig. 1. We performed a node-exchangeable sampling onCiteSeer dataset. At each timestep, we ran the model ( GCN trained on the train/val nodes) on the existing subgraph extracting conformal scores for all existing test nodes. The heatmap in Fig. 1 (left) shows the distribution of test scores (sorted) at each timestep. An oracle CP (with access to test nodes and their label) will choose the line labeled as “ground truth” as the conformal threshold since it is the exact α quantile of existing test scores. This is shown as the ideal reference to evaluate each CP approach with respect to it. We draw the threshold computed by standard CP and NodeEx CP. It is shown that the NodeEx CP picks thresholds close to the ideal line while the naive CP shifts from it due to the message passing with the new nodes. We drew the distribution of conformity score for calibration nodes at some selected times (sampled with equal distance across from all timesteps) and compared it with calibration scores used by stan- dard CP in Fig. 1(upper right). A distribution shift is clearly observable. We showed this shift in Fig. 1(upper right) by plotting the distibutions across various timesteps. The corresponding timestep for each distribution is marked in the left subplot with the same color. Here we computed the EMD (earth mover distance) between the scores of standard CP and conformity scores (of true labels) for calibration and test points. It is shown that the distribution shift is increasing by the number of new nodes introduced to the graph. This shift is almost similar in calibration and test. One source of 19Published as a conference paper at ICLR 2024 0 1000 2000 No. Nodes 0.80 0.85 0.90 0.95 1.00Coverage NodeEx CP Naive CP Guarantee 0 792 1188 1584 1980 No. Nodes 1 2 3 4 5Average Set Size Figure 12: [Left] The empirical coverage of standard CP and NodeEx CP with different initial train/val splits. [Right] The average set size. The result is shown for CoraML dataset and GCN model for node-exchangeable samplings. 0 500 1000 1500 2000 No. Nodes 0.80 0.85 0.90 0.95 1.00Coverage NodeEx CP Naive CP EdgeEx CP Guarantee 0 1188 2376 3564 4752 No. Nodes 1.5 2.0 2.5 3.0 3.5Average Set Size Figure 13: [Left] The empirical coverage of standard CP and EdgeEx CP with different initial train/val splits. [Right] The average set size. The result is shown for CoraML dataset and GCN model for edge-exchangeable samplings. the noise in thresholds and EMD is the uniform random value in APS scoring function. EMD is smoothed over 10 steps. Large Datasets. We ran NodeEx CP and compared it with the baseline for two large datasets: Flickr (Young et al., 2014), Reddit2 (Zeng et al., 2019). Our results for Flicker and Reddit2 datasets under node exchangeable sampling are shown in Fig. 14. Both NodeEx CP and naive CP show similar results. In the edge-exchangeable sampling Fig. 15 shows a significant deviation from the guarantee for NodeEx CP and naive CP, while EdgeEx CP maintains a valid coverage. Class-conditional coverage. Conformal prediction comes with a marginal guarantee which means that it does not guarantee conditional to group or class. In Fig. 16 we compare this metric. In most of the classes, we observe the standard CP to be closer to the guaranteed line. Effect of calibration set size . In non-graph data as mentioned in § 2 the coverage probability follows a Beta distribution. In graph data with a fixed number of test nodes (see Theorem 2) the distribution of coverage over test nodes follows a collection of hyper-geometric distributions. In both scenarios, there is a possible variance around predefined 1 − α probability which is a function 20Published as a conference paper at ICLR 2024 0 5000 10000 15000 20000 Timestep 0.86 0.88 0.90 0.92 0.94Coverage Naive CP NodeEx CP 0 5000 10000 15000 20000 Timestep 0.86 0.88 0.90 0.92 0.94Coverage Naive CP NodeEx CP Figure 14: Coverage Results for [Left] Flickr, and [Right] Reddit2 dataset under node- exchangeable sampling. The results are shown for GCN model. 0 5000 10000 15000 20000 Timestep 0.80 0.85 0.90 0.95 1.00Coverage Naive CP NodeEx CP EdgeEx CP 0 5000 10000 15000 20000 Timestep 0.80 0.85 0.90 0.95 1.00Coverage Naive CP NodeEx CP EdgeEx CP Figure 15: Coverage Results for [Left] Flickr, and [Right] Reddit2 dataset under edge- exchangeable sampling. The results are shown for GCN model. of calibration set size. As the number of calibration nodes increases, the distribution of coverage probability concentrates around 1 − α. Since in our experiments we followed a realistic setup (not allowing calibration set to be larger than training set) there is a variance observed around the guarantee line – each line converges to a value close to but not exactly equal to1 −α. Fig. 17 shows that as we increase the calibration set size the result concentrates around the guarantee. E S UPPLEMENTARY DETAILS OF EXPERIMENTS Datasets. Table 2 provides specifications of datasets used in our experimental evaluations, including the number of nodes, edges, and homophily. The details of label sampling are provided in § 6. For each experiment, we ran each CP approach on 10 different sequences in node-exchangeable and 15 different sequences on edge-exchangeable setup. All transparent lines in plots show the result for one sequence. In those experiments, the solid line shows the average of sequences at each timestep. Models. For all architectures, we built one hidden layer of 64 units and one output layer. We applied dropout on the hidden layer with probability 0.6 for GCN, and GAT, 0.5 for APPNPNet, and 0.8 for 21Published as a conference paper at ICLR 2024 0 1000 2000 3000 0.80 0.85 0.90 0.95 1.00Coverage Naive. CP NodeEx. CP Guarantee 0 1000 2000 3000 0.80 0.85 0.90 0.95 1.00 0 1000 2000 3000 Timestep 0.80 0.85 0.90 0.95 1.00Coverage 0 1000 2000 3000 Timestep 0.80 0.85 0.90 0.95 1.00 Figure 16: NodeEx CP and standard CP in class-conditional coverage. The result is for CiteSeer dataset and GCN model. Plots show classes 1 and 2 [Upper row left to right], 3 and 4 [Lower row]. 0 1000 2000 Timestep 0.80 0.85 0.90 0.95 1.00Coverage Guarantee 0 500 1000 1500 Timestep 0.80 0.85 0.90 0.95 1.00Coverage NodeEx CP. Naive CP. Figure 17: Effect of different calibration set sizes on the concentration of coverage probability. The results are on Cora-ML dataset and for 200 [Left] and 1000 [Right] calibration nodes. MLP. For GAT we used 8 heads. We trained all models with categorical cross-entropy loss, and Adam optimizer with L2 regularization. Adaptive and constant coverage. Despite many studies on CP, Zargarbashi et al. (2023) chooses an adaptive value for 1 − α which is conditional to the model accuracy. This is a suitable choice when the main comparison is based on set size and other efficiency-related metrics. The main supporting idea is that efficiency should be compared with a guarantee that is not trivially achievable by the normal model prediction. However, our main concern is to recover the guarantee meaning that the empirical coverage is the main comparison criteria. Hence we choose 0.9 as the required coverage for all datasets and models regardless of the accuracy. Furthermore, NodeEx CP works for any user-specified coverage guarantee including model-conditional values. 22Published as a conference paper at ICLR 2024 Table 2: Statistics of the datasets. Dataset Name Vertices Attributes Edges Classes Homophily CoraML 2995 2879 16316 7 78.85% PubMed 19717 500 88648 3 80.23% CiteSeer 4230 602 10674 6 94.94% Coaut. CS 18333 6805 163788 15 80.80% Coauth. Physics 34493 8415 495924 5 93.14% Amz. Comp. 13752 767 491722 10 77.72% Amz. Photo 7650 745 238162 8 82.72% F R ELATED WORKS Standard conformal prediction . CP is introduced by V ovk et al. (2005) and further developed in Lei & Wasserman (2014); Shafer & V ovk (2008); Barber (2020). Different variants of CP are yet defined ranging in the trade-off between statistical and computational efficiency. While full conformal prediction requires multiple training rounds for each single test point, Jackknife+ Barber et al. (2019a), and split conformal prediction sacrifice statistical efficiency, defining faster and hence more scalable CP algorithms. A comprehensive survey of CP can be found in Angelopoulos & Bates (2021a). In this study, we focus on split conformal prediction. Further contributions in this area include generalization of the guarantee from including the true label to any risk function Angelopoulos et al. (2024), improving the efficiency (reducing the average set size) by simulating calibration during training Stutz et al. (2022), limiting the false positive rate Fisch et al. (2022), etc. CP without exchangeability . Standard CP requires exchangeability for datapoints and assumes the model to treat datapoints symmetrically. The latter assumption ensures the exchangeability of datapoints even after observing the fitted model. Tibshirani et al. (2019) extend CP to cases where exchangeability breaks via different p(X) for calibration set and the test data, given p(Y | X) is still the same. In this case, CP is adapted via reweighing the calibration set using the likelihood ratio to compare the training and test covariate distributions. This requires the high-dimensional likelihood to be known or well-approximated. Barber et al. (2023) does neither rely on known likelihood between the original and shifted distribution nor the symmetry in the learning algorithm and proposes a coverage gap based on the total variation distance between calibration points and the test point. The main upperbound on the coverage requires a predefined fixed weight function. To adapt this bound to data-dependent weights, the dTV-distance must be computed conditional to the assigned weights. CP for graphs. Recently adapting CP to graphs got increasing attention. Wijegunawardana et al. (2020) adapted conformal prediction for node classification to achieve bounded error. Clarkson (2023) assumed exchangeability violated for node classification hence adapting weighted exchange- ability without any lowerbound on the coverage, however, Zargarbashi et al. (2023) and Huang et al. (2023) proved the applicability of standard CP for transductive setting in GNNs. Moreover, Huang et al. (2023) proposed a secondary GNN trained to increase the efficiency of prediction sets, and Zargarbashi et al. (2023) stated that in homophily networks, diffusion of conformal score can increase the efficiency significantly. For the inductive GNNs, the only work we are aware of is the neighborhood APS (NAPS) approach which is an adaptation of weighted CP Clarkson (2023), however, it is shown that NAPS can not be applied on a significant fraction of nodes if the network is sparse or the quantity of calibration nodes are limited Zargarbashi et al. (2023) (this number increases to over 70% in some benchmark datasets). As our approach is adaptable to sparse networks and works for any calibration set size, we do not compare our approach with NAPS. 23",
      "references": [
        "Representations for partially exchangeable arrays of random variables",
        "A gentle introduction to conformal prediction and distribution-free uncertainty quantification",
        "Uncertainty sets for image classifiers using conformal prediction",
        "Conformal risk control",
        "Is distribution-free inference possible for binary regression?",
        "Predictive inference with the jackknife+",
        "The limits of distribution-free conditional predictive inference",
        "Conformal prediction beyond exchangeability",
        "Edge-exchangeable graphs and sparsity",
        "Distribution free prediction sets for node classification",
        "Conformal prediction sets with limited false positives",
        "Relations on probability spaces and arrays of random variables",
        "Uncertainty quantification over graph with conformalized graph neural networks",
        "Semi-supervised classification with graph convolutional networks",
        "Predict then propagate: Graph neural networks meet personalized pagerank",
        "Distribution-free prediction bands for non-parametric regression",
        "Image-based recommendations on styles and substitutes",
        "Automating the construction of internet portals with machine learning",
        "Query-driven active surveying for collective classification",
        "Bayesian models of graphs, arrays and other exchangeable random structures",
        "Classification with valid and adaptive coverage",
        "Least ambiguous set-valued classifiers with bounded error levels",
        "Collective classification in network data",
        "A tutorial on conformal prediction",
        "Pitfalls of graph neural network evaluation",
        "Graph posterior network: Bayesian predictive uncertainty for node classification",
        "Learning optimal conformal classifiers",
        "Conformal prediction under covariate shift",
        "Graph attention networks",
        "Conditional validity of inductive conformal predictors",
        "Algorithmic learning in a random world",
        "Node classification with bounded error rates",
        "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
        "Conformal prediction sets for graph neural networks",
        "Graphsaint: Graph sampling based inductive learning method"
      ],
      "meta_data": {
        "arxiv_id": "2407.09173v1",
        "authors": [
          "Soroush H. Zargarbashi",
          "Aleksandar Bojchevski"
        ],
        "published_date": "2024-07-12T11:12:49Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses invalidity of standard conformal prediction (CP) for inductive node classification with GNNs: when new nodes/edges arrive, message passing shifts embeddings and hence conformity scores, breaking exchangeability between calibration and test scores. Proposes Conformal Inductive GNNs that restore distribution-free 1−α marginal coverage in inductive settings under two realistic symmetry assumptions: (i) node-exchangeable graph sequences and (ii) edge-exchangeable graph sequences (allowing sparsity). Key theoretical results: (a) NodeEx CP—recompute calibration scores and quantile conditional on the current subgraph at the prediction time—recovers standard CP coverage without loss of statistical efficiency; (b) EdgeEx CP—apply weighted CP with weights 1/deg(v) on calibration nodes—recovers 1−α coverage for edge-exchangeable sequences by relating edge-exchangeability to a weighted node-exchangeable score process; (c) Coverage guarantee is independent of prediction time: a node can be predicted upon arrival or at any later time with the same marginal guarantee, provided the evaluation schedule is chosen without observing prediction sets.",
        "methodology": "Uses split conformal prediction with continuous conformity scores (ties broken by randomization) and adaptive prediction sets (APS) as the default score: s(x,y)=−(ρ(x,y)+u·π(x)_y), where ρ sums probabilities of classes ranked above y. For node-exchangeable inductive sequences: at each timestep t, run the (fixed, pre-trained) permutation-equivariant model on the updated graph G_t, recompute conformity scores for all calibration nodes conditional on G_t, compute the α-quantile threshold, and form prediction sets for evaluated test nodes using that time-specific threshold (NodeEx CP). For edge-exchangeable sequences: define a weighted quantile over calibration scores with weights w_i=1/deg(v_i|G_t) (EdgeEx CP). Proves validity via exchangeability arguments for scores conditional on subgraphs (node-exchangeable) and via a weighted-quantile lemma plus decomposition of edge-exchangeable sequences into weighted node-exchangeable subsequences (edge-exchangeable). Discusses that additional covariate-shift/beyond-exchangeability weighting (Barber et al., 2023) can be layered on top to handle genuine distribution shift, orthogonal to the message-passing-induced shift they correct.",
        "experimental_setup": "Task: inductive node classification on evolving graphs. Models: GCN, GAT, APPNP (structure-aware) and MLP (structure-independent). Datasets (9 total): citation graphs Cora-ML, CiteSeer, PubMed; coauthor graphs Coauthor CS, Coauthor Physics; Amazon co-purchase Computers, Photos; plus large-scale Flickr and Reddit2 (additional section). Data splits: stratified sampling with 20 labeled nodes per class for training and 20 per class for validation; calibration set size equal to training size for node-exchangeable sequences. Sequence simulation: train model on initial subgraph G0 induced by train/val nodes; then create (a) node-exchangeable inductive sequences by randomly selecting an unseen node at each timestep and adding it with all incident edges to existing nodes; or (b) edge-exchangeable sequences by sampling a set of calibration edges first (endpoints become calibration nodes), then adding remaining edges one-by-one. At each timestep, compute prediction sets either (i) upon node arrival, (ii) at a fixed common timestep, or (iii) at random node-specific timesteps; record coverage matrices across time. Main metric: absolute deviation (%) of empirical marginal coverage from target 1−α (set to 0.9), averaged over 10 random node sequences and 15 edge sequences; additional metrics: average set size (efficiency) and singleton hit ratio. Baselines: naive CP with static calibration threshold computed before graph updates; discussion/ancillary comparison to NAPS (Clarkson, 2023) showing inapplicability on sparse graphs due to empty local calibration neighborhoods.",
        "limitations": "(1) Guarantees are marginal (not conditional on node features/graph context or classes); class-conditional coverage can vary. (2) Assumes node-exchangeability or edge-exchangeability of the evolving graph sequence; real-world graphs may violate these symmetry assumptions, so guarantees may not hold without additional shift-handling. (3) Evaluation schedule must be non-adaptive: prediction time / which nodes to evaluate cannot be chosen after inspecting multiple prediction sets for the same node; adversarial or data-dependent selection can invalidate coverage. (4) Computational overhead: must recompute calibration scores and quantile at each prediction time (requires rerunning message passing on the updated graph); though reported overhead is small in their setup. (5) EdgeEx CP relies on degree-based weighting; behavior when degrees are noisy/hidden or in graphs with isolated nodes depends on their ‘active upon first incident edge’ convention.",
        "future_research_directions": "Extend beyond strict node/edge exchangeability by integrating formal ‘beyond exchangeability’ CP (e.g., Barber et al., 2023) to obtain valid bounds under realistic temporal/covariate shifts and quantify the coverage gap for graph drift. Develop methods for more informative/approximate conditional coverage on graphs (e.g., group-conditional, class-conditional, or neighborhood-conditional guarantees) while preserving validity. Study adaptive/online decision rules for when to predict that remain valid (e.g., stopping-time-safe conformal methods) to relax the non-adaptive evaluation constraint. Improve efficiency via graph-aware scores (DAPS/RAPS), learned score calibrators, or subgraph/ensemble conformalization, and analyze trade-offs under inductive updates. Generalize to other graph tasks (link prediction, edge classification, graph classification) and to richer dynamics (batch updates, node/edge deletions, feature drift). Investigate practical tests/diagnostics for exchangeability assumptions and robustness to violations, including degree-weight misspecification in EdgeEx CP and handling isolated/late-appearing nodes.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Continual Learning with Evolving Class Ontologies",
      "full_text": "Continual Learning with Evolving Class Ontologies Zhiqiu Lin1 Deepak Pathak1 Yu-Xiong Wang2 Deva Ramanan1,3∗ Shu Kong4∗ 1CMU 2UIUC 3Argo AI 4Texas A&M University Open-source code in webpage Abstract Lifelong learners must recognize concept vocabularies that evolve over time. A common yet underexplored scenario is learning with class labels that continually reﬁne/expand old classes. For example, humans learn to recognize dog before dog breeds. In practical settings, dataset versioning often introduces reﬁnement to ontologies, such as autonomous vehicle benchmarks that reﬁne a previous vehicle class into school-bus as autonomous operations expand to new cities. This paper formalizes a protocol for studying the problem of Learning with Evolving Class Ontology (LECO). LECO requires learning classiﬁers in distinct time periods (TPs); each TP introduces a new ontology of “ﬁne” labels that reﬁnes old ontologies of “coarse” labels (e.g., dog breeds that reﬁne the previous dog). LECO explores such questions as whether to annotate new data or relabel the old, how to exploit coarse labels, and whether to ﬁnetune the previous TP’s model or train from scratch. To answer these questions, we leverage insights from related problems such as class-incremental learning. We validate them under the LECO protocol through the lens of image classiﬁcation (on CIFAR and iNaturalist) and semantic segmentation (on Mapillary). Extensive experiments lead to some surprising conclusions; while the current status quo in the ﬁeld is to relabel existing datasets with new class ontologies (such as COCO-to-LVIS or Mapillary1.2-to-2.0), LECO demonstrates that a far better strategy is to annotate new data with the new ontology. However, this produces an aggregate dataset with inconsistent old-vs-new labels, complicating learning. To address this challenge, we adopt methods from semi-supervised and partial-label learning. We demonstrate that such strategies can surprisingly be made near-optimal, in the sense of approaching an “oracle” that learns on the aggregate dataset exhaustively labeled with the newest ontology. 1 Introduction Humans, as lifelong learners, learn to recognize the ontology of concepts which is being reﬁned and expanded over time. For example, we learn to recognize dog and then dog breeds (reﬁning the class dog). The class ontology often evolves from coarse to ﬁne concepts in the real open world and can sometimes introduce new classes. We call this Learning with Evolving Class Ontology (LECO). Motivation. LECO is common when building machine-learning systems in practice. One often trains and maintains machine-learned models with class labels (or a class ontology) that are reﬁned over time periods (TPs) (Fig. 2). Such ontology evolution can be caused by new requirements in applications, such as autonomous vehicles that expand operations to new cities. This is well demonstrated in many contemporary large-scale datasets that release updated versions by reﬁning and expanding classes, such as Mapillary [52, 55], Argoverse [12, 78], KITTI [4, 23] and iNaturalist [33–37, 69, 75]. For example, Mapillary V1.2 [55] deﬁned the road class, and Mapillary V2.0 [52] reﬁned it with ﬁne ∗Authors share senior authorship. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2210.04993v4  [cs.CV]  15 Dec 2022Figure 1: Learning with Evolving Class Ontology (LECO) requires training models in time periods (TPs) with new ontologies that reﬁne/expand the previous ones. Left: The Mapillary dataset, which was constructed to study semantic segmentation for autonomous vehicles, has updated versions from V1.2 [ 55] to V2.0 [52], which reﬁned previous labels with ﬁne-grained ones on the same data, e.g., V1.2’ssidewalk is split to V2.0’s sidewalk and driveway. The visual demonstration with semantic segmentation ground-truth blacks out unrelated classes for clarity. We explore LECO using this dataset. Right: To study LECO, we also repurpose the large-scale iNaturalist dataset [33–37, 69, 75] to simulate the coarse-to-ﬁne ontology evolution through the lens of image classiﬁcation. LECO asks the ﬁrst basic question: for the new TP that deﬁnes new ontology of classes, should one relabel the old data or label new data? Interestingly, both labeling protocols have been used in the community for large-scale datasets. Our study provides the (perhaps obvious) answer – one should always annotate new data with the new ontology rather than re-annotating the old. One reason is that the former produces more labeled data. But this produces an aggregate dataset with inconsistent old-vs-new labels. We make use of insights from semi-supervised learning and learning-with-partial-labels to learn from such heterogenous annotations, approaching the upper bound of learning from an oracle aggregate dataset with all new labels. labels including parking-aisle and road-shoulder; Argoverse V1.0 [12] deﬁned the bus class, and Argoverse V2.0 [78] reﬁned it with ﬁne classes including school-bus. Prior art. LECO requires learning new classes continually in TPs, similar to class-incremental learning (CIL) [18, 47, 74, 86]. They differ in important ways. First, CIL assumes brand-new labels that have no relations with the old [47, 49, 60], while LECO’s new labels are ﬁne-grained ones that expand the old / “coarse” labels (cf. Fig. 2). Interestingly, CIL can be seen as a special case of LECO by making use of a “catch-all”background class [41] (often denoted as void or unlabeled in existing datasets [14, 22, 49, 52, 55]), which can be reﬁned over time to include speciﬁc classes that were previously ignored. Second, CIL restricts itself to a small memory buffer for storing historical data to focus on information retention and catastrophic forgetting [10, 47]. However, LECO stores all historical data (since large-scale human annotations typically come at a much higher cost than that of the additional storage) and is concerned with the most-recent classes of interest. Most related to LECO are approaches that explore the relationship of new classes to old ones; Sariyildiz et al. [64] study concept similarity but remove common superclasses in their study, while Abdelsalam et al. [1] explore CIL with a goal to discover inter-class relations on data labeled (as opposed to assuming such relationships can be derived from a given taxonomic hierarchy). To explore LECO, we set up its benchmarking protocol (Section 3) and study extensive methods modiﬁed from those of related problems (Sections 4, 5, and 6). Technical insights. LECO studies learning strategies and answers questions as basic as whether one should label new data or relabel the old data2. Interestingly, both protocols have been used in the community for large-scale dataset versioning: Argoverse [ 12, 78] annotates new data, while Mapillary [52, 55] relabels its old data. Our experiments provide a deﬁnitive answer – one should always annotate new data with the new ontology rather than reannotating the old data. The simple reason is that the former produces more labeled data. However, this comes at a cost: the aggregate 2We assume the labeling cost is same for either case. One may think re-annotating old data is cheaper, because one does not need to curate new data and may be able to redesign an annotation interface that exploits the old annotations (e.g., show old vehicle labels and just ask for ﬁne-grained make-and-model label). But in practice, in order to prevent carry-over of annotation errors and to simplify the annotation interface, most benchmark curators tend to relabel from scratch, regardless of on old or new data. Examples of relabeling old-data from scratch include Mapillary V1.2 [55] →V2.0 [52], and COCO [48] →LVIS [27]. 2Benchmark #TPs #classes/TP #train/TP #test/TP CIFAR-LECO 2 20/100 10k 10k iNat-LECO 2 123/810 50k 4k Mapillary-LECO 2 66/116 2.5k 2k iNat-4TP-LECO 4 123/339/729/810 25k 4k Table 1: LECO benchmarks use CIFAR100 [42], iNaturalist [69, 75], and Mapillary [ 52, 55]. We also vary the number of training data in exper- iments; as conclusions hold across settings, we report these results in Table 9, 10, 11. dataset is now labeled with inconsistent old-vs-new annotations, complicating learning. We show that joint-training [47] on both new and old ontologies, when combined with semi-supervised learning (SSL), can be remarkably effective. Concretely, we generate pseudo labels for the new ontology on the old data. But because pseudo labels are noisy [ 44, 58, 68, 79] and potentially biased [ 77], we make use of the coarse labels from the old-ontology as “coarse supervision”, or the coarse-ﬁne label relationships [26, 45, 61, 73], to reconcile conﬂicts between the pseudo ﬁne labels and the true coarse labels (Section 6). There is another natural question: should one ﬁnetune the previous TP’s model or train from scratch? One may think the former has no beneﬁt (because it accesses the same training data) and might suffer from local minima. Suprisingly, we ﬁnd ﬁnetuning actually works much better, echoing curriculum learning [ 5, 21, 63]. Perhaps most suprisingly, we demonstrate that such strategies are near optimal, approaching an “upperbound” that trains onall available data re-annotated with the new ontology. Salient results. To study LECO, we repurpose three datasets: CIFAR100, iNaturlist, and Mapillary (Table 1). The latter two large-scale datasets have long-tailed distribution of class labels; particularly, Mapillary’s new ontology does not have strictly surjective mapping to the old, because it relabeled the same data from scratch with new ontology. Our results on these datasets lead to consistent technical insights summarized above. We preview the results on the iNaturalist-LECO setup (which simulates the large-scale long-tailed scenario): (1) ﬁnetuning the previous TP’s model outperforms training from scratch on data labeled with new ontology: 73.64% vs. 65.40% in accuracy; (2) jointly training on both old and new data signiﬁcantly boosts accuracy to 82.98%; (3) taking advantage of the relationship between old and new labels (via Learning-with-Partial-Labels and SSL with pseudo-label reﬁnement) further improves to 84.34%, effectively reaching an “upperbound” 84.51%! Contributions. We make three major contributions. First, we motivate the problem LECO and deﬁne its benchmarking protocol. Second, we extensively study approaches to LECO by modifying methods from related problems, including semi-supervised learning, class-incremental learning, and learning with partial labels. Third, we draw consistent conclusions and technical insights, as described above. 2 Related Work Class-incremental learning (CIL) [18, 47, 74, 86] and LECO both require learning classiﬁers for new classes over distinct time periods (TPs), but they have important differences. First, the typical CIL setup assumes new labels have no relations with the old [1, 47, 49], while LECO’s new labels reﬁne or expand the old / “coarse” labels (cf. Fig. 2). Second, CIL purposely sets a small memory buffer (for storing labeled data) and evaluates accuracy over both old and new classes with the emphasis on information retention or catastrophic forgetting [10, 40, 47, 83]. However, LECO allows all (history) labeled data to be stored and highlight the difﬁculty of learning new classes which are ﬁne/subclasses of the old ones. Further, we note that many real-world applications store history data (and should not limit a buffer size to save data) for privacy-related consideration (e.g., medical data records) and as forensic evidence (e.g., videos from surveillance cameras). Therefore, to approach LECO, we apply CIL methods that do not restrict buffer size, which usually serve as upper bounds in CIL [ 17]. In this work, we repurpose “upper bound” CIL methods for LECO including ﬁnetuning [3, 80], joint training [11, 47, 50, 51], as well as the “lower bound” methods by freezing the backbone [3, 20, 66]. Semi-supervised learning (SSL) learns over both labeled and unlabeled data. State-of-the-art SSL methods follow a self-training paradigm [54, 65, 79] that uses a model trained over labeled data to pseudo-label the unlabeled samples, which are then used together with labeled data for training. For example, Lee et al. [44] use conﬁdent predictions as target labels for unlabeled data, and Rizve et al. [62] further incorporate low-probability predictions as negative pseudo labels. Some others improve SSL using self-supervised learning techniques [ 24, 84], which force predictions to be similar for different augmentations of the same data [6, 7, 68, 79, 85]. We leverage insights of SSL to approach LECO, e.g., pseudo labeling the old data. As pseudo labels are often biased [77], we further exploit the old-ontology to reject or improve the inconsistent pseudo labels, yielding better performance. 3Learning with partial labels (LPL) tackles the case when some examples are fully labeled while others are only partially labeled [9, 15, 26, 53, 56, 88]. In the area of ﬁne-grained recognition, partial labels can be coarse superclasses which annotate some training data [ 31, 45, 61, 73]. In a TP of LECO, the old data from previous TPs can be used as partially-labeled examples as they contain only coarse labels. Therefore, to approach LECO, we explore state-of-the-art methods [70] in this line of work. However, it is important to note that ontology evolution in LECO can be more than splitting old classes, e.g., the new class special-subject can be a result of merging multiple classes child and police-officer. Indeed, we ﬁnd it happens quite often in the ontology evolution from the Mapillary’s versioning from V1.2 [55] to V2.0 [52]. In this case, the LPL method does not show better results than the simple joint training approach (31.04 vs. 31.05 on Mapillary in Table 4). 3 Problem Setup of Learning with Evolving Class Ontology (LECO) Notations. Among T time periods (TPs), TPt has data distributions3 Dt = X ×Yt, where X is the input and Yt is the label set. The evolution of class ontology is ideally modeled as a tree structure: the ith class yt i ∈Yt is a node that has a unique parent yt−1 j ∈Yt−1, which can be split into multiple ﬁne classes in TPt. Therefore, |Yt|>|Yt−1|. TPt concerns classiﬁcation w.r.t label set Yt. Benchmarks. We deﬁne LECO benchmarks using three datasets: CIFAR100 [42], iNaturalist [69, 75], and Mapillary [52, 55] (Table 1). CIFAR100 is released under the MIT license, and iNaturalist and Mapillary are publicly available for non-commercial research and educational purposes. CIFAR100 and Mapillary contain classes related to person and potentially have fairness and privacy concerns, hence we cautiously proceed our research and release our code under the MIT License without re-distributing the data. The iNaturalist and Mapillary are large-scale and have long-tailed class distributions (refer to the Table 4 and 5 for class distributions). For each benchmark, we sample data from the corresponding dataset to construct time periods (TPs), e.g., TP0 and TP1 deﬁne their own ontologies of class labels Y0 and Y1, respectively. For CIFAR-LECO, we use the two-level hierarchy offered by the original CIFAR100 dataset [42]: using the 20 superclasses as the ontology in TP0, each of them is split into 5 subclasses as new ontology in TP1. For iNat-LECO, we used the most recent version of Semi-iNat-2021 [71] that comes with rich taxonomic labels at seven-level hierarchy. To construct two TPs, we choose the third level (i.e., “order”) with 123 categories for TP0, and the seventh (“species”) with 810 categories for TP1. We further construct four TPs with each having an ontology out of four at distinct taxa levels [“order”, “family”, “genus”, “species”]. For Mapillary-LECO, we use its ontology of V1.2 [55] (2017) in TP0, and that of V2.0 [52] (2021) in TP1. It is worth noting that, as a real-world LECO example, Mapillary includes a catch-all background class (aka void) in V1.2, which was split into meaningful ﬁne classes in V2.0, such as temporary-barrier, traffic-island and void-ground. Moreover, in each TP of benchmark CIFAR-LECO and iNat-LECO, we randomly sample 20% data as validation set for hyperparameter tuning and model select. We use their ofﬁcial valsets as our test-sets for benchmarking. In Mapillary-LECO, we do not use a valset but instead use the default hyperparameters reported in [72], tuned to optimize another related dataset Cityscapes [14]. We ﬁnd it unreasonably computational demanding to tune on large-scale semantic segmentation datasets. Table 1 summarizes our benchmarks’ statistics. Remark: beyond class reﬁning in ontology evolution. In practice, ontology evolution also contains the case of class merging, as well as class renaming (which is trivial to address). The Mapillary’s versioning clearly demonstrates this: 10 of 66 classes of its V1.2 were reﬁned into new classes, resulting into 116 classes in total of its V2.0; objects of the same old class were either merged into a new class or assigned with new ﬁne classes. Because of these non-trivial cases under the Mapillary-LECO benchmark, state-of-the-art methods of learning with partial labels are less effective but our other solutions work well. Remark: number of TPs and buffer size. For most experiments, we set two TPs. One may think more TPs are required for experiments because CIL literature does so [18, 47, 74, 86]. Recall that CIL emphasizes catastrophic forgetting issue caused by using limited buffer to save history data, so using more TPs for more classes and limited buffer helps exacerbate this issue. Differently, LECO emphasizes the difﬁculty of learning new classes that are subclasses of the old ones, and does not 3The data distribution may not be necessarily static where LECO can still happen, as discussed in Section 1. Our work sets a static data distribution to simplify the study of LECO. 4limit a buffer to store history data. Therefore, using two TPs are sufﬁcient to show the difﬁculty. Even so, we have experiments that set four TPs, which demosntrate consistent conclusions drawn from those using two TPs. Moreover, even with unlimited buffer, CIL methods still work poorly on LECO benchmarks (cf. “TrainScratch” [59] and “FreezePrev” [47] in Table 2). Metric. Our benchmarks study LECO through the lens of image classiﬁcation (on CIFAR-LECO and iNat-LECO) and semantic segmentation (on Mapillary-LECO). Therefore, we evaluate methods using mean accuracy (mAcc) averaged over per-class accuracies, and mean intersection-over-union (mIoU) averaged over classes, respectively. The metrics treat all classes equally, avoiding the bias towards common classes due to the natural long-tailed class distribution (cf. iNaturalist [ 69, 75] and Mapillary [ 55]) (cf. class distributions in the appendix). We do not use speciﬁc algorithms to address the long-tailed recognition problem but instead adopt the recent simple technique to improve performance by tuning weight decay [2]. We run each method ﬁve times using random seeds on CIFAR-LECO and iNat-LECO, and report their averaged mAcc with standard deviations. For Mapillary, due to exoribitant training cost, we perform one run to benchmark methods. 4 Baseline Approaches to LECO We ﬁrst repurpose CIL and continual learning methods as baselines to approach LECO, as both require learning classiﬁers for new classes in TPs. We start with preliminary backgrounds. Preliminaries. Following prior art [ 39, 47], we train convolutional neural networks (CNNs) and treat a network as a feature extractor f (parametrized by θf) plus a classiﬁer g(parameterized by θg). The feature extractor f consists of all the layers below the penultimate layer of ResNet [28], and the classiﬁer gis the linear classiﬁer followed by softmax normalization. Speciﬁcally, in TP t, an input image xis fed into f to obtain its feature representation z= f(x; θf). The feature zis then fed into gfor the softmax probabilities, q= g(z; θg), w.r.t classes in Yt. We train CNNs by minimizing the Cross Entropy (CE) loss using mini-batch SGD. At TP t, to construct a training batch Bt, we randomly sample Kexamples, i.e., Bt = {(x1,yt 1),··· ,(xK,yt K)}. The CE loss on Bt is L(Bt) =− ∑ (xk,yt k)∈Bt H(yt k,g(f(xk))) (1) where H(p,q) =−∑ cp(c)log(q(c)) is the entropy between two probability vectors. Similary, for semantic segmentation, we use the same CE loss 1 at pixel level, along with the recent architecture HRNet with OCR module [72, 76, 81] (refer to appendix C for details). Annotation Strategies . Closely related to LECO is the problem of class-incremental learning (CIL) or more generally, continual learning (CL) [47, 59, 60, 83, 86]. However, unlike typical CL benchmarks, LECO does not artiﬁcially limit the buffer size for storing history samples. Indeed, the expense of hard drive buffer is less of a problem compared to the high cost of data annotation. Therefore, LECO embraces all the history labeled data, regardless of being annoated using old or new ontologies. This leads to a fundamental question: whether to annotate new data or re-label the old, given a ﬁxed labeling budget N at each TP (cf. Fig. 2)? • (LabelNew) Sample and annotate N new examples using new ontology, which trivially infers the old-ontology labels. We further have the history data as labeled with old ontology. • (RelabelOld) Re-annotate theNhistory data using new ontology. However, theseNexamples are all available data although they have both old- and new-ontology labels. The answer is perhaps obvious – LabelNew – because it produces more labeled data. Furthermore, LabelNew allows one to exploit the old data to boost performance (Section 5). With proper techniques, this signiﬁcantly reduces the performance gap with a model trained over both old and new data assumably annotated with new-ontology, termed as AllFine short for “supervised learning with all ﬁne labels”. Training Strategies. We consider CL baseline methods as LECO baselines. When new tasks (i.e., classes of Yt) arrive in TP t, it is desirable to transfer model weights trained for Yt−1 in TPt−1 [47, 60]. To do so, we initialize weights θft of the feature extractor ft with θft−1 trained in TPt−1, i.e., θft := θft−1 . Then we can • (FreezePrev) Freeze ft to extract features, and learn a classiﬁer gt over them [47]. 5Table 2: Results of baseline methods for LECO (more results in the Table 9). For all our experiments, we run each method ﬁve times and report the averaged mean per-class accuracy/intersection-over-union (mAcc/mIoU in %) with standard deviations when available. For each benchmark, we bold the best mAcc/mIoU including ties that fall within its std. FinetunePrev is the best learning strategy that outperforms TrainScratch (cf. 73.64% vs. 65.69% with LabelNew on iNat-LECO). This implies that the coarse-to-ﬁne evolved ontology serves as a good learning curriculum. FreezePrev underperforms TrainScratch (cf. 50.82% vs. 65.69% with LabelNew on iNat-LECO), conﬁrming the difﬁculty of learning new classes that reﬁne the old. LabelNew and RelabelOld achieve similar performance with the former using only new data but not the old altogether. If using both old and new data, we boost performance as shown in Table 3. The conclusions hold for varied number of training images and generalize to the Mapillary experiments, yet all methods fall short to the AllFine (which trains on all data assumed to be labeled with new ontology). Benchmark #images / TP TP0 mAcc/mIoU TP1 Strategy TP1 mAcc/mIoU LabelNew RelabelOld AllFine CIFAR-LECO 10000 77.78±0.27 FinetunePrev65.83±0.53 65.30 ±0.32 71.30±0.41 FreezePrev 52.64±0.50 52 .60±0.52 53 .79±0.43 TrainScratch 65.40±0.53 64.98±0.34 71 .43±0.24 iNat-LECO 50000 64.21±1.61 FinetunePrev73.64±0.46 71.26±0.54 84 .51±0.66 FreezePrev 50.82±0.68 54 .60±0.49 54 .08±0.31 TrainScratch 65.69±0.46 65 .78±0.56 73 .10±0.80 Mapillary-LECO 2500 37.01 FinetunePrev 30.39 29.05 31 .73 TrainScratch 27.24 27 .21 27 .73 • (FinetunePrev) Finetune ft and learn a classiﬁer gt altogether [47]. • (TrainScratch) Train ft from random weights [59]. Results. We experiment with different combinations of data annotation and training strategies in Table 2. We adopt standard training techniques including SGD with momentum, cosine annealing learning rate schedule, weight decay and RandAugment [ 16]. We ensure the maximum training epochs (2000/300/800 on CIFAR/iNat/Mapillary respectively) to be large enough for convergence. See Table 2-caption for salient conclusions. 5 Improving LECO with the Old-Ontology Data The previous section shows that FinetunePrev is the best training strategy, and LabelNew and RelabelOld achieve similar performance with the former being not exploiting the old data. In this section, we stick to FinetunePrev and LabelNew and study how to exploit the old data to improve LECO. To do so, we repurpose methods from semi-supervised learning and incremental learning. Notation: We refer to the new-ontology (ﬁne) labeled samples at TP t as St ⊂X ×Yt, and old-ontology (coarse) labeled samples accumulated from previous TPs as S1:t−1 = S1 + ··· + St−1. 5.1 Exploiting the Old-Ontology Data via Semi-Supervised Learning (SSL) State-of-the-art SSL methods aim to increase the amount of labeled data by pseudo-labeling the unlabeled examples [25, 70]. Therefore, a natural thought to utilize the old examples is toignore their old-ontology labels and use SSL methods. Below, we describe several representative SSL methods which are developed in the context of image classiﬁcation. Each input sample x, if not stated otherwise, undergoes a strong augmentation (RandAugment [16] for classiﬁcation and HRNet [72] augmentation for segmentation) denoted as A(x) before fed into f. For all SSL algorithms, we construct a batch BK of K samples from St and a batch ˆBM of M samples from S1:t−1. For brevity, we hereafter ignore the superscript (TP index). On new-ontology labeled samples, we use Eq. (1) to obtain L(BK), and sum it with an extra SSL loss on ˆBM, denoted as LSSL( ˆBM). We have four SSL losses below. Following [70], we use “Self-Training” to refer to a teacher-student distillation procedure [29]. Note that we adopt the FinetunePrev strategy and so both teacher and student models are initialized from the previous TP’s weights. We ﬁrst have two types of Self-Training: 6• ST-Hard (Self-Training with one-hot labels): We ﬁrst train a teacher model ( fteacher and gteacher) on the training set St, then use the teacher to “pseudo-label” history data inS1:t−1. Essentially, for a (xm,ym) ∈S1:t−1, the old-ontology label ym is ignored and a one-hot hard label is produced as qh = arg maxgteacher(fteacher(A(xm))). Finally, we train a student model (fstudent and gstudent) with CE loss: LSSL( ˆBM) = ∑ (xm,·)∈ˆBM H(qh,gstudent(fstudent(A(xm))) (2) • ST-Soft (Self-Training with soft labels): The teacher is trained in same way as ST- Hard, but the difference is that pseudo-labels are softmax probability vectors qs = gteacher(fteacher(A(xm))) instead. The ﬁnal loss is the KL divergence [58, 70]: LSSL( ˆBM) = ∑ (xm,·)∈ˆBM H(qs,gstudent(fstudent(A(xm))) (3) In contrast to the above SSL methods that train separate teacher and student models, the following two train a single model along with the pseudo-labeling technique. • PL (Pseudo-Labeling) [44] pseudo-labels image xm as q= g(f(A(xm))) using the current model. It also ﬁlters out pseudo-labels that have probabilities less than 0.95. LSSL( ˆBM) = ∑ (xm,·)∈ˆBM 1 [max q >0.95]H(q,g(f(A(xm))) (4) • FixMatch [68] imposes a consistency regularization into PL by enforcing the model to produce the same output across weak augmentation (a(xm), usually standard random ﬂipping and cropping) and strong augmentation (A(xm), in our case is weak augmentation followed by RandAugment [16]). The pseudo-label is produced by weakly augmented version qw = g(f(a(xm))). The ﬁnal loss is: LSSL( ˆBM) = ∑ (xm,·)∈ˆBM 1 [max qw >0.95]H(qw,g(f(A(xm))) (5) While the above techniques are developed in the context of image classiﬁcation, they appear to be less effective for semantic segmentation (on the Mapillary dataset), e.g., ST-Soft requires extremely large storage to save per-pixel soft labels, FixMatch requires two large models designed for semantic segmentation. Therefore, in Mapillary-LECO, we adopt the ST-Hard which is computationally friendly given our compute resource (Nvidia GTX-3090 Ti with 24GB RAM). 5.2 Exploiting the Old-Ontology Data via Joint Training The above SSL approaches discard the old-ontology labels, which could be a coarse supervision in training. A simple approach to leverage this coarse supervision is Joint Training, a classic incremental learning method [18, 47] that trains jointly on both new and old tasks. Interestingly,Joint Training is usually the upper bound performance in traditional incremental learning benchmarks [47]. Therefore, we perform Joint Trainingto learn a shared feature extractorfand independent classiﬁers (g1,··· ,gt) w.r.t both new and old ontologies. We deﬁne the following loss at TPt using old-ontology data from TP1 to TPt−1 : Lold( ˆBM) = ∑ t′∈{1,···,t−1} ∑ (xm,yt′ m)∈ˆBt′ M H(yt′ m,gt′ (f(A(xm))) (6) where yt′ m is the coarse label of xm at TPt′ and ˆBt′ M is the subset of ˆBM with label yt′ m available. Importantly, because new-ontology ﬁne labels yt i ∈Yt reﬁne old-ontology coarse labels, e.g., yt−1 j ∈Yt−1, we can trivially infer the coarse label for any ﬁne label . Therefore, we also apply Lold for ﬁne-grained samples in BK, i.e., Lold(BK). We have the ﬁnal loss for Joint Training as LJoint = Lold(BK) +Lold( ˆBM). 5.3 Results Table 3 compares the above-mentioned SSL and Joint Training methods. In training, we sample the same amount of old data and new data in a batch, i.e., |ˆBM|= |BK|(64 / 30 / 4 for CIFAR / iNat / Mapillary). We assign equal weight to LSSL, LJoint, and L. See Table 3-caption for conclusions. 7Table 3: Results of SSL and Joint Training methods. Following the notation of Table 2, we study LabelNew annotation strategy that make use of algorithms for learning from old exampleswithout exploiting the relationship between old-vs-new labels. Recall that AllFine uses Lonly to train on all the data labeled with new ontology (doubling annotation cost). Both LJoint and the best LSSL (ST-Soft) outperform the baseline which uses loss L only (i.e., LabelNew + FinetunePrev in Table 2). ST-Soft yields the most performance gain. Using all the three losses together consistently improves mAcc / mIoU for all the benchmarks, conﬁrming the beneﬁt of exploiting the old examples, e.g., on iNat-LECO, this boosts the performance to 83.6%, approaching the AllFine (84.5%)! Benchmark #images / TP SSL Alg TP1 mAcc/mIoU Lonly +LSSL +LJoint +LJoint +LSSL AllFine CIFAR-LECO 10000 ST-Hard 65.83±0.27 67.11±0.34 68.74±0.21 67.97±0.37 71.30±0.41ST-Soft 69.86±0.30 69.77 ±0.71 PL 65.86±0.14 68 .43±0.40 FixMatch 67.13±0.36 69 .03±0.43 iNat-LECO 50000 ST-Hard 73.64±0.46 75.23±0.14 82.98±0.33 78.23±0.38 84.51±0.66ST-Soft 79.64±0.41 83.61±0.39 PL 73.56±0.09 82 .87±0.51 FixMatch 74.57±0.54 83 .00±0.33 Mapillary-LECO 2500 ST-Hard 30.39 30.06 31.05 31.09 31.73 6 Improving LECO with Taxonomic Hierarchy While the SSL approaches exploit old samples and Joint Training further utilizes their old-ontology labels, none of them fully utilize the given taxonomic relationships between old and new ontologies. The fact that each ﬁne-grained labelyt i ∈Yt reﬁnes a coarse label yt−1 j ∈Yt−1 suggests a chance to improve LECO by exploiting such taxonomic relationship between the old and new classes [58, 69]. 6.1 Incorporating Taxonomic Hierarchy via Learning-with-Partial-Labels (LPL) Joint Training trains separate classiﬁers for each TP. This is suboptimal because we can simply marginalize ﬁne-classes’ probabilities for coarse-classes’ probabilities [ 19]. This technique has been used in other works [32, 69] and is commonly called Learning-with-Partial-Labels (LPL) [32]. Concretely, the relationship between classes in Yt and Yt′ with t > t′can be summarized by an edge matrix Et→t′ with shape |Yt|×|Yt′ |. Et→t′[i,j] = 1when an edge yt i →yt′ j exists in the taxonomy tree, i.e., yt′ j is the superclass of yt i; otherwise Et→t′[i,j] = 0. Therefore, to obtain prediction probability of a coarse-grained label yt′ for a sample x, we sum up the probabilities in gt(ft(x)) that correspond to ﬁne labels of yt′ , which can be efﬁciently done by a matrix vector multiplication: gt(ft(x)) ·Et→t′. Formally, we deﬁne a loss over a sampled batch ˆBM ⊂S1:t−1 as: LOldLPL( ˆBM) = ∑ t′∈{1,···,t−1} ∑ (xm,yt′ m)∈ˆBt′ M H(yt′ m,gt(ft(A(xm))) ·Et→t′) (7) We apply this loss on the ﬁne-labeled data and deﬁne the LPL Loss LLPL = LoldLPL(BK) + LoldLPL( ˆBM). 6.2 Incorporating Taxonomic Hierarchy via Pseudo-label Reﬁnement Another strategy to incorporate the taxonomic hierarchy is to exploit coarse labels in the pseudo- labeling process, which has been explored in a different task (semi-supervised few-shot learning) [58]. Prior art shows that the accuracy of pseudo labels is positively correlated with SSL performance [68]. Therefore, at TPt, in pseudo-labeling an old sample xm, we use its coarse label yt′ m (t′<t) to clean up inconsistent pseudo labels. In particular, we consider 1. Filtering: we ﬁlter out (reject) pseudo-labels that do not align with the ground-truth coarse labels, i.e., rejecting pseudo-label q= gt(ft(x)) if Et→t′[arg maxcqc,yt′ m] = 0. 2. Conditioning: we clean up the pseudo-label qby zeroing out all scores unaligned with the true coarse label: q[c] :=q[c] ∗Et→t′[c,yt′ m]. We then renormalize q[c] :=q[c]/∑ iq[i]. 8Table 4: Results of exploiting taxonomic hierarchy (accuracy in %). We copy the best results in Table 3 (all obtained by +LJoint +LSSL) for reference. Adding supervision via taxonomic hierarchy is effective: (1) simply using LLPL already matches the previously best performance (by by +LJoint +LSSL); (2) adding SSL loss along with pseudo-label reﬁnement via either Filtering or Conditioning further bridges the gap to AllFine. As a result, for example, on iNat-LECO, the best strategy (+LLPL + LSSL/Cond with ST-Hard) is only 0.2% lower than the AllFine! Benchmark #images / TP SSL Alg TP1 mAcc +LJoint +LSSL +LLPL +LLPL +LSSL/Filter +LLPL +LSSL/Cond AllFine CIFAR-LECO 10000 ST-Hard 69.77±0.71 69.31±0.30 69.90±0.29 70.42±0.23 71.30±0.41ST-Soft 70.10±0.12 70 .02±0.63 PL 69.20±0.20 69 .60±0.26 FixMatch 69.61±0.17 69 .85±0.46 iNat-LECO 50000 ST-Hard 83.61±0.39 83.62±0.21 84.23±0.61 84.34 ±0.25 84.51±0.66ST-Soft 84.12±0.31 83.76±0.40 PL 83.72±0.22 84.16±0.19 FixMatch 83.91±0.39 83 .95±0.40 Mapillary-LECO 2500 ST-Hard 31.09 31 .04 31.41 31.23 31.73 Table 5: Baseline results on iNat-4TP-LECO (accuracy in %). FinetunePrev signiﬁcantly and consistently outperforms TrainScratch, suggesting representation trained for coarse-grained classiﬁcation can serve as powerful initialization for ﬁnetning in later TPs. Furthermore, LabelNew and RelabelOld achieve almost the same performance across all TPs, but we will show in Table 6 that once exploiting the old-ontology data and labels, we can signiﬁcantly improve the performance, approaching the “upperbound”AllFine. Benchmark #images / TP TP0 mAcc Training Strategy Annotation TP1 mAcc TP2 mAcc TP3 mAcc iNat-4TP-LECO 25000 46.6±1.2 TrainScratch LabelNew 50.9±0.6 49 .4±0.6 48 .4±0.6 RelabelOld50.1±1.1 49 .7±0.4 47 .9±0.6 AllFine 62.7±0.5 63 .8±0.3 58 .0±0.2 FinetuneNew LabelNew 55.1±0.7 61.7±0.8 63.2 ±0.4 RelabelOld55.6±0.9 61.7 ±0.9 63.1±0.4 AllFine 67.7±0.6 78 .8±0.4 84 .6±0.3 6.3 Results Table 4 shows the results of LPL Loss and SSL strategies with pseudo-label reﬁnement. We can see that simply adding the LPL Loss LLPL matches the SOTA (achieved by naive SSL andJoint Training strategies in Table 3). Also, adding SSL loss along with pseudo-label reﬁnement via either Filtering or Conditioning further bridges the gap to AllFine. For example, on iNat-LECO, the best strategy (+LLPL + LSSL/Cond with ST-Hard) is only 0.2% lower than the AllFine (84.3% v.s. 84.5%). We acknowledge that the AllFine can be improved further if using LPL loss as well [8] but we use the current version just as a reference of performance. We also tried combining LSSL/Filter or LSSL/Cond with the Joint Training, and observe similar improvements upon LSSL + LJoint (results in Table 10 and 11). 7 Further Experiments with More Time Periods We further validate our proposed approahces with more TPs. In this experiment, we construct a benchmark iNat-4TP-LECO using iNaturalist by setting four TPs, each having an ontology at distinct taxa levels: “order” (123 classes), “family” (339 classes), “genus” (729 classes), “species” (810 classes). We show in Table 5 thatFinetuneNew outperforms TrainScratch from TP1 to TP3. Furthermore, we study LabelNew annotation strategy with FinetunePrev training strategy (as in Table 3 and 4) for iNat-4TP-LECO, and report results in Table 6. In summary, all the proposed solutions generalize to more than two TPs, and simple techniques that utilize old-ontology labels (such as LJoint) and label hierarchy (such as LLPL) achieve near-optimal performance compared to AllFine “upperbound”. 9Table 6: Results of iNat-4TP-LECO with FinetuneNew training strategy and LabelNew annotation strat- egy (accuracy in %). We select two best performing SSL methods on iNat-LECO in Table 3, FixMatch and ST-Soft, for benchmarking. Clearly, all solutions proposed in the paper achieve consistent gains over baseline methods (Table 5). First, leveraging old-ontology data via SSL (e.g., +LST-Soft) can improve the classiﬁca- tion accuracy from 63.2% to 67.6% at TP3. Also, utilizing the old-ontology labels via simple joint training can boost performance to 83.0%. Lastly, incorporating taxonomic hierarchy through LPL and SSL (e.g., +LLPL + LST-Soft/Filter) further improves to 85.3%. Benchmark #images / TP Combination of losses TP 1 mAcc TP 2 mAcc TP 3 mAcc iNat-4TP-LECO 25000 AllFine 67.7 ±0.6 78 .8 ±0.4 84 .6 ±0.3 Lonly 55.1 ±0.7 61 .7 ±0.8 63 .2 ±0.4 +LFixMatch 56.7 ±1.0 61 .4 ±1.0 65 .0 ±0.9 +LST-Soft 59.6 ±1.2 67 .8 ±1.2 67 .6 ±1.3 +LJoint 64.7 ±0.6 74 .3 ±0.9 82 .4 ±0.9 +LJoint + LFixMatch 64.6 ±0.8 75 .5 ±0.7 82 .8 ±0.7 +LJoint + LST-Soft 64.4 ±1.4 76 .1 ±1.5 83 .0 ±1.5 +LLPL 67.5 ±0.9 76 .7 ±0.9 83 .6 ±1.0 +LLPL + LFixMatch/Filter 67.7 ±0.8 77.0 ±0.9 85.3 ±0.6 +LLPL + LFixMatch/Cond 67.5 ±0.7 71 .0 ±0.8 78 .8 ±0.6 +LLPL + LST-Soft/Filter 67.7 ±1.1 77.3 ±1.1 85.1 ±1.2 +LLPL + LST-Soft/Cond 66.3 ±1.0 72 .7 ±1.3 79 .4 ±1.2 8 Discussions and Conclusions We introduce the problem Learning with Evolving Class Ontology (LECO) motivated by the fact that lifelong learners must recognize concept vocabularies that evolve over time. To explore LECO, we set up its benchmarking protocol and study various approaches by repurposing methods of related problems. Extensive experiments lead to several surprising conclusions. First, we ﬁnd the status-quo for dataset versioning, where old data is relabeled, is not the most effective strategy. One should label new data with new labels, but the mixture of old and new labels is effective only by exploiting semi-supervised methods that exploit the relationship between old and new labels. Surprisingly, such strategies can approach an upperbound of learning from an oracle aggregate dataset assumably annotated with new labels. Societal Impacts. Studying LECO has various positive impacts as it reﬂects open-world development of machine-learning solutions. For example, LECO emphasizes lifelong learning and maintainance of machine-learning systems, which arise in inter-disciplinary research (where a bio-image analysis system needs to learn from a reﬁned taxonomy, cf. iNaturalist), and autonomous vehicles (where critical ﬁne-grained classes appear as operations expand to more cities). We are not aware of negative societal impacts of LECO over those that arise in traditional image classiﬁcation. Future work to address limitations. We notice several limitations and anticipate future work to address them. Firstly, the assumption of constant annotation cost between the old and new data needs to be examined, but this likely requires the design of novel interfaces for (re)annotation. Secondly, our work did not consider the cost for obtaining new data, which might include data mining and cleaning. Yet, in many applications, data is already being continuously collected (e.g., by autonomous ﬂeets and surveillance cameras). This also suggests that LECO incorporate unlabeled data. Moreover, perhaps surprisingly, ﬁnetuning previous TP’s model signiﬁcantly outperforms training from scratch. This suggests that the coarse-to-ﬁne ontology evolution serve as a good curriculum of training, a topic widely studied in cognition, behavior and psychology [43, 57, 67]. Future work should investigate this, and favorably study efﬁciently ﬁnetuning algorithms [ 13, 30, 38, 46, 87]. Lastly, we explore LECO on well-known benchmarks with static data distributions over time periods; in the future, we would like to embrace temporal shifts in the data distribution as well [49]. Acknowledgments and Disclosure of Funding This research was supported by CMU Argo AI Center for Autonomous Vehicle Research. 10Appendix A: In LECO, label new data or relabel the old? In LECO, the foremost question to answer is whether to label new data or relabel the old (Fig. 2- right). As discussed in the main paper, both labeling protocols have been used in the community. For example, in the context of autonomous driving research, Argoverse labeled new data with new ontology in its updated version (from V1.0 [12] to V2.0 [78]), whereas Mapillary related the old data from its V1.2 [55] to V2.0 [52]. Our extensive experiments convincingly demonstrate that a better strategy is to label new data, simply because doing so provides more (labeled) data. Data acquisition (for new data) might be costly. However, many real-world applications acquire data continuously regardless of the cost. For example, autonomous vehicle ﬂeets collect data continuously, so do surveillance cameras that record video frames. Therefore, labeling such new data is reasonable in the real world. Figure 2: Left: Learning with Evolving Class Ontology (LECO) requires training models in time periods (TPs) that reﬁne old ontologies in a coarse-to-ﬁne fashion. This leads to a basic question: for the next TP, should one relabel the old data or label new data? Interestingly, both labeling protocols have been used in the community for large-scale datasets. Right: Our extensive experiments provide the (perhaps obvious) answer – one should always annotate new data with the new ontology rather than re-annotating the old. One reason is that the former produces more labeled data. Following this labeling protocol and to address LECO, we leverage insights from semi-supervised learning and learning-with-partial-labels to learn from such heterogenous annotations, approaching the upper bound of learning from an oracle aggregate dataset with all new labels. B: Rationale behind the selection of levels on iNaturalist We repurpose the iNaturalist dataset to set up a LECO benchmarking protocol. The dataset has seven levels of taxa, allowing to use such as superclasses in LECO’s time periods. To determine the levels to use, we have two principles aiming to have a clean and challenging setup to study LECO. First, we think it is good to have more ﬁne-grained classes in TP1, hence we choose the most ﬁne-grained level “species” in TP1. Second, we think all coarse-classes should be split later to emphasize the difﬁculty of learning with class-evolution, and TP0’s classiﬁcation task should be challenging as well with more classes. Therefore, we choose the “order” level which has 123 taxa. As reference, the original iNat dataset has seven levels: kingdom (3 taxa), phylum (8), class (29), order (123), family (339), genus (729), and species (810). The long-tailed class distributions in the two TPs are depicted in Fig. 4. C: Experiments on Mapillary V1.2 →V2.0 As we brieﬂy discussed in the main paper, Mapillary is a real-world example where a large dataset evolved over time from V1.2 [55] to V2.0 [52]. We include additional experiments on this real-world dataset versioning scenario. Mapillary is a rich and diverse street-level imagery dataset that was 11Figure 3: Visual illustration of LECO strategies. Left: LSSL generates pseudo new-ontology/ﬁne labels for old-ontology samples via semi-supervised learning (SSL). However, this ignores old-ontology/coarse labels which might serve as coarse-level supervision. We therefore advocate for LSSL/Filter that ﬁlters out pseudo- labels with wrong coarse labels, or LSSL/Cond that conditions leave’s probabilities based on the ground-truth parent/coarse class. Middle: LJoint utilizes the old-ontology labels that come with the samples; to reconcile for different labels at different TPs, it trains a model with multiple classiﬁcation heads. Right: LLPL (learning- with-partial-labels) further utilizes the taxonomic hierarchy by exploiting the fact that newly added classes in LECO are reﬁned from old classes. It does so by marginalizing leaves’ probabilties for their parent classes. TP0 Distribution (123 classes) TP 1 Distribution (810 classes) iNat-LECOPer-Class Image Distributions (Y-axis is log-scaled) Figure 4: iNat-LECO Per-Class Distributions. We plot distributions of training images for iNat-LECO in log-scale. The x-axis are classes sorted by the number of images. The y-axis shows the number of images per class. collected for semantic segmentation research in the context of autonomous driving. We repurpose this dataset for setting a LECO benchmark. Brieﬂy, we use the ontology of Mapillary V1.2 in time period 1 (TP1), and V2.0 in TP2. TP1 has 66 classes including a catch-all background (aka void) class. TP1 (i.e., Mapillary V2.0 4) contains 124 classes, out of which 116 are used for evaluation. Mapillary-LECO benchmark. The original Mapillary contains 18k training images, which is much larger than other mainstream street-level segmentation datasets such as Cityscape [14] (which has 2975 training and 500 validation images). In this work, we repurpose Mapillary to set up a LECO benchmark by splitting training set into two subsets (each containing 2.5k or 9k images) for the two time periods. We use the original validation set of Mapillary with 2k images as our test set. The detailed statistics is shown in Table 7. Baseline Experiments on Mapillary-LECO. We used the state-of-the-art architecture HRNet-V2- W48 for semantic segmentation with OCR module [ 72, 76, 81]. We noticed that for semantic segmentation on street-level imagery datasets [14], it is common to start from a Imagenet-pretrained model. Hence we name the weight initialization schemes for Mapillary-LECO as: • (TrainRandom) Train an entire network from randomly initialized weights. • (TrainScratch) Initialize the model backbone with ImageNet-pretrained weights.5 As we can see from Table 8,TrainScratch (from ImageNet pretrained weights) as a popular initializa- tion strategy for segmentation tasks [14, 55] indeed performs better than TrainRandom. Therefore, 4https://blog.mapillary.com/update/2021/01/18/vistas-2-dataset.html 5Weights are available athttps://github.com/HRNet/HRNet-Semantic-Segmentation 12Table 7: Mapillary-LECO statistics. We repurpose Mapillary [ 52, 55] V1.2 to V2.0 (a real-world dataset versioning scenario) to set up a LECO benchmark. Since V2.0 contains the same set of images as V1.2, in order to simulate a scenario with new samples, we select the ﬁrst 5k or all 18k training images, then split them to 2.5k/9k for TP0 and 2.5k/9k for TP1. Note that in TP0, the 66 classes include a catch-all background (or void) class, which is subsequently reﬁned to 9 ﬁne-grained classes such as traffic-cone and traffic-island. dataset Time Period 0 (TP0) Time Period 1 (TP 1) #classes #train #test #classes #train #test Mapillary-LECO 66 2.5k/9k 2k 116 2.5k/9k 2k for our FinetunePrev strategy, we ﬁnetune the model checkpoint obtained via TrainScratch strat- egy on TP0 data. To further bridge the gap to AllFine, we experiment other advanced techniques introduced in this paper such as LSSL and LJoint as shown in Table 10. Results of various LECO strategies. Since Mapillary does not reveal the taxonomic hierarchy from V1.2 to V2.0, we use the ground-truth label maps to automatically retrace the ontology evolution. Because Mapillary adopts RelabelOld strategy, each input image has both V1.2 and V2.0 label map. We exploit this fact to determine a single parent class in V1.2 for each of the 116 classes in V2.0 following a simple procedure (taking bird class in V2.0 as an example): • First, we use the ground-truth V2.0 label maps to ﬁnd all pixels labeled as bird. • Then, we locate those pixels in V1.2 label maps, and choose the V1.2 class that occupies the most pixels as bird’s parent. We ﬁnd that this simple procedure produces a reliable taxonomic hierarchy that can be used to improve the performance via LLPL, LSSL/Filter or LSSL/Cond, as shown in Table 11. Note that this procedure does not model class merging in Mapillary; however, our solutions still remain effective. Segmentation Loss. We make use of a standard pixel-level cross-entropy loss function for training on Mapillary. One difference from our previous image classiﬁcation setup is that both the ontology and semantic label maps changed from V1.2 to V2.0. In general, we ﬁnd the label maps on V2.0 to be of higher quality. As such, we (1) do not add coarse supervision on TP1 data (BK), since this pollutes the quality of annotations on the new data, i.e., LJoint = Lold( ˆBM) and LLPL = LoldLPL( ˆBM), and (2) we mask out gradients on pixel regions (around 0.3% of all pixels) where the given V1.2 labels do not align with the parent class of their V2.0 labels for Lold( ˆBM) and LoldLPL( ˆBM). Training and Inference Details . For both TrainRandom and TrainScratch, we use an initial learning rate of 0.03; for FinetunePrev, we use an initial learning rate of 0.003. The L2 weight decay is selected as 0.0005. Other hyperparameters followed the default strategy in HRNet used to achieve the SOTA results on Cityscape. We use SGD with 0.9 momentum and a batch size of 16 for BK (or 8 for BK and 8 for ˆBM if we use LSSL or LJoint). We use linear learning rate decay schedule that sets the learning rate to η(1 −k K)0.9 where ηis the initial learning rate and k/Kare the current and total iteration. For data augmentation, an input image and its label map are randomly ﬂipped horizontally and then its longer edge will be scaled with a base size of 2200 pixels multiplied by a scalar factor sampled between 0.5 and 2.1. Finally, a 720x720 region will be randomly cropped for each of the 16 images in mini-batch for training. For inference, we use a sliding window of 720x720 without scaling of the image to determine the ﬁnal pixel-level prediction. We also perform inference on the ﬂipped image and take the average prediction as ﬁnal result. Note that it is possible to use more advanced inference techniques such as multi-scaled testing, but we omit them to speed up inference since they are orthogonal to our research goal. We allocate the same training budget for all experiments. In particular, we use a total number of 1600 epochs for 2.5k/9k samples (a single TP), or 800 epochs for 5k/18k samples (two TPs data such as AllFine and LabelNew with SSL/Joint/LPL techniques). All experiments are conducted on internal clusters with 8 GeForce RTX 3090 cards. D: Dataset Statistics To better understand the long-tailed nature of the datasets, we include the per-class image and pixel distributions for iNat-LECO in Figure 4 andMapillary-LECO in Figure 5. We note that the long-tailed class distributions make semi-supervised methods less effective, as shown in Table 3, 4, 10, 11, e.g., the pseudo-labeling method of SSL underperforms the simple supervised learning baseline (Lonly). 13Table 8: Results of baseline methods for Mapillary-LECO , analogous to Table 2 in the main paper. We report the mean Intersection-over-Union (mIoU in %). TrainRandom trains a model from scratch, as is the protocol in the main paper. Because segmentation tasks typically make use of ImageNet pre-training, we also show results forTrainScratch that trains with an ImageNet-pretrained encoder, improving uponTrainRandom). Finally, FinetunePrev makes use of TP0’s model, which is trained withTrainScratch. In Table 10, we explore Joint-Training and SSL, leading to further improvement. Benchmark #images / TP TP1 Strategy TP1 mIoU LabelNew RelabelOld AllFine Mapillary-LECO 2500 TrainRandom 27.24 27 .21 27 .73 TrainScratch 28.22 29 .28 30 .71 FinetunePrev 30.39 29.05 31 .73 Mapillary-LECO 9000 TrainScratch 32.83 33 .44 33 .82 FinetunePrev 34.08 35.01 36 .20 TP0 Distribution (66 categories) TP 1 Distribution (116 categories) Mapillary-LECOPer-Class Pixel Distributions (Y-axis is log-scaled) Figure 5: Mapillary-LECO Per-Class Distributions. We plot distributions of training pixels for iNat-LECO in log-scale. The x-axis are classes sorted by the number of pixels. The y-axis shows the number of pixels per class (in log scale). E: Training Details on CIFAR-LECO and iNat-LECO We include all training details for reproducibility in this section. We adopt SOTA training strategies from recent papers [ 68–70]. For all experiments, we train the CNN with mini-batch stochastic gradient descent with 0.9 momentum. Same as FixMatch [68], we use the an exponential moving average (EMA) of model parameters for ﬁnal inference with a decay parameter of 0.999. We use a cosine annealing learning rate decay schedule which sets the learning rate to ηcos 7πk 16K where ηis the initial learning rate and k/Kare the current and total iteration. By default, we use a strong data augmentation scheme because it provides better generalization performance. In particular, we adopt RandAugment [16] as implemented in this repository.6 We did a grid search for best learning rate and weight decay using the validation set of each benchmark. The benchmark-speciﬁc hyperparameters are detailed below: CIFAR100-LECO. We use the same model architecture (Wide-ResNet-28-2 [ 82]) in FixMatch paper for CIFAR100 experiments [68]. We use a batch size of 128 (if using Lonly), or we split the batch to 64 for BK and 64 for ˆBM. For each experiment, we search for initial learning rate in [0.6,0.06,0.006,0.0006] and report the best mAcc result. We use an L2 weight decay of 5e-4. We run for a total number of 160K iterations and evaluate on the validation set per 1K iterations (equivalent to around 2000 epochs for 10000 images). iNat-LECO. We use the same model architecture (ResNet50 [28]) as in [70]. We use a batch size of 60 (if using Lonly), or we split the batch to 30 for BK and 30 for ˆBM. Because it is a long-tailed recognition problem, we search for L2 weight decay as suggested by [2] in [0.001,0.0001,0.00001] and found out 0.001 produces the best mAcc. Then for each experiment, we search for initial learning rate in [0.01,0.001,0.0001] and report the best mAcc result. We run for a total number of 250K iterations and evaluate on the validation set per 1K iterations (equivalent to around 300 epochs for 50000 images). 6https://github.com/kekmodel/FixMatch-pytorch/ 14Table 9: Results of baseline methods for LECO (complete results). For all our experiments except on Mapillary, we run each method ﬁve times and report the mean accuracy averaged over per-class accuracies with standard deviations. Because running on Mapillary is very compute-demanding, we run it once and report mean intersection-over-union (mIoU in %) over per-class IoU. For each benchmark, we bold the best mAcc/mIoU including ties that fall within its std. All conclusions we derived in main paper still hold, e.g.,FinetunePrev is the best learning strategy that outperforms TrainScratch (cf. 73.64% vs. 65.69% with LabelNew on iNat-LECO), implying that the coarse-to-ﬁne evolved ontology serves as a good learning curriculum.FreezePrev signiﬁcantly underperforms TrainScratch (cf. 50.82% vs. 65.69% with LabelNew on iNat-LECO), conﬁrming the difﬁculty of learning new classes that reﬁne the old. LabelNew and RelabelOld achieve similar performance with the former using only new data but not the old altogether. If using both old and new data, we boost performance as shown in Table 10. The conclusions hold for varied number of training images and generalize to the Mapillary experiments, yet all methods fall short to the AllFine (which trains on all data assumed to be labeled with new ontology). Benchmark #images / TP TP0 mAcc/mIoU TP1 Strategy TP1 mAcc/mIoU LabelNew RelabelOld AllFine CIFAR-LECO 1000 50.92±0.89 FinetunePrev33.53±0.57 32.85±0.47 42 .27±0.83 FreezePrev 24.13±0.67 24 .70±0.41 27 .82±0.43 TrainScratch 30.88±1.05 31 .02±0.68 41 .74±0.56 CIFAR-LECO 2000 59.48±0.66 FinetunePrev43.44±0.73 42.88 ±0.72 52.72±0.38 FreezePrev 33.74±1.46 33 .69±1.44 36 .57±1.68 TrainScratch 41.61±0.53 41 .77±0.79 52 .78±0.35 CIFAR-LECO 10000 77.78±0.27 FinetunePrev65.83±0.53 65.30 ±0.32 71.30±0.41 FreezePrev 52.64±0.50 52 .60±0.52 53 .79±0.43 TrainScratch 65.40±0.53 64.98±0.34 71 .43±0.24 iNat-LECO 50000 64.21±1.61 FinetunePrev73.64±0.46 71.26±0.54 84 .51±0.66 FreezePrev 50.82±0.68 54 .60±0.49 54 .08±0.31 TrainScratch 65.69±0.46 65 .78±0.56 73 .10±0.80 Mapillary-LECO 2500 37.01 FinetunePrev 30.39 29.05 31 .73 TrainScratch 27.24 27 .21 27 .73 Mapillary-LECO 9000 44.14 FinetunePrev 34.08 35.01 36.20 TrainScratch 32.83 33 .44 33 .82 iNat-4TP-LECO. The range of grid search of hyperparameters follows that ofiNat-LECO, including 250K iterations (equivalent to around 600 epochs for 25000 images) for each TP. All the classiﬁcation experiments were performed on a single modern GPU card. Based on the batch size scale, we use different GPU types, e.g., GeForce RTX 2080 Ti for image classiﬁcation on CIFAR and iNaturalist, and GeForce RTX 3090 Ti on Mapillary. F: Comprehensive Experiment Results We now show the complete results on varying sample sizes per TP for all benchmarks. Table 9, 10, 11 in appendix correspond to Table 2, 3, 4 in main paper. All conclusions generalize to varied number of samples and all 3 datasets. We also have complete results using all combinations of the various SSL/Joint/LPL techniques introduced in main paper. Results can be found on Table 12 ( CIFAR-LECO and iNat-LECO) and Table 13 (Mapillary-LECO). 15Table 10: Results of SSL and Joint Training methods (complete results). Following the notation of Table 9, we study LabelNew annotation strategies that make use of algorithms for learning from old examples without exploiting the relationship between old-vs-new labels. Recall that AllFine uses Lonly to train on all the data labeled with new ontology (doubling annotation cost). Using all the three losses together consistently improves mAcc / mIoU for all the benchmarks, conﬁrming the beneﬁt of exploiting the old examples. For instance, on iNat-LECO, this boosts the performance to 83.6%, approaching the AllFine (84.5%)! Similar trends hold for Mapillary-LECO. Benchmark #images / TP SSL Alg TP1 mAcc/mIoU Lonly +LSSL +LJoint +LJoint +LSSL AllFine CIFAR-LECO 1000 ST-Hard 33.53±0.57 34.59±0.57 37.25±0.66 35.27±0.64 42.27±0.83ST-Soft 37.67±0.46 38.48±0.67 PL 33.53±0.36 37 .28±0.67 FixMatch 34.20±0.24 37 .47±0.53 CIFAR-LECO 2000 ST-Hard 43.44±0.73 44.87±0.35 47.68±0.70 45.67±0.32 52.72±0.38ST-Soft 48.23±0.55 48.72 ±0.53 PL 43.46±0.61 47 .24±0.40 FixMatch 44.98±0.59 48.58±0.71 CIFAR-LECO 10000 ST-Hard 65.83±0.27 67.11±0.34 68.74±0.21 67.97±0.37 71.30±0.41ST-Soft 69.86±0.30 69.77 ±0.71 PL 65.86±0.14 68 .43±0.40 FixMatch 67.13±0.36 69 .03±0.43 iNat-LECO 50000 ST-Hard 73.64±0.46 75.23±0.14 82.98±0.33 78.23±0.38 84.51±0.66ST-Soft 79.64±0.41 83.61±0.39 PL 73.56±0.09 82 .87±0.51 FixMatch 74.57±0.54 83 .00±0.33 Mapillary-LECO 2500 ST-Hard 30.39 30.06 31.05 31.09 31.73 Mapillary-LECO 9000 ST-Hard 34.08 32.68 35.40 35.72 36.20 Table 11: Results of exploiting taxonomic hierarchy (complete results). We copy the best results in Table 10 (all obtained by +LJoint +LSSL) for reference. We can see that LLPL is less effective but still competitive on Mapillary-LECO, presumably because it does not model the class-merging scenarios. Nonetheless, combinations of the strategies, i.e., adding SSL loss along with pseudo-label reﬁnement via either Filtering or Conditioning still bridge the gap to AllFine. Benchmark #images / TP SSL Alg TP1 mAcc +LJoint +LSSL +LLPL +LLPL +LSSL/Filter +LLPL +LSSL/Cond AllFine CIFAR-LECO 1000 ST-Hard 38.48±0.67 38.47±0.52 38.87±0.54 38.68 ±0.41 42.27±0.83ST-Soft 38.87±0.52 38.19±0.71 PL 38.56±0.44 38.67 ±0.81 FixMatch 37.55±0.69 37 .35±0.72 CIFAR-LECO 2000 ST-Hard 48.72±0.53 49.04±0.47 49.36±0.56 48.65±0.53 52.72±0.38ST-Soft 49.09±0.46 48 .54±0.52 PL 49.25±0.34 48.91±0.87 FixMatch 49.44±0.22 48.81±0.40 CIFAR-LECO 10000 ST-Hard 69.77±0.71 69.31±0.30 69.90±0.29 70.42±0.23 71.30±0.41ST-Soft 70.10±0.12 70 .02±0.63 PL 69.20±0.20 69 .60±0.26 FixMatch 69.61±0.17 69 .85±0.46 iNat-LECO 50000 ST-Hard 83.61±0.39 83.62±0.21 84.23±0.61 84.34 ±0.25 84.51±0.66ST-Soft 84.12±0.31 83.76±0.40 PL 83.72±0.22 84.16±0.19 FixMatch 83.91±0.39 83 .95±0.40 Mapillary-LECO 2500 ST-Hard 31.09 31 .04 31.41 31.23 31.73 Mapillary-LECO 9000 ST-Hard 35.72 35 .04 36.12 35.47 36.20 16Table 12: Complete results of LECO strategies on classiﬁcation benchmarks. We report results on combi- nations of all LECO stratgies on CIFAR100-LECO and iNat-LECO with FinetunePrev strategy on TP1. Major conclusions hold across all benchmarks: (1) Utilizing old-ontology samples and labels, i.e., LSSL and LJoint, helps bridge the gap to AllFine, and (2) exploiting taxonomic hierarchy via LSSL/Filter , LSSL/Cond , or LLPL further improves the performance. SetupTP0mAcc TP1mAcc AllFine LabelNewNo SSL PL FixMatch ST-Hard ST-SoftL LLPLLJoint L LLPLLJoint L LLPLLJoint L LLPLLJoint L LLPLLJoint CIFAR100-1k50.92±0.8942.27±0.8333.53±0.57 38.47±0.52 37.25±0.66+LSSL33.53±0.36 38.70±0.65 37.28±0.6734.20±0.24 37.56±0.54 37.47±0.5334.59±0.57 36.78±0.40 35.27±0.6437.67±0.46 38.80±0.49 38.48±0.67+LSSL/Filter33.33±0.69 38.56±0.44 37.44±0.8436.02±0.33 37.55±0.69 37.23±0.2037.93±0.50 38.87±0.54 38.57±0.7437.59±0.69 38.87±0.52 38.43±0.67+LSSL/Cond37.40±1.19 38.67±0.81 38.00±0.5936.92±0.64 37.35±0.72 37.08±0.5737.95±0.34 38.68±0.41 38.20±0.3038.25±0.62 38.19±0.71 38.45±0.49 CIFAR100-2k59.48±0.6652.72±0.3843.44±0.73 49.04±0.47 47.68±0.70 LSSL43.46±0.61 48.82±0.30 47.24±0.4044.98±0.59 49.38±0.29 48.58±0.7144.87±0.35 46.98±0.33 45.67±0.3248.23±0.55 49.12±0.50 48.72±0.53LSSL/Filter43.50±0.47 49.25±0.34 47.38±0.6046.14±0.33 49.44±0.22 48.23±1.2147.00±0.36 49.36±0.56 49.01±0.7347.62±0.74 49.09±0.46 48.56±0.50LSSL/Cond48.28±0.27 48.91±0.87 48.45±0.3448.23±0.93 48.81±0.40 48.57±0.5848.81±0.57 48.65±0.53 48.60±0.6848.70±0.58 48.54±0.52 48.86±0.61 CIFAR100-10k77.78±0.2771.30±0.4165.83±0.27 69.31±0.30 68.74±0.21 LSSL65.86±0.14 69.48±0.24 68.43±0.4067.13±0.36 69.71±0.49 69.03±0.4367.11±0.34 68.58±0.51 67.97±0.3769.86±0.30 68.96±0.27 69.77±0.71LSSL/Filter65.80±0.34 69.20±0.20 68.82±0.2367.60±0.25 69.61±0.17 69.15±0.4169.04±0.29 69.90±0.29 69.95±0.2868.76±0.20 70.10±0.12 69.85±0.38LSSL/Cond68.32±0.26 69.60±0.26 69.09±0.1368.86±0.52 69.85±0.46 69.41±0.3970.22±0.38 70.42±0.23 70.05±0.1870.33±0.15 70.02±0.63 69.93±0.24 iNat-50k64.21±1.6184.51±0.6673.64±0.46 83.62±0.21 82.98±0.33 LSSL73.56±0.09 83.71±0.44 82.87±0.5174.57±0.54 84.05±0.26 83.00±0.3375.23±0.14 79.07±0.48 78.23±0.3879.64±0.41 84.10±0.54 83.61±0.39LSSL/Filter74.01±0.41 83.72±0.22 82.81±0.2774.55±0.33 83.91±0.39 83.16±0.3379.91±0.58 84.23±0.61 83.92±0.2578.71±0.22 84.12±0.31 83.80±0.60LSSL/Cond81.98±0.12 84.16±0.19 82.99±0.6482.62±0.21 83.95±0.40 83.66±0.3684.20±0.29 84.34±0.25 84.37±0.3983.96±0.47 83.76±0.40 83.95±0.20 Table 13: Complete results of LECO strategies on Mapillary-LECO. We report results on combinations of all LECO stratgies on Mapillary-LECO with FinetunePrev strategy on TP1. It is worth noting that new labels in V2.0 do not necessarily have surjective mappings to old labels of V1.2. This means, it is non-trivial to apply coarse supervision as partial labels. Moreover, we only perform ST-Hard for SSL because other methods are excessively compute-demanding (e.g., ST-SOft requires saving per-class heatmaps for each image to allow making use of pseudo-labels’ scores, FixMatch requires learning a separate large-scale model, etc.). Concretely, we (1) do not add coarse supervision for BK since it pollutes quality of annotation on new data, and (2) mask out gradient where the given V1.2 labels do not align with the parent class forLold( ˆBM ) and LoldLPL ( ˆBM ). Major conclusions hold here. We can see that all combinations of various LECO stratgies achieve suprisingly good performances, whereas the best combination bridges 95% of the gap to AllFine (gap reduces from 1.34 to 0.08 for 2.5k samples with LSSL and LLPL , and from 2.12 to 0.08 for 9k samples with LSSL/Filter and LLPL ). Setup TP0mIoU TP1mIoU AllFine LabelNew No SSL ST-Hard L L LPL LJoint L L LPL LJoint Mapillary-2.5k 37.01 31 .73 30 .39 31 .04 31 .05 +LSSL 30.06 31 .65 31 .09 +LSSL/Filter 30.64 31 .41 31 .57 +LSSL/Cond 31.01 31 .23 31 .50 Mapillary-9k 44.14 36 .20 34 .08 35 .04 35 .40 +LSSL 32.68 35 .97 35 .72 +LSSL/Filter 34.96 36 .12 36 .06 +LSSL/Cond 35.29 35 .47 35 .70 17References [1] Abdelsalam, M., Faramarzi, M., Sodhani, S., and Chandar, S. (2021). Iirc: Incremental implicitly-reﬁned classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11038–11047. [2] Alshammari, S., Wang, Y ., Ramanan, D., and Kong, S. (2022). Long-tailed recognition via weight balancing. In CVPR. [3] Azizpour, H., Razavian, A. S., Sullivan, J., Maki, A., and Carlsson, S. (2015). Factors of transferability for a generic convnet representation. IEEE transactions on pattern analysis and machine intelligence, 38(9), 1790–1802. [4] Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., and Gall, J. (2019). Semantickitti: A dataset for semantic scene understanding of lidar sequences. In ICCV. [5] Bengio, Y ., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. InProceedings of the 26th annual international conference on machine learning, pages 41–48. [6] Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and Raffel, C. A. (2019a). Mixmatch: A holistic approach to semi-supervised learning. NeurIPS, 32. [7] Berthelot, D., Carlini, N., Cubuk, E. D., Kurakin, A., Sohn, K., Zhang, H., and Raffel, C. (2019b). Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. arXiv preprint arXiv:1911.09785. [8] Bertinetto, L., Mueller, R., Tertikas, K., Samangooei, S., and Lord, N. A. (2020). Making better mistakes: Leveraging class hierarchies with deep networks. In CVPR. [9] Bucak, S. S., Jin, R., and Jain, A. K. (2011). Multi-label learning with incomplete class assignments. In CVPR 2011, pages 2801–2808. IEEE. [10] Cai, Z., Sener, O., and Koltun, V . (2021). Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8281–8290. [11] Caruana, R. (1997). Multitask learning. Machine learning, 28(1), 41–75. [12] Chang, M.-F., Lambert, J., Sangkloy, P., Singh, J., Bak, S., Hartnett, A., Wang, D., Carr, P., Lucey, S., Ramanan, D., et al. (2019). Argoverse: 3d tracking and forecasting with rich maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8748–8757. [13] Cohen, N., Gal, R., Meirom, E. A., Chechik, G., and Atzmon, Y . (2022). \" this is my unicorn, ﬂuffy\": Personalizing frozen vision-language representations. arXiv preprint arXiv:2204.01694. [14] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., and Schiele, B. (2016). The cityscapes dataset for semantic urban scene understanding. In CVPR. [15] Cour, T., Sapp, B., and Taskar, B. (2011). Learning from partial labels. The Journal of Machine Learning Research, 12, 1501–1536. [16] Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V . (2020). Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703. [17] De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. (2021). A continual learning survey: Defying forgetting in classiﬁcation tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7), 3366–3385. [18] Delange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. (2021). A continual learning survey: Defying forgetting in classiﬁcation tasks. PAMI. [19] Deng, J., Ding, N., Jia, Y ., Frome, A., Murphy, K., Bengio, S., Li, Y ., Neven, H., and Adam, H. (2014). Large-scale object classiﬁcation using label relation graphs. In European conference on computer vision, pages 48–64. Springer. [20] Donahue, J., Jia, Y ., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pages 647–655. PMLR. 18[21] Elman, J. L. (1993). Learning and development in neural networks: The importance of starting small. Cognition, 48(1), 71–99. [22] Everingham, M., Eslami, S. A., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. (2015). The pascal visual object classes challenge: A retrospective. IJCV, 111(1), 98–136. [23] Geiger, A., Lenz, P., and Urtasun, R. (2012). Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 3354–3361. IEEE. [24] Gidaris, S., Bursuc, A., Komodakis, N., Pérez, P., and Cord, M. (2019). Boosting few-shot visual learning with self-supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8059–8068. [25] Goel, A., Fernando, B., Keller, F., and Bilen, H. (2022). Not all relations are equal: Mining informative labels for scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15596–15606. [26] Grandvalet, Y ., Bengio, Y .,et al. (2004). Learning from partial labels with minimum entropy. Technical report, CIRANO. [27] Gupta, A., Dollar, P., and Girshick, R. (2019). Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5356–5364. [28] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In CVPR. [29] Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. [30] Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019). Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR. [31] Hsieh, C.-Y ., Xu, M., Niu, G., Lin, H.-T., and Sugiyama, M. (2019). A pseudo-label method for coarse-to- ﬁne multi-label learning with limited supervision. [32] Hu, P., Lipton, Z. C., Anandkumar, A., and Ramanan, D. (2018). Active learning with partial feedback. arXiv preprint arXiv:1802.07427. [33] iNaturalist 2017 (2017). https://www.inaturalist.org/stats/2017. [34] iNaturalist 2018 (2018). https://www.inaturalist.org/stats/2018. [35] iNaturalist 2019 (2019). https://www.inaturalist.org/stats/2019. [36] iNaturalist 2020 (2020). https://www.inaturalist.org/stats/2020. [37] iNaturalist 2021 (2021). https://www.inaturalist.org/stats/2021. [38] Jia, M., Tang, L., Chen, B.-C., Cardie, C., Belongie, S., Hariharan, B., and Lim, S.-N. (2022). Visual prompt tuning. arXiv preprint arXiv:2203.12119. [39] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., and Kalantidis, Y . (2020). Decoupling repre- sentation and classiﬁer for long-tailed recognition. In International Conference on Learning Representations. [40] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13), 3521–3526. [41] Kong, S. and Ramanan, D. (2021). Opengan: Open-set recognition via open data generation. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 813–822. [42] Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. [43] Krueger, K. A. and Dayan, P. (2009). Flexible shaping: How learning in small steps helps. Cognition, 110(3), 380–394. [44] Lee, D.-H. et al. (2013). Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, page 896. 19[45] Lei, J., Guo, Z., and Wang, Y . (2017). Weakly supervised image classiﬁcation with coarse and ﬁne labels. In 2017 14th Conference on Computer and Robot Vision (CRV), pages 240–247. IEEE. [46] Li, W.-H., Liu, X., and Bilen, H. (2022). Cross-domain few-shot learning with task-speciﬁc adapters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7161–7170. [47] Li, Z. and Hoiem, D. (2017). Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12), 2935–2947. [48] Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In ECCV. Springer. [49] Lin, Z., Shi, J., Pathak, D., and Ramanan, D. (2021). The clear benchmark: Continual learning on real-world imagery. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). [50] Lomonaco, V . and Maltoni, D. (2017). Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pages 17–26. PMLR. [51] Maltoni, D. and Lomonaco, V . (2019). Continuous learning in single-incremental-task scenarios.Neural Networks, 116, 56–73. [52] Mapillary-Vistas-2.0 (2021). https://blog.mapillary.com/update/2021/01/18/ vistas-2-dataset.html. [53] Marszalek, M. and Schmid, C. (2007). Semantic hierarchies for visual object recognition. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–7. IEEE. [54] McLachlan, G. J. (1975). Iterative reclassiﬁcation procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis. Journal of the American Statistical Association, 70(350), 365–369. [55] Neuhold, G., Ollmann, T., Rota Bulo, S., and Kontschieder, P. (2017). The mapillary vistas dataset for semantic understanding of street scenes. In Proceedings of the IEEE international conference on computer vision, pages 4990–4999. [56] Nguyen, N. and Caruana, R. (2008). Classiﬁcation with partial labels. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 551–559. [57] Peterson, G. B. (2004). A day of great illumination: Bf skinner’s discovery of shaping. Journal of the experimental analysis of behavior, 82(3), 317–328. [58] Phoo, C. P. and Hariharan, B. (2021). Coarsely-labeled data for better few-shot transfer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9052–9061. [59] Prabhu, A., Torr, P. H., and Dokania, P. K. (2020). Gdumb: A simple approach that questions our progress in continual learning. In ECCV. [60] Rebufﬁ, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. (2017). icarl: Incremental classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010. [61] Ristin, M., Gall, J., Guillaumin, M., and Van Gool, L. (2015). From categories to subcategories: large-scale image classiﬁcation with partial class label reﬁnement. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 231–239. [62] Rizve, M. N., Duarte, K., Rawat, Y . S., and Shah, M. (2021). In defense of pseudo-labeling: An uncertainty- aware pseudo-label selection framework for semi-supervised learning. arXiv preprint arXiv:2101.06329. [63] Sanger, T. D. (1994). Neural network learning control of robot manipulators using gradually increasing task difﬁculty. IEEE transactions on Robotics and Automation, 10(3), 323–333. [64] Sariyildiz, M. B., Kalantidis, Y ., Larlus, D., and Alahari, K. (2021). Concept generalization in visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9629–9639. [65] Scudder, H. (1965). Probability of error of some adaptive pattern-recognition machines.IEEE Transactions on Information Theory, 11(3), 363–371. 20[66] Sharif Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 806–813. [67] Skinner, B. F. (1958). Reinforcement today. American Psychologist, 13(3), 94. [68] Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C. A., Cubuk, E. D., Kurakin, A., and Li, C.-L. (2020). Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence. NeurIPS. [69] Su, J. and Maji, S. (2021a). Semi-supervised learning with taxonomic labels. In British Machine Vision Conference (BMVC). [70] Su, J., Cheng, Z., and Maji, S. (2021). A realistic evaluation of semi-supervised learning for ﬁne-grained classiﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [71] Su, J.-C. and Maji, S. (2021b). The semi-supervised inaturalist challenge at the fgvc8 workshop. arXiv preprint arXiv:2106.01364. [72] Sun, K., Xiao, B., Liu, D., and Wang, J. (2019). Deep high-resolution representation learning for human pose estimation. In CVPR. [73] Taherkhani, F., Kazemi, H., Dabouei, A., Dawson, J., and Nasrabadi, N. M. (2019). A weakly supervised ﬁne label classiﬁer enhanced by coarse supervision. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 6459–6468. [74] Van de Ven, G. M. and Tolias, A. S. (2019). Three scenarios for continual learning. arXiv preprint arXiv:1904.07734. [75] Van Horn, G., Mac Aodha, O., Song, Y ., Cui, Y ., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. (2018). The inaturalist species classiﬁcation and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8769–8778. [76] Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y ., Liu, D., Mu, Y ., Tan, M., Wang, X., Liu, W., and Xiao, B. (2019). Deep high-resolution representation learning for visual recognition. TPAMI. [77] Wang, X., Wu, Z., Lian, L., and Yu, S. X. (2022). Debiased learning from naturally imbalanced pseudo- labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14647–14657. [78] Wilson, B., Qi, W., Agarwal, T., Lambert, J., Singh, J., Khandelwal, S., Pan, B., Kumar, R., Hartnett, A., Pontes, J. K., et al. (2021). Argoverse 2: Next generation datasets for self-driving perception and forecasting. [79] Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V . (2020). Self-training with noisy student improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687–10698. [80] Yosinski, J., Clune, J., Bengio, Y ., and Lipson, H. (2014). How transferable are features in deep neural networks? NeurIPS. [81] Yuan, Y ., Chen, X., and Wang, J. (2020). Object-contextual representations for semantic segmentation. [82] Zagoruyko, S. and Komodakis, N. (2016). Wide residual networks. arXiv preprint arXiv:1605.07146. [83] Zenke, F., Poole, B., and Ganguli, S. (2017). Continual learning through synaptic intelligence. In ICML. [84] Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. (2019). S4l: Self-supervised semi-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1476–1485. [85] Zhang, H., Cisse, M., Dauphin, Y . N., and Lopez-Paz, D. (2018). mixup: Beyond empirical risk minimiza- tion. In International Conference on Learning Representations. [86] Zhang, J., Zhang, J., Ghosh, S., Li, D., Tasci, S., Heck, L., Zhang, H., and Kuo, C.-C. J. (2020a). Class- incremental learning via deep model consolidation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1131–1140. [87] Zhang, J. O., Sax, A., Zamir, A., Guibas, L., and Malik, J. (2020b). Side-tuning: a baseline for network adaptation via additive side networks. In European Conference on Computer Vision, pages 698–714. Springer. [88] Zweig, A. and Weinshall, D. (2007). Exploiting object hierarchy: Combining models from different category levels. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE. 21",
      "references": [
        "Iirc: Incremental implicitly-refined classification",
        "Long-tailed recognition via weight balancing",
        "Factors of transferability for a generic convnet representation",
        "Semantickitti: A dataset for semantic scene understanding of lidar sequences",
        "Curriculum learning",
        "Mixmatch: A holistic approach to semi-supervised learning",
        "Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring",
        "Making better mistakes: Leveraging class hierarchies with deep networks",
        "Multi-label learning with incomplete class assignments",
        "Online continual learning with natural distribution shifts: An empirical study with visual data",
        "Multitask learning",
        "Argoverse: 3d tracking and forecasting with rich maps",
        "\" this is my unicorn, fluffy\": Personalizing frozen vision-language representations",
        "The cityscapes dataset for semantic urban scene understanding",
        "Learning from partial labels",
        "Randaugment: Practical automated data augmentation with a reduced search space",
        "A continual learning survey: Defying forgetting in classification tasks",
        "Large-scale object classification using label relation graphs",
        "Decaf: A deep convolutional activation feature for generic visual recognition",
        "Learning and development in neural networks: The importance of starting small",
        "The pascal visual object classes challenge: A retrospective",
        "Are we ready for autonomous driving? the kitti vision benchmark suite",
        "Boosting few-shot visual learning with self-supervision",
        "Not all relations are equal: Mining informative labels for scene graph generation",
        "Learning from partial labels with minimum entropy",
        "Lvis: A dataset for large vocabulary instance segmentation",
        "Deep residual learning for image recognition",
        "Distilling the knowledge in a neural network",
        "Parameter-efficient transfer learning for nlp",
        "A pseudo-label method for coarse-to-fine multi-label learning with limited supervision",
        "Active learning with partial feedback",
        "Decoupling representation and classifier for long-tailed recognition",
        "Overcoming catastrophic forgetting in neural networks",
        "Opengan: Open-set recognition via open data generation",
        "Learning multiple layers of features from tiny images",
        "Flexible shaping: How learning in small steps helps",
        "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
        "Weakly supervised image classification with coarse and fine labels",
        "Visual prompt tuning",
        "The clear benchmark: Continual learning on real-world imagery",
        "Core50: a new dataset and benchmark for continuous object recognition",
        "Continuous learning in single-incremental-task scenarios",
        "Semantic hierarchies for visual object recognition",
        "Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis",
        "The mapillary vistas dataset for semantic understanding of street scenes",
        "Classification with partial labels",
        "A day of great illumination: Bf skinner’s discovery of shaping",
        "Coarsely-labeled data for better few-shot transfer",
        "Gdumb: A simple approach that questions our progress in continual learning",
        "Learning without forgetting",
        "Microsoft coco: Common objects in context",
        "Continual learning on real-world imagery",
        "From categories to subcategories: large-scale image classification with partial class label refinement",
        "In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning",
        "Neural network learning control of robot manipulators using gradually increasing task difficulty",
        "Concept generalization in visual representation learning",
        "Probability of error of some adaptive pattern-recognition machines",
        "Cnn features off-the-shelf: an astounding baseline for recognition",
        "Reinforcement today",
        "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
        "Semi-supervised learning with taxonomic labels",
        "A realistic evaluation of semi-supervised learning for fine-grained classification",
        "The semi-supervised inaturalist challenge at the fgvc8 workshop",
        "Deep high-resolution representation learning for human pose estimation",
        "Weakly supervised fine label classifier enhanced by coarse supervision",
        "Three scenarios for continual learning",
        "The inaturalist species classification and detection dataset",
        "Deep high-resolution representation learning for visual recognition",
        "Debiased learning from naturally imbalanced pseudo-labels",
        "Argoverse 2: Next generation datasets for self-driving perception and forecasting",
        "Self-training with noisy student improves imagenet classification",
        "How transferable are features in deep neural networks?",
        "Object-contextual representations for semantic segmentation",
        "Wide residual networks",
        "Continual learning through synaptic intelligence",
        "S4l: Self-supervised semi-supervised learning",
        "mixup: Beyond empirical risk minimization",
        "Class-incremental learning via deep model consolidation",
        "Side-tuning: a baseline for network adaptation via additive side networks",
        "Exploiting object hierarchy: Combining models from different category levels"
      ],
      "meta_data": {
        "arxiv_id": "2210.04993v4",
        "authors": [
          "Zhiqiu Lin",
          "Deepak Pathak",
          "Yu-Xiong Wang",
          "Deva Ramanan",
          "Shu Kong"
        ],
        "published_date": "2022-10-10T19:58:23Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces and formalizes Learning with Evolving Class Ontology (LECO), where class vocabularies evolve over time periods by refining/expanding coarse labels into fine labels (and sometimes merging/splitting/renaming). Establishes a benchmarking protocol and empirical study showing: (i) annotating new data with the new ontology (LabelNew) is consistently better than relabeling old data (RelabelOld) under equal labeling budget; (ii) finetuning the previous model is superior to training from scratch, suggesting coarse-to-fine curricula; (iii) learning from heterogeneous annotations (old coarse + new fine) via joint training plus semi-supervised/pseudo-labeling and hierarchy-aware reconciliation can approach an oracle upper bound where all data is exhaustively labeled with the newest ontology.",
        "methodology": "Repurposes continual learning and semi-supervised learning for LECO with unlimited storage of historical data. Core strategies: (1) Training strategies—FreezePrev, TrainScratch, FinetunePrev. (2) Annotation strategies—LabelNew vs RelabelOld. (3) Leveraging old data: SSL/self-training on old samples by pseudo-labeling them with the new ontology (ST-Hard one-hot, ST-Soft soft targets via KL, Pseudo-Labeling with confidence threshold, FixMatch consistency). (4) Leveraging old labels: Joint Training with multiple heads for each TP and cross-entropy on old heads (coarse supervision). (5) Exploiting hierarchy: Learning-with-Partial-Labels (LPL) by marginalizing fine-class probabilities to compute coarse probabilities using an edge matrix E and training with cross-entropy on coarse labels; and hierarchy-aware pseudo-label refinement on old samples via filtering inconsistent pseudo labels or conditioning/renormalizing probabilities to be consistent with the known coarse parent label. Uses standard CNN backbones (WideResNet-28-2, ResNet-50) and HRNet+OCR for segmentation; SGD with momentum, cosine LR, weight decay tuning, strong augmentation (RandAugment), EMA for inference in classification.",
        "experimental_setup": "Benchmarks (mostly 2 TPs; also 4 TPs): CIFAR-LECO: CIFAR-100 hierarchy with TP0=20 superclasses, TP1=100 subclasses; 10k train/TP, 10k test/TP; 20% train held out for validation. iNat-LECO: Semi-iNat-2021 with TP0=‘order’ (123 classes), TP1=‘species’ (810); 50k train/TP, 4k test; 20% validation. iNat-4TP-LECO: 4 TPs across taxonomic levels order/family/genus/species (123/339/729/810); 25k train/TP, 4k test; 20% validation. Mapillary-LECO: semantic segmentation with Mapillary Vistas v1.2 (66 classes) as TP0 and v2.0 (116 eval classes) as TP1; 2.5k train/TP, 2k test; no validation (uses fixed hyperparams from prior work). Metrics: mean per-class accuracy (mAcc) for classification; mean IoU (mIoU) for segmentation. Runs: 5 seeds for CIFAR/iNat; single run for Mapillary due to compute. Compares against ‘AllFine’ oracle that assumes all data labeled with newest ontology. Reports large gains from joint training + SSL + hierarchy methods, often within ~0.2% of AllFine on iNat-LECO.",
        "limitations": "Assumes equal annotation cost for labeling new data vs relabeling old; does not model/measure real acquisition/curation costs for collecting new data. Mostly assumes static data distribution across time periods (no temporal/domain shift). Relies on availability/quality of taxonomic mappings; LPL less effective when ontology evolution includes class merging/non-surjective mappings (notably Mapillary). SSL pseudo-labeling can be noisy/bias-prone and may be less effective in long-tailed settings; segmentation SSL variants limited by compute/memory constraints. Limited hyperparameter tuning for large-scale segmentation (Mapillary) and fewer runs there, reducing statistical confidence. Focuses on classification/segmentation; does not address other tasks or open-set/unknown-class emergence beyond a background-class remark.",
        "future_research_directions": "Study realistic annotation economics: varying costs, interface design for efficient relabeling, and data acquisition/mining/cleaning costs; incorporate continuously collected unlabeled streams and active learning for selecting what to label under evolving ontologies. Extend LECO to non-stationary/temporally shifting distributions and evaluate robustness under domain shifts. Develop methods that explicitly handle complex ontology edits (splits, merges, re-parenting) and uncertain/learned hierarchies rather than given trees. Explore parameter-efficient and scalable finetuning (adapters, prompts, LoRA) for frequent ontology updates. Improve hierarchy-aware SSL for long-tailed and segmentation settings (better calibration, debiasing, uncertainty-aware pseudo labels, memory-efficient per-pixel soft labels). Add broader benchmarks (detection, instance segmentation) and evaluation protocols that consider backward compatibility and performance on both old and new ontologies.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Learning to Navigate Wikipedia by Taking Random Walks",
      "full_text": "Learning to Navigate Wikipedia by Taking Random Walks Manzil Zaheer, Kenneth Marino, Will Grathwohl, John Schultz, Wendy Shang, Sheila Babayan, Arun Ahuja, Ishita Dasgupta, Christine Kaeser-Chen, Rob Fergus DeepMind New York {manzilzaheer, kmarino, wgrathwohl, jhtschultz, wendyshang, sbabayan, arahuja, idg, christinech, robfergus}@google.com Abstract A fundamental ability of an intelligent web-based agent is seeking out and acquiring new information. Internet search engines reliably ﬁnd the correct vicinity but the top results may be a few links away from the desired target. A complementary approach is navigation via hyperlinks, employing a policy that comprehends local content and selects a link that moves it closer to the target. In this paper, we show that behavioral cloning of randomly sampled trajectories is sufﬁcient to learn an effective link selection policy. We demonstrate the approach on a graph version of Wikipedia with 38M nodes and 387M edges. The model is able to efﬁciently navigate between nodes 5 and 20 steps apart 96% and 92% of the time, respectively. We then use the resulting embeddings and policy in downstream fact veriﬁcation and question answering tasks where, in combination with basic TF-IDF search and ranking methods, they are competitive results to the state-of-the-art methods. 1 Introduction The ability to gather new knowledge about the world is a fundamental aspect of intelligence. Based only on a few words, a fact, question, or even vague idea, humans have the ability to use the internet to ﬁnd extremely speciﬁc information about the world. From the important (how to administer CPR) to the trivial (the LA Raiders won the 1983 SuperBowl), virtually any knowledge is a click away. In this work, we focus in one particular aspect of this ability: web navigation. In general, navigation is a key component of an embodied agent: the ability to move efﬁciently toward a target. In known environments, where map information is available, shortest-path algorithms provide a viable solution. However, in novel settings, the agent lacks such global information and must instead navigate to the target, using its understanding of the local environment to select actions. Other approaches to web agents have focused mostly on retrieval or search engines. These, however, only provide a partial solution, typically getting close to a desired target but often not exactly to the right page. In this work, we present an approach for navigation on graph-structured web data that uses hyperlinks within articles to navigate toward a target. It complements search engines, using them to provide a sensible starting point for a local search for speciﬁc information. To be able to navigate on the web effectively, we must read the text of the page, see how concepts in the current page are related to a possible next page, and use our understanding to learn a policy to make the right navigation decision. Consider the scenario shown in Figure 1. The agent is trying to navigate to the target node NORTH AMERICA while currently at the node PRESIDENTS OF THE UNITED STATES containing two hyperlinks: BARACK OBAMA and USA . Our approach learns an embedding from the text of the current page and the text for each hyperlink node and passes these 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2211.00177v1  [cs.LG]  31 Oct 2022Barack  Obama USA Current Node Presidents of the  United States Node Node Text “Barack Obama was  the President of the  United States…” “Barack Obama is an  American Politician…” “The USA is a nation in  North America…” ϕ Transformer st a1 “The USA is a nation in  North America…” a2 North America  “North America is a  continent on the…” sgGoal Node MLP Action probability p1 p2 Node 1 Node 2 ϕ ϕ ϕ θ θ Figure 1: Our agent navigates through hyperlinks on a Wikipedia graph towards a target node. It ﬁrst embeds the text at each link node using Transformer φ(.) and then evaluates different actions/links a1,a2 with a policy θ, conditioned on the current and target embeddings st and sg, respectively. to a goal-conditioned policy that selects the best possible action. To make the correct choice in this example, the agent should associate USA with N ORTH AMERICA . The particular web environment we consider here is Wikipedia, converted into a graph form. Each of the 38M paragraphs is represented by a node; edges are links within and between articles. We introduce an approach for navigation on this graph based on behavioral cloning of automatically generated trajectories. This allows for unsupervised pre-training of both web page embeddings and navigation policy. We explore different trajectory distributions for training the model, the best practical choice being equivalent to random walks on the graph. Contributions: Our work presents the ﬁrst viable solution to the problem of web navigation 1, as demonstrated by reliable (>90% success) navigation on a graph of size107. Previous attempts such as Nogueira and Cho [2016] show a far inferior performance, well below that needed for practical utility. Furthermore, as there is nothing Wikipedia-speciﬁc about our approach and it scales gracefully, the method is applicable to general web settings. We also demonstrate the relevance of the pre-train + ﬁne-tune paradigm to the web domain, showing that pre-training for navigation is a suitable objective to aid downstream tasks with limited data such as Q&A. We demonstrate this by applying our navigation approach on two challenging tasks: fact veriﬁcation (FEVER; Thorne et al. [2018]) and Q&A (Natural Questions; Lee et al. [2019]) for evidence gathering. In both settings, we show a jump in performance over existing search-based approaches that rely on retrieval followed by re-ranking, matching state-of-the-art methods despite using far simpler retrieval and re-ranking mechanisms. 2 Background Wikipedia as a navigation environment: Human behavior on Wikipedia [West and Leskovec, 2012a] was collected via the Wikispeedia game [West et al., 2009], which asked the players to move between a pair of arbitrarily chosen articles in the fewest steps. This rule allows the emergence of a semantic distance between concepts that facilitates retrieval. West and Leskovec [2012b] conducted initial studies on this dataset for navigation, using supervised learning and reinforcement learning methods on a small subset of articles (4.6K) with simple TF-IDF features. Most relevantly, Nogueira and Cho [2016] explore the same problem of navigation on a large Wikipedia graph (12M nodes). They train LSTM agents to perform navigation on the graph, using Bag-of-Word features for each article. The trained agent is then applied to a Jeopardy Q&A task. Our approach revisits this framing but with a number of important differences: (i) training on a signiﬁcantly larger graph (38M vs 12M nodes, 51.5M vs 380M edges); (ii) using Transformer-based architectures for paragraph encodings and for the navigation policy; (iii) they use a single ﬁxed start node, whereas ours is able to navigate from any start node; (iv) removal of beam search, or other complex search mechanisms at evaluation time. Collectively, our approach boosts the navigation success rate from 12.5% in Nogueira and Cho [2016] to over 90%,2 despite a much larger graph. 1A demo of our agent navigating is shown here https://www.youtube.com/watch?v=LVqOaPKpC2c 2Exact comparisons are infeasible given idiosyncrasies in graph construction and other differences in setup. 2Fact veriﬁcation and Q&A using Wikipedia: Wikipedia is a rich resource for evaluating language understanding through Q&A tasks. Chen et al. [2017] combine TF-IDF retrieval with an LSTM docu- ment reader to ﬁnd answers to free-form questions within Wikipedia documents. Contemporary Q&A systems [Semnani and Pandey, 2020, Qu et al., 2020] extend this general approach to more complex pipelines, combining traditional and neural methods with inverted index search/retrieval schemes, followed by document readers that perform re-ranking. We demonstrate our navigation approach in conjunction with very basic retrieval and re-ranking approaches, so as to clearly demonstrate the value of navigation. In common with our approach, Wang et al. [2021] also use a graph version of Wikipedia, but with the goal of aligning it to a knowledge base, rather than navigation. Stammbach and Neumann [2019] propose a scheme for ﬁnding evidence for the FEVER [Thorne et al., 2018] benchmark that explores all links one step away from an initial retrieval set. This can be viewed as a 1-step navigation operation, in contrast to our scheme that permits an arbitrary number of steps. Navigation in knowledge bases: Graph navigation is a key element in utilizing knowledge bases (KB), where facts are represented as tuples of entity nodes connected by a labelled edge corresponding to relation between the two nodes. For example, Das et al. [2017] answer complex queries by navigating the graph, inferring missing edges along the way, and ﬁnding a valid path to the answer node. In our case, the graph is ﬁxed but consists of natural language text and is partially observed. This makes the task much harder on two fronts. First, the policy must comprehend the natural language actions, without access to crisply labelled relation actions. Second, the action space is open and varying in size, unlike in a KB which typically has a ﬁxed schema, i.e. relations/actions come from a ﬁxed size set. Perhaps the closest work involving natural language actions, Fu et al. [2019] augment a KB with textual nodes. However, it still operates under the ﬁxed KB schema. Further improvements on navigation in KB have been shown by incorporating tricks like reward-shaping [Xiong et al., 2017] and action dropout [Lin et al., 2018]. We adopted the action dropout where some outgoing edges are masked during training to enable more effective path exploration. Pushing more on exploration, inspired by AlphaGo, Shen et al. [2018], He et al. [2022] combine the policy with Monte-Carlo tree search to navigate KB. We ﬁnd that no search method is necessary for our approach at evaluation time, with good performance obtained from the policy alone. Internet powered Q&A: Several recent works have attempted complex question answering using the internet. Talmor and Berant [2018] retrieve snippets of information from the web with a search engine to answer a structured decomposition of the question with a simpler Q&A model. The recent WebGPT [Nakano et al., 2021] system takes a broadly similar approach to long-form question answering. WebGPT accures information from the Bing search API as well as some navigation, and passes it to an LM to generate the answer text. The complexity of the system necessitates the use of human demonstrations for training. Related work from Lazaridou et al. [2022] utilizes a few-shot supervision of an LM to perform Google queries, which are then used by the LM to generate an answer. They show that by incorporating information from Google queries, the answers are more factual than those generated directly from LMs. Another line of work, has leveraged link structure of webpages found on the internet to improve Q&A. In LinkBERT [Yasunaga et al., 2022] is pretrained to capture dependencies between documents and results in improvement for for multi-hop reasoning and few-shot reading comprehension. Asai et al. [2020] use the link structure to enhance open domain Q&A by learning to retrieve according to reasoning paths. In contrast to these works, we focus on mastering navigation rather than any combination with search/retrieval, albeit in more limited domain (Wikipedia vs the internet). However, our approach could potentially act as a core component in a general web navigation, being more versatile on account of not requiring supervision. 3 Approach We consider navigation in a web environment W= {N,L}, where N := {ni}N i=1 is the set of nodes, each ni representing a web page with content si. The links between pages are represented by L := {li}N i=1, where li = {l1 i,...,l j i,...,l Mi i }are the set of Mi (directed) out-links from node i. We deﬁne NH(ni) as the set of nodes reached by following all links li from node ni, where the jth neighbor is given by lj i(ni). We assume some target node ng and some starting node n0 are given, and wish to train an agent that can navigate fromn0 to ng by walking along the edges of the graph withinT steps. To accomplish this, we construct a parametric policy pθ(nt+1|nt,ng) which, at time step t, parameterizes a distribution over nt+1 ∈NH(nt). We train this model via behavioral cloning to maximize 3L(θ) = EE(n0,...,nT) [T−1∑ t=0 log pθ(nt+1|nt,ng) ] (1) where E(n0,...,n T) is our trajectory distribution which generates trajectories (n0,...,n T) that we would like our model to replicate (i.e. setting ng = nT). 3.1 Trajectory distribution This framework opens up many choices for E. Clearly some will result in ineffective navigation strategies while others may enable us to train a policy which is capable of generalizing to new graphs and downstream applications. We elaborate on some choices below. Reverse trajectories: We deﬁne E(nT) = Uniform(N) (the uniform distribution over nodes in the graph). Now we can deﬁne E(nt|nt+1) = 1 |{ni; ni ∈NH(nt)}| if nt+1 ∈NH(nt), else 0 (2) or, a distribution which uniformly walks the reverse graph (with transposed adjacency matrix). We begin with some uniformly chosen target node and walk backward randomly to n0 in such a way that we can reach the target by walking forward from n0. Intuitively, training with the reverse trajectories deﬁned above should allow our model to be able to reach any node in the graph from any node it is connected to. Unfortunately, in realistic web graph data, there exist many “dead-ends” or nodes with no in-going edges. The trajectories get stuck at these nodes leading to low-diversity training data. Random forward trajectories: In our real-world knowledge graph data, there are far fewer nodes which have no out-going edges. Thus we propose an alternative method to generate target nodes based on randomly walking forward. We deﬁne E(n0) to be the Uniform(N) and E(nt+1|nt) = 1 |NH(nt)| if nt+1 ∈NH(nt), else 0. (3) or, a distribution which uniformly picks a starting node and randomly walks theforward graph for T steps. Training in this way should encourage our model to be able to reach any node which is reachable from some uniformly chosen start. We note that the target node distribution E(nT) will not be uniform and instead be a function of the graph structure. We also note that while this generates a T-step trajectory, because it is generated by random walk, the shortest path distance between start and target nodes may be smaller. Random shortest paths: We deﬁne E(nT) = Uniform(N) and let E(n0|nT) be the uniform distri- bution of nodes whose shortest path length to nT is T steps. We then deﬁne E(nt|n0,nT) to be the delta distribution that nt is the t-th node of this shortest path. While it can be challenging to compute the probabilities of this distribution efﬁciently we can draw samples from it using a shortest path ﬁnding algorithm such as Dijkstra’s algorithm. 3.2 Model parameterization From above, we can see the core of our model is the pθ(nt+1|nt,nT) distribution. Given a sampled trajectory that begins at n0, this is the distribution which attempts to choose the correct action to take at timestep t+ 1 starting at node nt in order to get to target node nT. We parameterize this distribution as a function of the text available at node nt. pθ(nt+1|nt,ng) ∝exp(fθ(st+1,st,sg,t)) (4) where fθ embeds the text sg, st and st+1 of the target, current and destination nodes, respectively. Further details on how we apply this general model to Wikipedia are given in Section 4.2 and an alternative interpretation of this approach based on variational inference in a latent-variable model is presented in Appendix D. 44 Application to Wikipedia 4.1 Navigation and sentence search tasks Data processing: We convert a snapshot of English Wikipedia into a graph but split each article (¯µ= 1000 words) into paragraph-sized blocks ( ¯µ= 100 words), which form the nodes. Edges in the graph are the union of (i) organic hyperlinks, (ii) additional entity linking and (iii) next/previous paragraph links. We consider two snapshots from year 2017 and 2018, which produces large full graphs with ∼ 37M nodes and ∼ 370M edges. For initial experiments, we also use a smaller subsampled graph, for which we sub-sample disjoint sets of 200k train / 200k evaluation nodes from the 2018 graph. All the obtained graphs are well connected with median path length of 15. Further details have been relegated to the Appendix (full graphs in Appendix A.1, the 200k graph in Appendix A.2, and graph statistics in Appendix A.3). T-step navigation:The most basic task is T-step navigation. At the start of the episode, we randomly sample a start node and generate a T-step random walk trajectory. We then take the last node of this trajectory as the target ng. The agent is then given a time budget B(in most experiments, we ﬁx max steps to B = 100), and succeeds if it reaches the target node within Bsteps. In our experiments, we show results on various graphs and splits with T = {5,10,20}-step navigation. 1-T Multistep navigation: The Multistep task is much like the previous setting, except that instead of generating ﬁxed T-step trajectories, we sample from Uniform(1,T), T = 20. T-step sentence search: This is the same setup as T-step navigation, except that rather than provide the full text for the target nodeng, only one sentence within that text is selected at random to compute the target embeddings from. Thus agents have the more challenging task of locating the correct node, given only a small snippet of its text (see Section 4.2 for details on target representation). We include this task because one of the difﬁculties of web navigation is establishing which page should be the target in the ﬁrst place. It also has direct relevance to our downstream fact veriﬁcation task. 4.2 Model architecture State representation: For each node ni we embed its text si using a Transformer φ(.) to produce φ(si). By basing the state representation on semantic content, rather than i.e. node index, the agent is able to learn about the relationships between entities in a manner that allows for generalization to new graphs at evaluation time, instead of memorizing the structure of the training graph. Follow- ing Karpukhin et al. [2020], we append to the node text the title of the Wikipedia article to which it belongs. This often provides context to the node that might otherwise be missing – in many articles, the subject of the article will be referred to indirectly (e.g. it might state that “she” was born in 1945 instead of repeating the subject’s name). For those experiments where the node embeddings are kept ﬁxed, φ(.) is a pre-trained RoBERTA [Liu et al., 2019] model that encodes the node text & title. However, in our large-scale experiments, we pre-train the Transformer φ(.) directly using the text & title (see Appendix B.1). In both cases, we apply the Transformer to the tokenized text, take the mean over the input tokens, and apply atanh nonlinearity to produce φ(si). Target representation: Similarly, we use the article text sg of the target node as input to the Transformer model φto compute its representation φ(sg). For some tasks, such as sentence search or fact veriﬁcation, the target representation instead relies on a single sentence from each node or other text speciﬁc to the task and a separate transformer model φtarget is trained to embed the target. Navigation policy network: The network operates on the target node ng and the current node nt, and outputs a distribution over the possible actions that can be taken at nt. As the nodes in the graph have a variable number of outgoing edges, the policy must produce a distribution with a variable number of outcomes. To do this, we ﬁrst concatenate the embeddings of sg and st, and pass them through a 1-layer feed-forward network to produce a combined embedding etg = FF[φ(st),φ(sg)]. Next we embed each possible action ai. When we use a ﬁxed φ, we directly use ai = φ(si) for ni ∈NH(nt). However this is not feasible when pre-training our embedding model due to memory concerns. Instead, we compute ai = φ(st[li t]) where s[li] selects the words in swhich belong to hyperlink i– see Appendix B.1 for more details. We then deﬁne the probability of moving to ni from 5nt with target ng as p(ni|nt,ng) ∝exp (etg ·ai) which is normalized to produce a distribution over the |NH(nt)|actions. In our larger-scale model, we also include the trajectory leading up to the current node (n0,...,n t). We pass their embeddings, combined with the target embedding, [φ(s0),...,φ (st),φ(sg)], through a 4-layer transformer and use the output as etg when computing the transition probabilities above. We explore these two policy architecture choices, feed-forward and Transformer in Table 4. 4.3 Downstream tasks Finally, we use our navigation approach to gather information for the tasks of fact veriﬁcation on the FEVER benchmark [Thorne et al., 2018] and question answering on Natural Questions (NQ) [Lee et al., 2019] which also use Wikipedia as a knowledge base. One important difference from our earlier navigation task is that the target node ng is not speciﬁed at evaluation time. Instead, we are given a claim whose veracity must be established or a question that must be answered by ﬁnding information in a particular node in the graph. To determine which node this is, we use a target encoder which maps the claim or question to an embedding vector that can be used in place of φ(sg) in Section 4.2. Due to the limited size of the downstream training sets, we pre-train our graph embeddings, navigation policy and target encoder on the similar sentence search task. We then freeze the embeddings and policy and ﬁne-tune the target encoder on (ng,sg) pairs from the downstream task’s training set. To avoid over-ﬁtting, we add an auxiliary loss which enforces the similarity between our target embedding and the embedding of the ground truth node containing the sentence needed to resolve the claim or question. Training and evaluation on FEVER and NQ required alignment between our version of Wikipedia and the one used to compile the benchmark and details of this can be found in Appendix C.1 and C.2. Next, our navigation method requires a starting node. To put our agent close to the articles we need in the downstream tasks, we run a popular variant of TF-IDF, called BM25 [Robertson and Zaragoza, 2009]. We ﬁrst create an index over all of the nodes in the graph, then take the top-5 BM25 matches as the starting nodes, and run navigation for 20 steps on each. Note that BM25 often does not ﬁnd the exact text (i.e. the hits@1 is not high) but matches are in the right vicinity, usually the same or similar article. Thus the agent only has to navigate a few steps to reach the target: in FEVER and NQ datasets, the mean length of shortest path between start and target node is 4.3 and 5.1 steps respectively. The ﬁnal component is a re-ranker that takes all the sentences from all nodes visited by the agent and assigns a match score between each one and the claim. The ranked list of sentences can then be used for benchmark evaluation. We explore two types of ranker: (a) basic TF-IDF and (b) a transformer based model like BERT or BigBird, ﬁne tuned using hard negatives mined from the TF-IDF ranker. See Appendix C.1 and C.2 for more details. Evaluation of our approach thus consists of: (i) given a claim/question, use BM-25 to return a list of promising start locations; (ii) from the top N = 10 of these, run our navigation model for 20 steps, using the target encoder to embed the claim/question, (iii) use the re-ranker to score sentences visited against the claim/question and (iv) select the top 5 for computing the recall, precision, or F1 score, for comparison on the benchmarks. 5 Experiments 5.1 Investigation of training trajectory distribution On the smaller 200k node graph, we investigate different choices for the trajectory policy distributions Edescribed in Section 3: (i) Random Forward Trajectories, where we randomly sample start nodes and do a random walk to get a target, (ii) Reverse Trajectories, where we randomly sample a target node and and do a random walk on the reverse graph to get a start node, and (iii) Random Shortest Paths where we randomly sample a start node and take a random walk and then compute the shortest path between the start and target nodes. For each of these we trained policies on 5, 10, and 20 step navigation tasks. We evaluate these in two different ways: (i) forward sampling of source and target in the disjoint evaluation graph, (ii) reverse sampling of source and target. The results of this experiment are shown in Table 1. Unsurprisingly, we can see that forward and reverse trained policies both do better when evaluated in the way that matches training. However, forward trajectory policies 6Table 1: Different trajectory distributions on small graph (200k nodes) Navigation (Forward) Navigation (Reverse) Trajectory Policy 5 10 20 5 10 20 Forward 85.3 76.4 67.5 31.1 32.6 19.9 Reverse 2.3 0.5 0.2 91.3 87.9 87.3 Shortest path 86.7 85.0 84.6 74.3 31.6 28.4 generalize much better to reverse navigation. When we analyze the traces of these experiments, we see that in the reverse sampling, many target nodes are in “dead-ends” where many target nodes do not have long random trajectories to sample. This results in easier navigation (as we can see from higher accuracy in reverse trajectory trained policies on reverse navigation) and more extreme overﬁtting, leading to poor performance of reverse trajectory on forward navigation. Not surprisingly, we also see that training on a shortest path trajectories leads to better performance both in the forward and reverse sampling. This advantage is especially seen in the 20-step navigation task. Ideally, we would always train on shortest paths. However, the time complexity of pre-computing all shortest paths in a graph using Dijkstra’s algorithm isO(V(E+ VlogV )) for the number of nodes V and number of edges Ein the graph. On the full 38M Wikipedia graph, this computation would be completely intractable. Luckily, the random forward trajectories perform almost as well and can be computed in constant time with respect to the size of the graph and only linear time with respect to the trajectory length. For all subsequent experiments, we use the Random Forward Trajectories for our trajectory policy and evaluate on the forward version of the navigation tasks, where we call our approach Random Forward Behavioral Cloning (RFBC). 5.2 Navigation We next do a more thorough set of experiments on the smaller200k Wikipedia graph. Because the graph is much smaller than the full Wikipedia and thus prone to overﬁtting, for all of our methods and baselines we use the ﬁxed RoBERTA embeddings φ(si), the simplest 1-layer feed-forward network, and MiniBERT [Turc et al., 2019] for target sentence embedding φ(sg). Alongside our approach, we employ the following baselines/ablations: • RFBC: Random-forward behavioral cloning (our approach). • RFBC + RF: RFBC but with random features sampled from the unit sphere instead of φ • RL + RF: RL with random features. • Random: policy that chooses random action (out-link) at each timestep. • Greedy: selects the action embedding which has the smallest cosine distance with the target. • Random DFS: DFS of depth equal to number of steps, choosing actions at random. • Greedy DFS: DFS but select action with smallest cosine distance to the target. More training details are in Appendix B.2 and computational cost of each in Appendix E. The results evaluated on the held-out graph are shown in columns 1-4 of Table 2. Our approach (RFBC) performs well, achieving 77% in the multistep case. Performance also drops as the distance to the target increases. In contrast, the RL agent performs poorly (around 40% on navigation tasks). Because we generate virtually inﬁnite expert trajectories with RFBC, RL performs worse as it is trained with essentially the same amount of exploration, but with a more sparse signal than behavioral cloning. Using random features in place of φ(.) caused the performance to drop to near chance, showing that the models are utilizing the semantics at each node and are not relying on a general search strategy of some kind. Random methods also do quite poorly, even when adding a DFS. Greedy methods do a bit better than random, especially with DFS, but still well below our method. We also consider the more challenging sentence search task described in Section 4.1. This task requires not only ﬁnding a known target node, but learning to ﬁnd a target node given only a single random sentence from that node, more closely matching our downstream task and the way humans tend to approach ﬁnding information on the web. Columns 5-7 of Table 2 show performance on this task for our approach and RL. Our method has reduced performance on the task due to the extra difﬁculty of learning the target embedding, but still performs reasonably. RL, however, degrades to near chance performance. 7Table 2: Small graph navigation (200k nodes) – Success rate (%) Navigation Sentence Search Method 5 10 20 multistep 5 10 20 RFBC (ours) 85.3 76.4 67.5 77.4 60.7 47.6 34.6 RL 41.4 40.2 41.6 43.6 14.0 1.5 8.9 RFBC + RL (ours) 85.1 77.6 68.1 78.3 - - - RFBC + RF 17.4 16.3 16.6 17.1 - - - RL + RF 6.0 7.5 7.0 6.7 - - - Random 12.3 14.9 12.3 14.0 - - - Greedy 19.7 16.7 21.7 23.4 - - - Random DFS 10.0 9.5 8.3 10.0 - - - Greedy DFS 31.1 23.8 22.7 51.8 - - - Table 3: Wikipedia graph statistics Year # Articles # Nodes # Edges # Words / Node Median Path Length 2017 4.92M 36.3M 359M 110 15 2018 5.27M 38.5M 387M 109 15 5.3 Navigation on full 38M node Wikipedia graph: Our method scales naturally to much larger graphs, which we demonstrate by training on the entire Wikipedia 2017 graph. We evaluate on the entire Wikipedia 2018 graph. Each graph is ∼37M nodes and ∼370M edges (see Table 3 for details). We would like to point out there is signiﬁcant evolution (difference) between the 2017 and 2018 graph as analysed in Appendix A.3 along with further statistics. We explore both architectures described in Section 4.2 for the policy network: (i) the single feed- forward layer (as used in the smaller graph) and (ii) the 4-layer Transformer model. We also compare using a ﬁxed RoBERTA model for φversus pre-training a text transformer directly on the navigation task (see Appendix B.3). DistillBERT [Sanh et al., 2019] is used for φ(ng). Table 4 shows the results on the full graph, evaluating on{5,10,20}step tasks, for both navigation and sentence search. Figure 2 shows example trajectories of the trained agent. The ﬁrst thing to note is that the overall navigation and sentence search performance of our methods is high. The 2018 Wikipedia graph we use for evaluation contains over 5 million articles, over 38 million nodes and 387 million edges. On this graph (which we generalize to from the previous year’s graph) our best method can ﬁnd an article 20 steps away 92.2% percent of the time and can locate it given just a single sentence of that article 90.2% of the time. This compares favorably with a similar evaluation performed in Nogueira and Cho [2016], where their WebNav approach achieved a12.5% success rate for 16 step tasks on a 12M node graph. Compared to the performance of our method on the smaller 200k graph, we see that performance has greatly improved, most likely due to the addition of orders of magnitude more training data. In particular we see that sentence search performance on 20-step improves greatly from 34.6% to a best performance of 90.2%. Additionally, in all cases, our learned embeddings perform better than using the ﬁxed features, derived from large language models. In general, feed-forward policies perform best, except for 20 step navigation tasks. For longer trajectories, the Transformer policy network achieves slightly better performance, while on sentence search, the feed-forward model is consistently superior. Table 4: Full graph navigation (38M nodes) - Success rate (%) Navigation Sentence Search Embedding + Policy 5 10 20 5 10 20 RoBERTA + Feed-forward 85.5 80.1 71.5 91.2 88.9 77.5 RoBERTA + Transformer 85.3 88.3 87.9 81.6 75.4 70.9 Embed train (ours) + Feed-forward 96.1 94.1 89.8 96.3 92.8 90.2 Embed train (ours) + Transformer 93.5 90.6 92.2 93.9 86.3 79.7 85.4 Application to fact veriﬁcation We now evaluate the ability of our navigation approach to select evidence on the FEVER development set [Thorne et al., 2018] with results shown in Table 5. The primary metric used is F1@top5 but we also give precision and recall. Paired with a basic retrieval (BM25) and re-ranker (TF-IDF), the approach obtains a respectable F1 of 0.46. If we remove our navigation component then this drops substantially to 0.36, demonstrating its utility. Swapping to a more powerful re-ranker (BigBird) boosts the F1 to 0.731, which is competitive with the state-of-the-art (FEVER leaderboard [2022] rank #6; rank #1 F1 is 0.799). We also compare to a leading approach [Stammbach, 2021] that similarly relies on the BigBird re-ranker, with our approach having a signiﬁcantly better F1 score. Table 5: Results on the FEVER benchmark (evidence retrieval only). The ﬁrst two rows show the effect of adding our navigation scheme to the simple BM25 retrieval and TF-IDF re-ranker combination. Switching to a more powerful re-ranker (BigBird) results in a signiﬁcant boost in F1 score, surpassing Stammbach [2021] who also use BigBird [Zaheer et al., 2020]. Method Precision@5 Recall@5 F1@5 BM25 + TF-IDF [Thorne et al., 2018] 0.33 0.40 0.36 BM25 + RFBC (Ours) + TF-IDF 0.38 0.55 0.46 BM25 + RFBC (Ours) + BigBird 0.71 0.75 0.73 Stammbach [2021] 0.26 0.94 0.41 5LVHOGD\u00036HODM :DOORQLD .H\\\u0003FHUHPRQ\\ 9ODGLPLU\u00033DGZD \u0018\u0019WK\u00037UDLQLQJ\u00036TXDGURQ 0\\FHQDHDQ\u0003*UHHFH )LHOG\u0003\u000bPDWKHPDWLFV\f 0HWDSK\\VLFV 5HSURGXFWLRQ :RUG \u0013\u001d\u00035LVHOGD\u00036HODM \u0014\u001d\u0003&KDPSLRQ \u0015\u001d\u0003/DWLQ \u0016\u001d\u0003*UHHFH \u0017\u001d\u0003*UHHFH \u0018\u001d\u0003$WKHQV \u0019\u001d\u0003$WWLFD \u001a\u001d\u0003$QFLHQW\u0003*UHHN \u001b\u001d\u0003$QFLHQW\u0003*UHHN \u001c\u001d\u00030HGLHYDO\u0003*UHHN \u0014\u0013\u001d\u00030HGLHYDO\u0003*UHHN \u0014\u0014\u001d\u00032WWRPDQ\u0003(PSLUH \u0014\u0015\u001d\u0003$QDWROLD \u0014\u0016\u001d\u00030HGLHYDO\u0003*UHHN \u0014\u0017\u001d\u00030HGLHYDO\u0003*UHHN \u0014\u0018\u001d\u0003/DQJXDJH \u0014\u0019\u001d\u0003$QFLHQW\u0003*UHHFH \u0014\u001a\u001d\u0003$UFKDLF\u0003*UHHFH \u0014\u001b\u001d\u00036HFRQG\u00033HUVLDQ\u0003LQYDVLRQ\u0003RI\u0003*UHHFH \u0014\u001c\u001d\u0003$QFLHQW\u0003*UHHFH \u0015\u0013\u001d\u0003$QFLHQW\u0003*UHHFH \u0013\u001d\u0003:DOORQLD\u0003\u000b\u0015\u0013\f \u0014\u001d\u0003&KHPLVWU\\ \u0015\u001d\u00033URSHUW\\\u0003\u000bSKLORVRSK\\\f \u0016\u001d\u0003([WHQVLRQDOLW\\ \u0017\u001d\u0003([WHQVLRQDOLW\\\u0003\u000b\u0014\f \u0018\u001d\u0003([WHQVLRQDOLW\\ \u0019\u001d\u0003([WHQVLRQDOLW\\ \u001a\u001d\u0003([WHQVLRQDOLW\\ \u001b\u001d\u0003(TXDOLW\\\u0003\u000bPDWKHPDWLFV\f \u001c\u001d\u00034XDQWLW\\ \u0014\u0013\u001d\u00030DJQLWXGH\u0003\u000bPDWKHPDWLFV\f \u0014\u0014\u001d\u00030DJQLWXGH\u0003\u000bPDWKHPDWLFV\f \u0014\u0015\u001d\u00030DWKHPDWLFDO\u0003REMHFW \u0014\u0016\u001d\u0003)LHOG\u0003\u000bPDWKHPDWLFV\f \u0014\u0017\u001d\u0003$GGLWLRQ \u0014\u0018\u001d\u0003$GGLWLRQ \u0014\u0019\u001d\u00032SHUDWLRQ\u0003\u000bPDWKHPDWLFV\f \u0014\u001a\u001d\u00032SHUDQG \u0014\u001b\u001d\u00032SHUDWRU\u0003\u000bPDWKHPDWLFV\f \u0014\u001c\u001d\u00036SDFH\u0003\u000bPDWKHPDWLFV\f \u0015\u0013\u001d\u00030DWKHPDWLFDO\u0003VWUXFWXUH \u0013\u001d\u0003.H\\\u0003FHUHPRQ\\\u0003\u000b\u0017\f \u0014\u001d\u0003&RPPXQLFDWLRQ \u0015\u001d\u00036\\PERO \u0016\u001d\u0003%HOLHI \u0017\u001d\u00033LVWLV \u0018\u001d\u0003%HOLHI \u0019\u001d\u0003&RQFHSW \u001a\u001d\u00037KRXJKW \u001b\u001d\u0003*RDO \u001c\u001d\u0003,GHD \u0014\u0013\u001d\u00033KLORVRSKHU \u0014\u0014\u001d\u00030HDQLQJ\u0003RI\u0003OLIH \u0014\u0015\u001d\u00030HWDSK\\VLFV \u0014\u0016\u001d\u00033RWHQWLDOLW\\\u0003DQG\u0003DFWXDOLW\\ \u0014\u0017\u001d\u0003)RXU\u0003FDXVHV \u0014\u0018\u001d\u0003)RXU\u0003FDXVHV \u0014\u0019\u001d\u0003([SODQDWLRQ \u0014\u001a\u001d\u0003)DFW \u0014\u001b\u001d\u00035HDOLW\\ \u0014\u001c\u001d\u00033KLORVRSK\\ \u0015\u0013\u001d\u00033KLORVRSK\\ \u0013\u001d\u00039ODGLPLU\u00033DGZD\u0003\u000b\u0018\f \u0014\u001d\u00033ODQ \u0015\u001d\u0003%XVLQHVV \u0016\u001d\u00033URGXFW\u0003\u000bEXVLQHVV\f \u0017\u001d\u00036\\VWHP \u0018\u001d\u00036\\VWHP\u0003\u000b\u0014\f \u0019\u001d\u00036\\VWHP\u0003\u000b\u0015\f \u001a\u001d\u0003%LRORJLVW \u001b\u001d\u0003/LIH \u001c\u001d\u0003&HOO\u0003VLJQDOLQJ \u0014\u0013\u001d\u0003'LVHDVH \u0014\u0014\u001d\u0003)XQFWLRQ\u0003\u000bELRORJ\\\f \u0014\u0015\u001d\u0003)LWQHVV\u0003\u000bELRORJ\\\f \u0014\u0016\u001d\u00036H[XDO\u0003VHOHFWLRQ \u0014\u0017\u001d\u00035HSURGXFWLYH\u0003VXFFHVV \u0014\u0018\u001d\u0003*HQRW\\SH \u0014\u0019\u001d\u0003*HQRW\\SH \u0014\u001a\u001d\u0003*HQH \u0014\u001b\u001d\u0003%LRORJ\\ \u0014\u001c\u001d\u0003&HOO\u0003\u000bELRORJ\\\f \u0015\u0013\u001d\u0003(PEU\\R \u0013\u001d\u0003\u0018\u0019WK\u00037UDLQLQJ\u00036TXDGURQ\u0003\u000b\u0014\u0014\f \u0014\u001d\u0003\u0018\u0019WK\u00037UDLQLQJ\u00036TXDGURQ\u0003\u000b\u0014\u0014\f \u0015\u001d\u0003\u0018\u0019WK\u00037UDLQLQJ\u00036TXDGURQ\u0003\u000b\u0014\u0014\f \u0016\u001d\u0003\u0018\u0019WK\u00037UDLQLQJ\u00036TXDGURQ\u0003\u000b\u0014\u0014\f \u0017\u001d\u0003\u0018\u0019WK\u00037UDLQLQJ\u00036TXDGURQ\u0003\u000b\u0014\u0013\f \u0018\u001d\u00036TXDGURQ\u0003\u000bQDYDO\f \u0019\u001d\u0003:DUVKLS \u001a\u001d\u00031DY\\ \u001b\u001d\u00033RZHU\u0003SURMHFWLRQ \u001c\u001d\u00033RZHU\u0003SURMHFWLRQ \u0014\u0013\u001d\u00036RYHUHLJQ\u0003VWDWH \u0014\u0014\u001d\u0003&HQWUDOLVDWLRQ \u0014\u0015\u001d\u0003)RUPV\u0003RI\u0003JRYHUQPHQW \u0014\u0016\u001d\u0003'HILQLWLRQ \u0014\u0017\u001d\u0003:RUG \u0014\u0018\u001d\u0003:RUG \u0014\u0019\u001d\u0003:RUG \u0014\u001a\u001d\u0003:RUG \u0014\u001b\u001d\u00030RUSKHPH \u0014\u001c\u001d\u0003:RUG \u0015\u0013\u001d\u0003:RUG ([DPSOH\u001d\u0003\u0013 5LVHOGD\u00036HODM 0\\FHQDHDQ\u0003*UHHFH \u0013\u001d\u00035LVHOGD\u00036HODM \u0014\u001d\u0003&KDPSLRQ \u0015\u001d\u0003/DWLQ \u0016\u001d\u0003*UHHFH \u0017\u001d\u0003*UHHFH \u0018\u001d\u0003$WKHQV \u0019\u001d\u0003$WWLFD \u001a\u001d\u0003$QFLHQW\u0003*UHHN \u001b\u001d\u0003$QFLHQW\u0003*UHHN \u001c\u001d\u00030HGLHYDO\u0003*UHHN \u0014\u0013\u001d\u00030HGLHYDO\u0003*UHHN \u0014\u0014\u001d\u00032WWRPDQ\u0003(PSLUH \u0014\u0015\u001d\u0003$QDWROLD \u0014\u0016\u001d\u00030HGLHYDO\u0003*UHHN \u0014\u0017\u001d\u00030HGLHYDO\u0003*UHHN \u0014\u0018\u001d\u0003/DQJXDJH \u0014\u0019\u001d\u0003$QFLHQW\u0003*UHHFH \u0014\u001a\u001d\u0003$UFKDLF\u0003*UHHFH \u0014\u001b\u001d\u00036HFRQG\u00033HUVLDQ\u0003LQYDVLRQ\u0003RI\u0003*UHHFH \u0014\u001c\u001d\u0003$QFLHQW\u0003*UHHFH \u0015\u0013\u001d\u0003$QFLHQW\u0003*UHHFH ([DPSOH\u001d\u0003\u0014 :DOORQLD Figure 2: Example navigation trajectories in the 38M node Wikipedia graph. The start and target nodes are shown in the ﬁrst two rows. Parentheses indicate paragraph block (zero-indexed) within article. Note that in cases where the agent fails to ﬁnd the target node (cols 1 & 3), it visits ones that are very close by. 7KH\u0003;\u0010)LOHV\u0003VWDUUHG\u0003DQ\u0003DFWUHVV\u00116RODQXP\u0003FRQWDLQV\u0003WKH\u0003KRUVH QHWWOHV\u0011 &+L3V\u0003ZDV ZULWWHQ\u0003E\\\u0003'D[ 6KHSDUG\u0003LQ\u0003\u0015\u0013\u0014\u0014\u0011 %URZQ\u00038QLYHUVLW\\ V\u0003PDLQ FDPSXV\u0003LV\u0003ORFDWHG\u0003LQ\u0003WKH PRVW\u0003SRSXORXV\u0003FLW\\\u0003LQ\u00035KRGH ,VODQG\u0011 7KH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003IHDWXUHG\u0003RQH EXLOGLQJ\u0011 7KH\u0003;\u0010)LOHV /LVW\u0003RI\u0003SODQWV\u0003NQRZQ\u0003DV\u0003QHWWOH'D[\u00036KHSDUG -RKQ\u0003%URZQ\u0003\u000b5KRGH\u0003,VODQG\f2QH\u0003:RUOG\u00037UDGH\u0003&HQWHU *LOOLDQ\u0003$QGHUVRQ 6RODQXP &+L3V\u0003\u000bILOP\f 3URYLGHQFH\u000f\u00035KRGH\u0003,VODQG:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0014\u001c\u001a\u0016°\u0015\u0013\u0013\u0014\f ,QWURGXFWLRQ\u0011\u0003*LOOLDQ\u0003/HLJK\u0003$QGHUVRQ\u000f 2%(\u0003\u000bERUQ\u0003$XJXVW\u0003\u001c\u000f\u0003\u0014\u001c\u0019\u001b\f\u000f\u0003LV\u0003DQ $PHULFDQ\u0010%ULWLVK\u0003ILOP\u000f\u0003WHOHYLVLRQ\u0003DQG WKHDWUH\u0003DFWUHVV\u000f\u0003DFWLYLVW\u0003DQG\u0003ZULWHU\u0011 +HU\u0003FUHGLWV\u0003LQFOXGH\u0003WKH\u0003UROHV\u0003RI\u0003)%, 6SHFLDO\u0003$JHQW\u0003'DQD\u00036FXOO\\\u0003LQ\u0003WKH ORQJ\u0010UXQQLQJ\u0003DQG\u0003ZLGHO\\\u0003SRSXODU\u0003VHULHV \u00057KH\u0003;\u0010)LOHV\u0005\u000f 6RODQXP\u0003LV\u0003D\u0003ODUJH\u0003DQG\u0003GLYHUVH *HQXV\u0003RI\u0003)ORZHULQJ\u0003SODQWV\u000f\u0003ZKLFK LQFOXGH\u0003WZR\u0003IRRG\u0003FURSV\u0003RI\u0003KLJK HFRQRPLF\u0003LPSRUWDQFH\u000f\u0003WKH\u00033RWDWR\u0003DQG WKH\u00037RPDWR\u0011\u0003,W\u0003DOVR\u0003FRQWDLQV\u0003WKH QLJKWVKDGHV\u0003DQG\u0003KRUVH\u0003QHWWOHV\u000f\u0011\u0011\u0011 &+L3V\u0003LV\u0003D\u0003\u0015\u0013\u0014\u001a $PHULFDQ\u0003DFWLRQ FRPHG\\\u0003%XGG\\\u0003FRS ILOP\u0003ZULWWHQ\u0003DQG GLUHFWHG\u0003E\\\u0003'D[ 6KHSDUG\u000f\u0003ª\u0011 3URYLGHQFH\u0003LV\u0003WKH\u0003FDSLWDO\u0003RI\u0003DQG PRVW\u0003SRSXORXV\u0003FLW\\\u0003LQ\u0003WKH\u00038\u00116\u0011 VWDWH\u0003RI\u00035KRGH\u0003,VODQG\u000f\u0003IRXQGHG LQ\u0003\u0014\u0019\u0016\u0019\u000f\u0003DQG\u0003RQH\u0003RI\u0003WKH\u0003ROGHVW FLWLHV\u0003LQ\u0003WKH\u00038QLWHG\u00036WDWHV\u0011\u0003,W LV\u0003ORFDWHG\u0003LQ\u00033URYLGHQFH\u0003&RXQW\\ DQG\u0003LV\u0003WKH\u0003WKLUG\u0003PRVW\u0003SRSXORXV FLW\\\u0003LQ\u00031HZ\u0003(QJODQGª 7KH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003ZDV\u0003D\u0003ODUJH\u0003FRPSOH[\u0003RI VHYHQ\u0003EXLOGLQJV\u0003LQ\u0003/RZHU\u00030DQKDWWDQ\u000f\u00031HZ\u0003<RUN &LW\\\u000f\u00038QLWHG\u00036WDWHV\u0011\u0003,W\u0003IHDWXUHG\u0003ODQGPDUN\u0003WZLQ WRZHUV\u000f\u0003ZKLFK\u0003RSHQHG\u0003RQ\u0003$SULO\u0003\u0017\u000f\u0003\u0014\u001c\u001a\u0016\u000f\u0003DQG\u0003ZHUH GHVWUR\\HG\u0003DV\u0003D\u0003UHVXOW\u0003RI\u0003WKH\u00036HSWHPEHU\u0003\u0014\u0014 DWWDFNV\u0011\u0003$W\u0003WKH\u0003WLPH\u0003RI\u0003WKHLU\u0003FRPSOHWLRQ\u000f\u0003WKH \u00057ZLQ\u00037RZHUV\u0005±WKH\u0003RULJLQDO\u0003\u0014\u0003:RUOG\u00037UDGH &HQWHU\u000f\u0003DW\u0003\u001e\u0003DQG\u0003\u0015\u0003:RUOG\u00037UDGH\u0003&HQWHU\u000f\u0003DW\u0003±ZHUH WKH\u0003WDOOHVW\u0003EXLOGLQJV\u0003LQ\u0003WKH\u0003ZRUOGª\u0011 \u0013\u001d\u00037KH\u0003;\u0010)LOHV\u0003\u000b\u001b\u0017\f \u0014\u001d\u0003+XPEXJ\u0003\u000b7KH\u0003;\u0010)LOHV\f \u0015\u001d\u0003+XPEXJ\u0003\u000b7KH\u0003;\u0010)LOHV\f \u0016\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0017\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0018\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0019\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u001a\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u001b\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u001c\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0014\u0013\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0014\u0014\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0014\u0015\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0014\u0016\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0014\u0017\u001d\u0003*LOOLDQ\u0003$QGHUVRQ \u0013\u001d\u0003/LVW\u0003RI\u0003SODQWV\u0003NQRZQ\u0003DV QHWWOH\u0003\u000b\u0014\f \u0014\u001d\u0003/LVW\u0003RI\u0003SODQWV\u0003NQRZQ\u0003DV QHWWOH\u0003\u000b\u0014\f \u0015\u001d\u0003/LVW\u0003RI\u0003SODQWV\u0003NQRZQ\u0003DV QHWWOH\u0003\u000b\u0014\f \u0016\u001d\u0003/LVW\u0003RI\u0003SODQWV\u0003NQRZQ\u0003DV QHWWOH \u0017\u001d\u0003/LVW\u0003RI\u0003SODQWV\u0003NQRZQ\u0003DV QHWWOH\u0003\u000b\u0014\f \u0018\u001d\u0003/LVW\u0003RI\u0003SODQWV\u0003NQRZQ\u0003DV QHWWOH\u0003\u000b\u0014\f \u0019\u001d\u00036RODQXP\u0003URVWUDWXP \u001a\u001d\u00036RODQXP \u0013\u001d\u0003'D[\u00036KHSDUG \u0014\u001d\u0003'D[\u00036KHSDUG \u0015\u001d\u0003'D[\u00036KHSDUG \u0016\u001d\u0003'D[\u00036KHSDUG \u0017\u001d\u0003'D[\u00036KHSDUG \u0018\u001d\u0003&+L3V\u0003\u000bILOP\f \u0019\u001d\u0003&+L3V\u0003\u000bILOP\f \u001a\u001d\u0003&+L3V\u0003\u000bILOP\f \u001b\u001d\u0003&+L3V\u0003\u000bILOP\f \u001c\u001d\u0003&+L3V\u0003\u000bILOP\f \u0014\u0013\u001d\u0003'D[\u00036KHSDUG \u0014\u0014\u001d\u0003'D[\u00036KHSDUG \u0014\u0015\u001d\u0003'D[\u00036KHSDUG \u0014\u0016\u001d\u0003'D[\u00036KHSDUG \u0014\u0017\u001d\u0003'D[\u00036KHSDUG \u0013\u001d\u0003-RKQ\u0003%URZQ\u0003\u000b5KRGH\u0003,VODQG\f \u0014\u001d\u0003%URZQ\u00038QLYHUVLW\\ \u0015\u001d\u0003%URZQ\u00038QLYHUVLW\\ \u0016\u001d\u0003%URZQ\u00038QLYHUVLW\\ \u0017\u001d\u0003%URZQ\u00038QLYHUVLW\\ \u0018\u001d\u0003%URZQ\u00038QLYHUVLW\\ \u0019\u001d\u0003%URZQ\u00038QLYHUVLW\\ \u001a\u001d\u00033URYLGHQFH\u000f\u00035KRGH\u0003,VODQG \u001b\u001d\u00033URYLGHQFH\u000f\u00035KRGH\u0003,VODQG \u001c\u001d\u00033URYLGHQFH\u000f\u00035KRGH\u0003,VODQG \u0014\u0013\u001d\u00033URYLGHQFH\u000f\u00035KRGH\u0003,VODQG \u0014\u0014\u001d\u00033URYLGHQFH\u000f\u00035KRGH\u0003,VODQG \u0014\u0015\u001d\u00033URYLGHQFH\u000f\u00035KRGH\u0003,VODQG \u0014\u0016\u001d\u00033URYLGHQFH\u000f\u00035KRGH\u0003,VODQG \u0014\u0017\u001d\u00033URYLGHQFH\u000f\u00035KRGH\u0003,VODQG \u0013\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU \u0014\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0014\f \u0015\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0014\f \u0016\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0014\f \u0017\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0014\f \u0018\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0015\f \u0019\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0015\f \u001a\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0015\f \u001b\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0016\f \u001c\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0016\f \u0014\u0013\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0016\f \u0014\u0014\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0016\f \u0014\u0015\u001d\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0014\u001c\u001a\u0016°\u0015\u0013\u0013\u0014\f \u0014\u0016\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU \u0014\u0017\u001d\u00032QH\u0003:RUOG\u00037UDGH\u0003&HQWHU\u0003\u000b\u0014\f Figure 3: Successful navigation trajectories on the FEVER [Thorne et al., 2018] evaluation set. Rows from top: claim to be veriﬁed; starting node (from BM25); ground truth target (not visible to agent); target node text; agent trajectory. 9A possible explanation is that their approach overwhelms the re-ranker with all possible hyperlinks whereas, navigation being inherently sparse, our model presents a more reﬁned set for re-ranking. We note that our approach: (a) is signiﬁcantly simpler than many top ranked approaches and (b) selects evidence over a much larger set than the curated version of Wikipedia used in FEVER (38M vs 5M). Examples of successful navigation traces are shown in Figure 3. 5.5 Application to question answering Finally, we present results on another downstream task where navigation helps. In particular, we consider the task of ﬁnding correct evidence passage for open domain question answering. We use the Natural Question (NQ) open-domain dataset presented in Lee et al. [2019]. Since we target navigating to the exact evidence passage required to answer the question, we use recall@{1,2,3,4,5} for ﬁnding the gold evidence passage as our metric. More commonly the metric marks a retrieved passage to be correct if it contains the answer string, but this causes a lot of false positives, e.g. the answer string appears in a totally irrelevant context. Table 6: Results on the Natural Questions open domain benchmark (evidence retrieval only). Third and fourth rows show the effect of adding our navigation scheme to the simple BM25 retrieval and BigBird re-ranker. First row is a reference re-training of state-of-the-art method on our setup. Recall@1 Recall@2 Recall@3 Recall@4 Recall@5 RocketQA [Qu et al., 2021] 32.7 43.8 51.7 58.3 62.5 BM25 11.1 17.3 21.8 24.9 27.3 BM25 + BigBird ReRank 22.2 30.2 35.1 38.7 41.2 BM25 + RFBC (Ours) + BigBird ReRank 31.6 41.3 46.6 49.8 51.6 Improvement +42.3% +36.7% +32.7% +28.7% +25.2% The results are tabulated in Table 6. Paired with a basic retrieval (BM25) and re-ranker (BigBird), the approach obtains a respectable Recall@1 of 31.6. If we remove our navigation component then this drops substantially to 22.2, demonstrating its utility. As a reference, we also ran a state-of-the-art system RocketQA [Qu et al., 2021] on our setup and evaluated it using our harder metric. It is worthwhile to note that our approach (a) is signiﬁcantly simpler than state-of-the-art approaches like RocketQA which employ 4 stages of dual-encoder and cross-attention models, and (b) selects evidence over a larger set than the commonly used preprocess Wikipedia passages (38M vs 21M). 6 Discussion We have presented a simple and effective scheme for navigating a large Wikipedia graph that is applicable to more general web navigation problems. We show that behavioral cloning of random trajectories is a viable approach to learning both entity embeddings and a navigation policy. When applied to the fact veriﬁcation task on FEVER dataset and the open-domain question answering task on NQ dataset, they offer highly competitive performance, whilst being complementary to existing approaches. Another advantage worth highlighting is that the navigating agent provides a provenance on how it arrived at the relevant evidence, which many other methods (like dense passage retrieval) do not provide. One limitation of our approach when we move from Wikipedia to the wider Internet is that our scheme relies on a good target encoder. For navigation and our downstream tasks, there was a clear ground-truth target node available training, but in other settings this might not be the case. Another limitation is that we require a re-ranker to decide on the ﬁnal retrieved sentence, but ideally the agent would decide for itself when it has reached the correct node. References A. Asai, K. Hashimoto, H. Hajishirzi, R. Socher, and C. Xiong. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJgVHkrYDH. D. Chen, A. Fisch, J. Weston, and A. Bordes. Reading wikipedia to answer open-domain questions. CoRR, abs/1704.00051, 2017. 10H. Dai, R. Singh, B. Dai, C. Sutton, and D. Schuurmans. Learning discrete energy-based models via auxiliary-variable local exploration. Advances in Neural Information Processing Systems, 2020. R. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar, A. Krishnamurthy, A. J. Smola, and A. McCallum. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. CoRR, abs/1711.05851, 2017. L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V . Mnih, T. Ward, Y . Doron, V . Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. ArXiv, abs/1802.01561, 2018. C. Fu, T. Chen, M. Qu, W. Jin, and X. Ren. Collaborative policy learning for open knowledge graph reasoning. CoRR, abs/1909.00230, 2019. L. He, B. Shao, Y . Xiao, Y . Li, T.-Y . Liu, E. Chen, and H. Xia. Neurally-guided semantic navigation in knowledge graph. IEEE Transactions on Big Data, 8(3):607–615, 2022. J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. S. Hofstätter, S. Althammer, M. Schröder, M. Sertkan, and A. Hanbury. Improving efﬁcient neural ranking models with cross-architecture knowledge distillation. CoRR, abs/2010.02666, 2020. V . Karpukhin, B. Oguz, S. Min, L. Wu, S. Edunov, D. Chen, and W. Yih. Dense passage retrieval for open-domain question answering. CoRR, abs/2004.04906, 2020. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. D. P. Kingma and M. Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering, 2022. F. leaderboard. FEVER development set leaderboard. https://competitions.codalab.org/ competitions/18814#results, 2022. K. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, 01 2019. doi: 10.18653/v1/P19-1612. X. V . Lin, R. Socher, and C. Xiong. Multi-hop knowledge graph reasoning with reward shaping. CoRR, abs/1808.10568, 2018. Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov. Roberta: A robustly optimized BERT pretraining approach.CoRR, abs/1907.11692, 2019. R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V . Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021. R. Nogueira and K. Cho. Webnav: A new large-scale task for natural language based sequential decision making. CoRR, abs/1602.02261, 2016. Y . Qu, Y . Ding, J. Liu, K. Liu, R. Ren, X. Zhao, D. Dong, H. Wu, and H. Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering.CoRR, abs/2010.08191, 2020. Y . Qu, Y . Ding, J. Liu, K. Liu, R. Ren, W. X. Zhao, D. Dong, H. Wu, and H. Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835–5847, 2021. 11S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Foun- dations and Trends ® in Information Retrieval , 3(4):333–389, 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL http://dx.doi.org/10.1561/1500000019. V . Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108, 2019. S. J. Semnani and M. Pandey. Revisiting the open-domain question answering pipeline. CoRR, abs/2009.00914, 2020. Y . Shen, J. Chen, P.-S. Huang, Y . Guo, and J. Gao. M-walk: Learning to walk over graphs using monte carlo tree search. Advances in Neural Information Processing Systems, 31, 2018. D. Stammbach. Evidence selection as a token-level prediction task. In Proceedings of the Fourth Workshop on Fact Extraction and VERiﬁcation (FEVER) , pages 14–20, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.fever-1.2. URL https://aclanthology.org/2021.fever-1.2. D. Stammbach and G. Neumann. Team DOMLIN: Exploiting evidence enhancement for the FEVER shared task. In Proceedings of the Second Workshop on Fact Extraction and VERiﬁcation (FEVER), pages 105–109, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-6616. URL https://aclanthology.org/D19-6616. A. Talmor and J. Berant. The web as a knowledge-base for answering complex questions. CoRR, abs/1803.06643, 2018. J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal. FEVER: a large-scale dataset for fact extraction and VERiﬁcation. In Association for Computational Linguistics; dataset released under Apache license., June 2018. I. Turc, M. Chang, K. Lee, and K. Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. CoRR, abs/1908.08962, 2019. URL http: //arxiv.org/abs/1908.08962. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. L. Wang, Y . Li, Ö. Aslan, and O. Vinyals. Wikigraphs: A wikipedia text - knowledge graph paired dataset. CoRR, abs/2107.09556, 2021. R. West and J. Leskovec. Human wayﬁnding in information networks. In WWW, page 619–628, 2012a. R. West and J. Leskovec. Automatic versus human navigation in information networks. In ICWSM, 2012b. R. West, J. Pineau, and D. Precup. Wikispeedia: An online game for inferring semantic distances be- tween concepts. In Proceedings of the 21st International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2009. W. Xiong, T. Hoang, and W. Y . Wang. Deeppath: A reinforcement learning method for knowledge graph reasoning. CoRR, abs/1707.06690, 2017. M. Yasunaga, J. Leskovec, and P. Liang. Linkbert: Pretraining language models with document links. CoRR, abs/2203.15827, 2022. Q. Ye, B. Wu, and B. Wang. Distance distribution and average shortest path length estimation in real-world networks. In International Conference on Advanced Data Mining and Applications, pages 322–333. Springer, 2010. M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297, 2020. 12A Data / Environment details A.1 Wikipedia data processing We start by downloading English Wikipedia snapshot (the pages-articles.xml.bz2 ﬁle) from Wiki- media3 or Internet Archive.4 We extract text from English Wikipedia for a given snapshot using Gensim’s WikiCorpus class.5 For each page, this tool extracts the plain text and hyperlinks, and strips out all structured data such as lists and ﬁgures. To avoid pages without sufﬁcient textual content, we ﬁlter out categories, listical, disambiguation pages and any other page with less than 200 characters. Each article (µ = 1000 words) is split into paragraph-sized blocks (µ = 100 words), which form the nodes in our Wikipedia Graph. This was done both for conceptual reasons (humans don’t read an entire article at once but look at bits and pieces of it at one go) and for modeling reasons (most language models have an upper input token length that is smaller than the average Wikipedia page). The edges in the graph are formed in three ways: 1. for nodes in the same article, we add “previous node” and “next node” links to connect them in a chain; 2. organic hyperlinks – the internal links connecting Wikipedia pages to each other; and 3. we additionally run entity linking6 because repeat mentions of an entity in Wikipedia lack hyperlinks, which is problematic when articles are split up. This constitutes our Wikipedia graph construction, where the text blocks act as vertices and the organic hyperlinks along with entity links are the edges. We store the graph as an adjacency list with metadata, using a memory mapped key-value data-structure to enable fast random access during navigation. We run the above graph generation pipeline for two different snapshots of Wikipedia: 1. June 01 2017 7: This corresponds to FEVER fact veriﬁcation dataset. 2. December 2018 8: This corresponds to Natural Questions open dataset. A.2 Smaller 200k node graph construction We construct smaller train and eval graphs with 200k nodes each for the following reasons: 1. Checking strict generalization by making train and eval graphs to be totally disjoint, i.e. no common nodes or edges. The full 2017 and 2018 are sufﬁciently different but not entirely disjoint. 2. Running experiments on full graphs are expensive, so for more thorough evaluation across multiple baselines we constructed the smaller graphs. To construct these two smaller graphs, we start with the full 2018 graph. We sort the nodes by their in-degrees. We mark nodes with odd ranks to be in train and even ranks to be in eval, which ensures complete separation between the two. For the train graph we start with rank 1 node (and for eval with rank 2 node) and then select all its neighbors which have odd rank (even rank). Then all the odd (even) nodes connected to the selected ones are picked. This process is continued until the desired number of nodes are selected. 3https://dumps.wikimedia.org/enwiki/ 4https://archive.org/search.php?query=Wikimedia%20database%20dump%20of%20the% 20English%20Wikipedia 5https://github.com/RaRe-Technologies/gensim/blob/master/gensim/corpora/ wikicorpus.py 6We use a standard linker from https://cloud.google.com/natural-language/docs/ analyzing-entities 7https://archive.org/download/enwiki-20170601/enwiki-20170601-pages-articles.xml. bz2 8https://archive.org/download/enwiki-20181220/enwiki-20181220-pages-articles.xml. bz2 13A.3 Graph statistics We construct two versions of the full Wikipedia graph following the method outlined in Appendix A.1. The main summary statistics (number of articles, nodes, edges, words per node, median path length) of the resulting graphs are listed in Table 3. A more detailed picture of the graph can be obtained by looking at its degree distribution, which is presented in Figure 4. It shows the graph has tell-tale sign of web-graph: it has a power-law distribution with a few hub nodes. 0 25 50 75 100 125 Out Degree 102 104 106 Number of Nodes (a) 2017: Out-degree 0 25 50 75 100 125 Out Degree 102 104 106 Number of Nodes (b) 2018: Out-degree 0.0 0.5 1.0 1.5 2.0 In Degree 1e6 101 103 105 107 Number of Nodes (c) 2017: In-degree 0.0 0.5 1.0 1.5 2.0 In Degree 1e6 101 103 105 107 Number of Nodes (d) 2018: In-degree Figure 4: Degree Distribution We try to estimate how far are two nodes in the graph. Calculating the entire distribution of shortest path length is prohibitively expensive for such large graph. We follow Ye et al. [2010] method to get an approximation. We start by selecting 250k nodes having highest in-degree, which is a good approximation of the giant component in a web graph as required by Ye et al. [2010]. We then perform a single source to all shortest paths on each of these 250k nodes using Dijkstra’s algorithm for sparse graphs. This resulted in computing approximately 1 trillion shortest paths based on which the estimated distribution of shortest path length is shown in Figure 5. 0 10 20 30 Number of changes 0.00 0.02 0.04 0.06Fraction of Nodes Figure 6: Number changes in edges among common nodes between 2017 and 2018 Finally, we would like to point out that there is signiﬁcant evolution (difference) between the 2017 and 2018 graph. Some important statistics reﬂecting the changes across the two graphs are: • New articles: 404,773 (7.5%) • New nodes: 3,871,578 (10%) • Deleted articles: 58,476 (1.1%) • Deleted nodes: 874,372 (2.3%) Further in Figure 6, we show histogram of changes in edges among the common nodes between the two graphs. Apart from addition/deletion of organic hyperlinks, just modiﬁcations of text would yield to differences as the chunking will be different. This shows signiﬁ- cant generalization is needed to successfully navigate across the two graphs, simply memorization will not work. 140 10 20 30 40 50 Path Length 0.00 0.02 0.04 0.06Fraction of Paths (a) PDF 0 10 20 30 40 50 Path Length 0.0 0.2 0.4 0.6 0.8 1.0Cum. Fraction of Paths (b) CDF 0 20 40 60 80 100 Path Length 0.00 0.02 0.04 0.06Fraction of Paths (c) PDF 0 20 40 60 80 100 Path Length 0.0 0.2 0.4 0.6 0.8 1.0Cum. Fraction of Paths (d) CDF Figure 5: Distribution of Short Path Length estimated by computing shortest path between 1 trillion pairs of random nodes in the 2018 graph. B Experimental details B.1 Embedding pretraining This section details our embedding pretraining procedure, as introduced in Section 3. Model details The transformer is ﬁne-tuned from a pre-trained RoBERTa [Liu et al., 2019] model. As in Equation 1 our model is trained to optimize L(θ) = EE(n0,...,nT) [T−1∑ t=1 log pθ(nt|nt,ng) ] (5) where we set ng = nT. We parameterize this distribution as pθ(nt+1|nt,ng) ∝exp(fθ(sφ(ng),sφ(nt),aφ(nt,nt+1))) (6) where sφ(·) and aφ(·,·) are functions which extract embeddings for an entire node and a node-action combination, respectively and fθ(·,·,·) is a function which combines these embeddings to produce action probabilities. At their core, sφ and aφ are based on the same transformer model. We tokenize the text at node nto produce Ltokens and pass these tokens through the transformer to produce L embeddings. The state representation sφ(n) is produced by taking the mean of these embeddings and passing this vector through a tanh nonlinearity. The node-action embedding aφ(n,n′) embeds the action of moving from node nto its neighbor n′. Let us assume the text at node nis “Barack Obama was the President of the United States during...” where the substrings “Barack Obama” and “President of the United States” correspond to hyperlinks to other neighboring articles n′and n′′. Then we construct the node-action embedding aφ(n,n′) by passing the tokenized text of node n through our transformer, again producing Ltransformed token embeddings. Then we take the transformed tokens which correspond to the text “Barack Obama” and take their mean and pass this through the tanh nonlinearity. If we instead wanted to produce aφ(n,n′′) we would take the transformed tokens which correspond to the text “President of the United States” instead, take the mean and apply the nonlinearity. This is visualized in Figure 7. The function fθ which combines these embeddings ﬁrst concatenates them and passes the combined input through a simple neural network with a single hidden layer and 1 output neuron. 15Figure 7: Description of how embeddings are extracted for navigation pretraining. Training details We train these models to optimize Equation 5. The objective is optimized with the Adam [Kingma and Ba, 2014] optimizer using an initial learning rate of 10−5 with a one-cycle cosine decay schedule and linear warm up of 10K steps. The training batch size is 512 and the model is trained over 3M iterations. Node text is clipped (or padded) to 200 tokens. The weights of the transformer are initialized from a pretrained RoBERTa [Liu et al., 2019] masked language model. B.2 Small graph experiments Model details In the small graph experiments, we use the simple policy network FF as described in Section 4.2. As discussed, we use a 1-layer MLP (meaning a single learned linear layer with no activation function). We use a hidden size of768 to match the size of φ. Because the ﬁnal logits are the result of an inner product between each state and action embedding of size 768, to aid network training, we add a normalization layer (jax.nn.normalize) before the inner product. For the action embedding ai = φ(si) we also concatenate a 1-hot vector that represents the action type (e.g. whether it was a link action, or a next action, or a prev action). We also concatenate another bit that indicates whether the state si of the particular action has been visited before. For the purposes of the RL baseline, we can view graph navigation as a goal-conditioned MDP.9 Our observation is the same as the inputs to the RFBC models. The reward function Rfor current node n and goal node ng we can write simply as: R= { 1 n= ng 0 otherwise (7) For the RL baseline, we used IMPALA [Espeholt et al., 2018], which is a kind of policy gradient approach, and thus has a policy network πand a “baseline” or value function network. Since the policy network has the same output space as our RFBC models, we use an identical architecture for this. For the value network, we use a very similar architecture. We again useφas our encoding of the current node and the target node, again concatenate them together. We then feed this through a 1-layer MLP to compute the value. Training details The procedure for generating the trajectories we use for the BC loss is described in Section 3.1. Except for our experiment in Section 5.1, we always use the random forward trajectories. Following Lin et al. [2018], in the navigation experiments, we randomly drop out edges in the training graph with probability 0.5 to reduce over-ﬁtting. As stated in Section 4.1, we set max steps 9On a known ﬁxed graph, we can formulate this an an MDP. However, in the case where the graph changes or in a setting where there are multiple possible graphs it is trivial to reformulate this as a POMDP where we only can see the current node and neighboring nodes 16to B = 100. For RFBC training on the small graph, we use RMSProp with a learning rate of 0.01, a decay of 0.9 an epsilon of 10−10. For the sentence search tasks, our models train a separate φfor the target embedding. As we stated, we use MiniBERT for this embedding φtarget and train it with the same optimization settings, except that we reduce the learning rate for these weights to 10−4. We use a batch size of 512 and we train for 50,000 network update steps. For RL training, we use VTRACE [Espeholt et al., 2018] loss, using the reward above. We again use RMSProp with a learning rate of 0.01, a decay of 0.9 an epsilon of 10−10. We set the baseline cost to 0.5, trajectory length (the number of timesteps the RL loss backprops through) to 100, batch size again to 512 and max update steps to 50,000. We did a parameter sweep for entropy costs of 0.1,0.01,0.001 and gamma of 0.8,0.9. To give RL the best chance possible, we choose the maximum over this sweep for each experiment (but still none of these match RFBC performance). B.3 Full Wikipedia navigation experiments On the full Wikipedia training experiments in Section 5.3, we evaluate on navigation and sen- tence search using either our feed-forward or transformer model and using either RoBERTA ﬁxed embeddings, or our trained embeddings described in Appendix B.1. The feed-forward model is identical to the one described in Appendix B.2. The transformer model is the standard transformer model from Vaswani et al. [2017] with 4 layers, an attention size of 64, 12 heads, and mlp hidden size of 3072 and dropout rate of 0. DistillBERT [Sanh et al., 2019] is used for φ(ng) for the sentence search experiments. The learning parameters and all other relevant training parameters for all models are identical to those in Appendix B.2 except that the learning rate for the transformer model is lower (10−4) as the earlier learning rate was too high for transformer models. B.4 RFBC + RL We additionally try ﬁnetuning an RL policy starting from our RFBC-trained navigation policies. We use the same network and learning settings as we did in the other RL experiments. We train for an additional 10,000 network updates (512M environment steps). We add the ﬁnal numbers in Table 2 and show the training curve in Figure 8. We can see from the training curve that RL training accuracy ﬂuctuates around the point where RFBC training left off. From the results in Table 2 we see little statistically signiﬁcant difference (about 1 point in either direction). Figure 8: Training curve for RFBC with RL ﬁnetuning. C Downstream task details C.1 FEVER experiments FEVER [Thorne et al., 2018] is a dataset containing 185,445 claims labeled as SUPPORTED , RE- FUTED , or NOTENOUGH INFO . Veriﬁable claims (i.e. SUPPORTED or REFUTED ) are annotated with 170 5 10 15 20 Shortest Path Length 0.00 0.05 0.10 0.15 0.20 0.25Frac. of Claims (a) Train/Density 0 5 10 15 20 Shortest Path Length 0.0 0.2 0.4 0.6 0.8 1.0Cum. Frac. of Claims (b) Train/Cumulative 0 5 10 15 20 Shortest Path Length 0.0 0.1 0.2 0.3Frac. of Claims (c) Test/Density 0 5 10 15 20 Shortest Path Length 0.0 0.2 0.4 0.6 0.8 1.0Cum. Frac. of Claims (d) Test/Cumulative Figure 9: Distribution of Short Path Length from starting node computed by BM25 to target node on FEVER dataset on 2017 graph. evidence sentences supporting this classiﬁcation, which are drawn from a preprocessed version of the June 2017 Wikipedia snapshot. Notably, only the article introductions are retained, and claims are generated from a curated set of approximately 50,000 popular pages. Data processing To make an aligned navigation graph for FEVER, we start with the June 01 2017 snapshot of Wikipedia and build the graph using the procedure described in Section A.1. Because of differences in text preprocessing, FEVER’s version of Wikipedia does not precisely align to our own. We then try to match FEVER’s Wikipedia articles to our graph, relying on Wikipedia URL redirections for handling disambiguations. This resulted in 99.5% matching of articles between our navigation graph and the FEVER version of Wikipedia. Second, we align the evidence sentences in FEVER to sentences from our text blocks using a fuzzy string match. Speciﬁcally, we use the token set ratio to score the similarity between an evidence sentence and the text in a graph node. If the score between a sentence and a node is ≥80, the node is added as an evidence node for the given claim. This threshold is sufﬁciently high to minimize the chances of a false positive, while also matching a high percentage of evidence sentences. Some sentences are difﬁcult to match, particularly those that are split between two text blocks. Ultimately, 93.1% of all evidence sentences were matched to a node. This gives us an augmented FEVER dataset, where for each claim we have a corresponding set of evidence nodes in the graph. These nodes are then used as navigation targets for ﬁne-tuning during the training phase. We generate trajectories for BC by running BM25 over all nodes in the graph, taking the top 10 matches as starting nodes, and ﬁnding shortest paths to evidence nodes. Data statistics We compute the shortest-path distance between top-1 retrieval of BM25 and target node in this graph in Figure 9. As can be seen that most path lengths are relatively short, which implies that BM25 lands us in the right vicinity and by a small amount of navigation around we will be able to ﬁnd the right evidence passage. Training details For the FEVER benchmark, our model uses learned embeddings for φwith a single feed-forward layer for the policy network. The model is pretrained on the 5-step sentence search task, and ﬁnetuned on trajectories generated from the augmented FEVER dataset using the the following loss function: L(θ) = LBC(θ) + 0.1∥φtarget(claim) −φ(sg)∥2 (8) where LBC is the normal BC loss we use in our other experiments. 18Only the φtarget weights are ﬁnetuned for this task. We use AdamW with a constant learning rate of 10−6,β1 = 0.9,β2 = 0.999, and ϵ= 10−6. The model is ﬁnetuned for 100,000 update steps with a batch size of 512. No edge dropout is used during ﬁnetuning. Evaluation details For each claim in the FEVER development dataset, we ﬁrst run BM25 over all nodes in the graph and take the top 5 matches as starting nodes. From each node, the ﬁnetuned model then navigates for 20 steps. We collect all sentences in all nodes visited by the agent over the 100 total navigation steps, and match them to FEVER evidence sentences using the WRatio function in the fuzzywuzzy package.10 Two sentence ranking methods are explored: Gensim’s TF-IDF implementation,11 and the BigBird re-ranker of Stammbach [2021], using their open source model and weights.12 Finally, the top-5 evidence sentences are then submitted to the ofﬁcial FEVER scorer13 to compute the accuracy, recall, and F1 scores. C.2 NQ experiments Natural Questions (NQ) is a dataset collected from real users asking questions on the Google search engine which are answerable using Wikipedia. We use the Natural Question open-domain subset presented in Lee et al. [2019] which has been aligned to Wikipedia dump of December 20, 2018 by Karpukhin et al. [2020]. In this split it has 58,880 questions for training and another 8,757 questions as development set for evaluation. (The test set is not aligned to Wikipedia passages so we do not evaluate on it.) Data processing Most prior works in literature utilize the processed collection of 21M passages provided by Karpukhin et al. [2020], but unfortunately we cannot use it as it has no hyperlink information which is crucial for our graph building. So we re-align the questions in NQ to our 2018 graph with 38M passages (i.e. nodes) which has different text blocks than Karpukhin et al. [2020]. As the article title corresponding to the passage was given as part of the dataset, the alignment task was local to the document. We could ﬁnd all the article titles, i.e. 100% match in locating the document in our dump. As before, to align the evidence passage in NQ to our text blocks, we ﬁrst used a fuzzy string match. Then in the ranked list of fuzzy string matches, we searched for exact answer string. The highest ranked node with answer string was labelled as the ground truth. Some answer strings were difﬁcult to match, particularly those that are split between two text blocks. Ultimately, 99.3% of all evidence passages were matched to a node. This gives the re-aligned NQ dataset, where for each question we have a corresponding set of evidence nodes in our graph. These nodes are then used as navigation targets for ﬁne-tuning during the training phase. We generate trajectories for BC by running BM25 over all nodes in the graph, taking the top 10 matches as starting nodes, and ﬁnding shortest paths to evidence nodes. Data statistics We compute the shortest-path distance between top-1 retrieval of BM25 and target node in this graph in Figure 10. As can be seen that most path lengths are relatively short, which implies that BM25 lands us in the right vicinity and by a small amount of navigation around we will be able to ﬁnd the right evidence passage. Training details For the NQ benchmark, our model uses learned embeddings for φwith a single feed-forward layer for the policy network. The model is pretrained on the 5-step sentence search task, and ﬁnetuned on trajectories generated from the NQ dataset using the the following loss function: L(θ) = LBC(θ) + 0.1∥φtarget(question) −φ(sg)∥2 (9) where LBC is the normal BC loss we use in our other experiments. Only the φtarget weights are ﬁnetuned for this task. We use AdamW with a constant learning rate of 10−6,β1 = 0.9,β2 = 0.999, and ϵ= 10−6. The model is ﬁnetuned for 100,000 update steps with a batch size of 512. No edge dropout is used during ﬁnetuning. 10https://pypi.org/project/fuzzywuzzy/ 11https://radimrehurek.com/gensim/models/tfidfmodel.html 12https://github.com/dominiksinsaarland/document-level-FEVER 13https://github.com/sheffieldnlp/fever-scorer/blob/9d9ed27637adddf73bc2f8e38c436bdc032c9f1f/ src/fever/scorer.py 190 5 10 15 20 Shortest Path Length 0.00 0.05 0.10 0.15Frac. of Questions (a) Train/PDF 0 5 10 15 20 Shortest Path Length 0.0 0.2 0.4 0.6 0.8 1.0Cum. Frac. of Questions (b) Train/CDF 0 20 40 60 80 100 Shortest Path Length 0.00 0.02 0.04 0.06 0.08Frac. of Questions (c) Train/PDF 0 20 40 60 80 100 Shortest Path Length 0.0 0.2 0.4 0.6 0.8 1.0Cum. Frac. of Questions (d) Train/CDF 0 5 10 15 20 Shortest Path Length 0.00 0.05 0.10 0.15Frac. of Questions (e) Test/PDF 0 5 10 15 20 Shortest Path Length 0.0 0.2 0.4 0.6 0.8 1.0Cum. Frac. of Questions (f) Test/CDF 0 20 40 60 80 100 Shortest Path Length 0.00 0.02 0.04 0.06 0.08Frac. of Questions (g) Test/PDF 0 20 40 60 80 100 Shortest Path Length 0.0 0.2 0.4 0.6 0.8 1.0Cum. Frac. of Questions (h) Test/CDF Figure 10: Distribution of Short Path Length from starting node computed by BM25 to target node on Natural Questions dataset on 2018 graph. Evaluation details For each question, we ﬁrst run BM25 over all nodes in the graph and take the top-5 matches as starting nodes. From each starting node, the agent then navigates for 20 steps. All the 100 nodes visited by the agent is then ranked by a simple cross-attention model. For the cross-attention model, we follow Hofstätter et al. [2020] to train 6-layer BigBird [Zaheer et al., 2020]. Since we target navigating to the exact evidence passage required to answer the question, we use recall@{1,2,3,4,5} for ﬁnding the gold evidence passage as our metric. More commonly the metric marks a retrieved passage to be correct if it contains the answer string, but this causes a lot of false positives, e.g. the answer string appears in a totally irrelevant context. 20D Variational Interpretation of our Approach We present an alternative motivation of our method based on variational inference in a latent variable model of states. Assume we are given some goal node ng. We would like to parameterize a model pθ(nT|ng) which will generate a trajectory of states n0,n1,...,n T such that nT = ng. We deﬁne our model as a latent variable model pθ(nT|ng) = ∑ n0,n1,...,nT−1 pθ(nT,nT−1,...,n 1,n0|ng) = ∑ n0,n1,...,nT−1 p(n0) T∏ t=1 pθ(nt|nt−1,ng) (10) i.e an autoregressive model which samples an initial state from n0 ∼p(n0) and then samples subsequent states nt ∼pθ(nt|nt−1,ng). The transition distribution samples the next node nt from the neighbors of nt−1 in our graph. We would like the probability that nT = ng to be large, thus we will train our model to maximize log pθ(nT = ng|ng). For latent-variable models such as this, we can rewrite the marginal likelihood as log pθ(nT|ng) = Epθ(n0,...,nT−1|nT,ng) [ p(n0) T∏ t=1 pθ(nt|nt−1,ng) −log pθ(n0,...,n T−1|nT,ng) ] (11) and can obtain a lower-bound on this quantity by replacing the intractable posterior pθ(n0,...,n T−1|nT,ng) with a variational approximation q(n0,...,n T−1|nT,ng), i.e Lθ(nT,ng; q) := log pθ(nT|ng) ≥Eq(n0,...,nT−1|nT,ng) [ p(n0) T∏ t=1 pθ(nt|nt−1,ng) −log q(n0,...,n T−1|nT,ng) ] . (12) The above bound becomes tight when q(n0,...,n T−1|nT,ng) = pθ(n0,...,n T−1|nT,ng). Thus, we can optimize our model parameters θto maximize Lθ(nT,ng; q). This approach has had a long history of successfully training latent-variable generative models [Kingma and Welling, 2013, Ho et al., 2020]. In the context of this work, we can view q(n0,...,n T−1|nT,ng) as a distribution over trajectories of nodes which end at our goal node. We can interpret the various trajectory generation methods introduced in Section 3.1 as different variational approximations q. Ideally, the option which most closely approximates the true posterior pθ(n0,...,n T−1|nT,ng) would be the most desirable but in general this distribution is intractable. We ﬁnd that using simple random trajectories provides a good-enough approximation to the posterior to enable us to train a model which reliably ﬁnds the goal state. This result is not completely surprising in context of prior work [Dai et al., 2020] which successfully trains latent-variable models using random trajectories as an inference model. E Efﬁciency Analysis In this section we analyze the asmpytotic runtime of our method and baselines. In Table 7 we show the asymptotic runtime and whether or not it is scalable for each method. We also show the 5-step accuracy from Table 2 for quick reference. T is the trajectory length (either the length of the trajectory for BC methods or the maximum allowed trajectory for other methods. For BC, T is guaranteed to be shorter than the max, but this is a constant factor difference. Eout is the average number of outgoing edges (i.e. actions) at a node. (small - 101) E is the total edges in the graph (very large - 108). V is the number of nodes in the graph (large - 107). 21Table 7: Asmptotic runtime analysis of RFBC and baselines. Method Cost (Big-O) Scalable Accuracy (5-step 200k graph) RFBC (ours) O(EoutT) Yes 85.3 Backwards BC O(EoutT) Yes 2.3 Shortest path BC O(Elog V + EoutT) No 86.7 RL O(EoutT) Yes 41.4 Random O(T) Yes 12.3 Greedy O(EoutT) Yes 19.7 Random DFS O(T) Yes 10.0 Greedy DFS O(EoutT) Yes 31.1 V nodes in the graph [large – 107] E total edges in the graph [very large – 108 −109] Eout average out degree of node (# of actions) [small – 101] T length of trajectory, i.e. the maximum steps agents can run. [small – 101] For most methods, the runtime is O(EoutT) because the method does some constant amount of work for each possible actions Eoutfor O(T) steps. For the random methods it is O(T) because the work is constant per step (choose randomly). Shortest path distance has the worst runtime. The O(ElogV ) term is the runtime of ﬁnding a shortest path (Dijkstra’s algorithm with Fibonacci heap). As the graph gets larger, this term because infeasible (we could not even run this on the full Wikipedia graph). DFS might seem like it should be larger than O(EoutT) or O(T), but because we restrict the maximum allowable steps, it is only proportional to T and (for greedy) Eout. From this Table, we see that all methods (except shortest path) have scalable runtimes. The dominant factor is Eout, which theoretically grows slowly with graph size. Moreover, in practice, there is a natural upper bound on the number of links one can have in a paragraph of text, and the runtime difference in our experiments going from 200k nodes to 38M nodes is only 1.5x despite the 200-fold increase in graph size. 22",
      "references": [
        "Learning to retrieve reasoning paths over wikipedia graph for question answering.",
        "Reading wikipedia to answer open-domain questions.",
        "Learning discrete energy-based models via auxiliary-variable local exploration.",
        "Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning.",
        "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.",
        "Collaborative policy learning for open knowledge graph reasoning.",
        "Neurally-guided semantic navigation in knowledge graph.",
        "Denoising diffusion probabilistic models.",
        "Improving efﬁcient neural ranking models with cross-architecture knowledge distillation.",
        "Dense passage retrieval for open-domain question answering.",
        "Adam: A method for stochastic optimization.",
        "Auto-encoding variational bayes.",
        "Internet-augmented language models through few-shot prompting for open-domain question answering,",
        "FEVER development set leaderboard.",
        "Latent retrieval for weakly supervised open domain question answering.",
        "Multi-hop knowledge graph reasoning with reward shaping.",
        "Roberta: A robustly optimized BERT pretraining approach.CoRR, abs/1907.11692, 2019.",
        "Webgpt: Browser-assisted question-answering with human feedback.",
        "Webnav: A new large-scale task for natural language based sequential decision making.",
        "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering.CoRR, abs/2010.08191, 2020.",
        "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering.",
        "The probabilistic relevance framework: Bm25 and beyond.",
        "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",
        "Revisiting the open-domain question answering pipeline.",
        "M-walk: Learning to walk over graphs using monte carlo tree search.",
        "Evidence selection as a token-level prediction task.",
        "Team DOMLIN: Exploiting evidence enhancement for the FEVER shared task.",
        "The web as a knowledge-base for answering complex questions.",
        "FEVER: a large-scale dataset for fact extraction and VERiﬁcation.",
        "Well-read students learn better: The impact of student initialization on knowledge distillation.",
        "Attention is all you need.",
        "Wikigraphs: A wikipedia text - knowledge graph paired dataset.",
        "Human wayﬁnding in information networks.",
        "Automatic versus human navigation in information networks.",
        "Wikispeedia: An online game for inferring semantic distances be- tween concepts.",
        "Deeppath: A reinforcement learning method for knowledge graph reasoning.",
        "Linkbert: Pretraining language models with document links.",
        "Distance distribution and average shortest path length estimation in real-world networks.",
        "Big bird: Transformers for longer sequences."
      ],
      "meta_data": {
        "arxiv_id": "2211.00177v1",
        "authors": [
          "Manzil Zaheer",
          "Kenneth Marino",
          "Will Grathwohl",
          "John Schultz",
          "Wendy Shang",
          "Sheila Babayan",
          "Arun Ahuja",
          "Ishita Dasgupta",
          "Christine Kaeser-Chen",
          "Rob Fergus"
        ],
        "published_date": "2022-10-31T22:40:26Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses large-scale web navigation: given a start Wikipedia paragraph node and a target node/snippet, learn to choose hyperlinks to reach the target efficiently without global graph knowledge or search at test time. Key contributions: (1) show simple behavioral cloning on automatically generated random-walk trajectories (Random Forward Behavioral Cloning, RFBC) is sufficient to learn an effective goal-conditioned link-selection policy and useful text/node embeddings; (2) scale to a paragraph-level Wikipedia graph with ~38M nodes and ~387M edges, achieving high success navigating to targets 5–20 steps away (up to ~96% @5 and ~92% @20) and strong “sentence search” where the goal is specified by a single sentence (~90% @20); (3) demonstrate the pretrained navigation policy/embeddings improve evidence gathering for downstream fact verification (FEVER) and open-domain QA (Natural Questions) when combined with simple BM25 starting points and lightweight re-ranking, reaching competitive/near-SOTA retrieval metrics with a simpler pipeline and providing provenance via navigation paths.",
        "methodology": "Formulate navigation on a directed web graph as goal-conditioned policy learning pθ(n_{t+1}|n_t,n_g), trained via behavioral cloning to maximize log-likelihood of expert next-steps along sampled trajectories where the last node is the goal. Trajectory generation choices are studied: reverse random walks, forward random walks, and shortest paths; forward random walks are chosen for scalability and robustness. Each graph node is a Wikipedia paragraph; text encoders φ embed node text (paragraph + article title) using Transformers (fixed RoBERTa or a RoBERTa initialized model pretrained with the navigation objective). Action representations are embeddings of candidate outgoing links; to reduce memory during large-scale training, action embeddings are derived from the hyperlink span tokens in the current paragraph. Policy scoring uses a dot-product between a combined (current, goal[, history]) embedding and each action embedding, normalized over outgoing links. Two policy variants: (1) simple feed-forward combination of current+goal embeddings; (2) a 4-layer Transformer over the trajectory history plus goal embedding to capture longer-horizon context. Training uses edge/action dropout during navigation pretraining to encourage robustness/exploration; RL baselines (IMPALA/V-trace) are compared and found substantially worse under sparse rewards. For downstream FEVER/NQ, a separate target encoder maps a claim/question (or sentence snippet) to the goal embedding space; embeddings/policy are pretrained on sentence search, then frozen while fine-tuning only the target encoder with BC loss plus an auxiliary alignment loss to the ground-truth evidence node embedding. Retrieval pipeline: BM25 to pick start nodes, RFBC navigation for fixed steps, then sentence/passages re-ranked by TF-IDF or a cross-attention model (BigBird).",
        "experimental_setup": "Environment construction: English Wikipedia snapshots converted to a paragraph-level directed graph; nodes are ~100-word paragraph blocks (~1000-word articles split), edges are (i) organic hyperlinks, (ii) additional entity links, and (iii) next/previous paragraph links. Full graphs: 2017 snapshot (~36.3M nodes, 359M edges) for training; 2018 snapshot (~38.5M nodes, 387M edges) for evaluation to test generalization across yearly graph changes; median path length ~15. Small controlled setting: disjoint 200k-node train graph and 200k-node eval graph subsampled from 2018 to test strict generalization and run many baselines.\nNavigation tasks: T-step navigation with T∈{5,10,20} where start is uniform random and goal is last node of a T-step random walk; success if goal reached within budget B (typically 100 steps). Multistep navigation samples T uniformly from 1..20. Sentence search variants provide only one sentence from the goal node to form the goal embedding.\nTrajectory-distribution study (small graph): compare forward random walks, reverse random walks, and random shortest paths; evaluate on forward-sampled and reverse-sampled source/goal pairs.\nBaselines/ablations (small graph): random policy, greedy cosine to goal, random/greedy DFS, RL with IMPALA, and versions with random features to test semantic reliance.\nLarge-scale models: compare fixed RoBERTa vs navigation-pretrained embeddings; feed-forward vs Transformer policy; DistilBERT/MiniBERT used for target sentence encoder where applicable.\nDownstream benchmarks (evidence retrieval): FEVER (dev) evidence selection measured by Precision@5/Recall@5/F1@5 using official scorer; starting points via BM25 (top-5) then navigate 20 steps each; re-rank candidate sentences with TF-IDF or BigBird. Natural Questions Open (dev) evidence passage retrieval measured by Recall@k (k=1..5) for exact gold passage (harder metric than answer-string). Compare to BM25, BM25+BigBird re-rank, and a reference RocketQA run under their evaluation setup.",
        "limitations": "Relies on having (or learning) a strong target/goal encoder: in core navigation experiments the goal node is known (or defined by a sentence from it), and downstream FEVER/NQ fine-tuning uses aligned ground-truth evidence nodes; in broader web settings, ground-truth targets and alignment may be unavailable or ambiguous. The method still needs an external mechanism to propose starting nodes (BM25) and a separate re-ranker to decide final evidence; the navigation policy does not intrinsically know when it has reached the correct node or when to stop. Trajectory supervision comes from random walks rather than optimal paths, which can be suboptimal for certain graph regions or rare semantics; shortest-path supervision would help but is computationally intractable at full scale. Evaluation focuses on Wikipedia paragraph graphs; generalization to the open Web may be challenged by noisier link structures, non-textual content, dynamic pages, and varying access constraints. Action space can be large (high out-degree hub nodes), potentially increasing inference cost linearly with outgoing links, and the approach assumes access to local page text and link anchor spans. Downstream results conflate navigation with re-ranking quality; end-to-end answer/verification accuracy is not the primary metric (evidence retrieval only).",
        "future_research_directions": "Develop target specification and stopping criteria: train the agent to (i) infer/confirm the correct target page, (ii) decide when sufficient evidence has been gathered, and (iii) terminate early, reducing dependence on external re-rankers. Integrate retrieval and navigation more tightly (e.g., joint learning with BM25/dense retrieval, or hybrid policies that can ‘teleport’ via search when stuck) and learn start-node selection. Improve training trajectories with scalable approximations to shortest paths (e.g., curriculum over path lengths, heuristic-guided random walks, contrastive learning over neighbor choices, or offline RL with better exploration) while retaining tractability at 10^7–10^8 scale. Extend to multi-hop reasoning/evidence composition: plan over multiple targets, gather and aggregate evidence across nodes, and incorporate memory/summarization modules. Expand beyond Wikipedia: apply to general web graphs with heterogeneous pages, incorporate structured signals (HTML, metadata, click logs), and handle link noise/loops/dead-ends more robustly. Optimize efficiency: approximate action scoring for high-degree nodes (candidate pruning, two-stage link scoring, learned link proposal) and investigate compression/distillation for real-time agents. Evaluate on end-to-end tasks (fact verification labels, QA exact match) and on robustness/safety (avoiding adversarial pages, bias, and stale information).",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Cell ontology guided transcriptome foundation model",
      "full_text": "Cell-ontology guided transcriptome foundation model Xinyu Yuan1,2, Zhihao Zhan1,2, Zuobai Zhang1,2, Manqi Zhou4 Jianan Zhao1,2, Boyu Han3, Yue Li1,3,*, Jian Tang1,5,6,* 1Mila - Québec AI Institute, 2University of Montréal 3McGill University, 4Cornell University, 5HEC Montréal, 6CIFAR AI Chair *Correspondence: liyue@mila.quebec; tangjian@mila.quebec Abstract Transcriptome foundation models (TFMs) hold great promises of deciphering the transcriptomic language that dictate diverse cell functions by self-supervised learn- ing on large-scale single-cell gene expression data, and ultimately unraveling the complex mechanisms of human diseases. However, current TFMs treat cells as independent samples and ignore the taxonomic relationships between cell types, which are available in cell ontology graphs. We argue that effectively leveraging this ontology information during the TFM pre-training can improve learning biolog- ically meaningful gene co-expression patterns while preserving TFM as a general purpose foundation model for downstream zero-shot and fine-tuning tasks. To this end, we present single cell, Cell-ontology guided TFM (scCello). We introduce cell-type coherence loss and ontology alignment loss, which are minimized along with the masked gene expression prediction loss during the pre-training. The novel loss component guide scCello to learn the cell-type-specific representation and the structural relation between cell types from the cell ontology graph, respectively. We pre-trained scCello on 22 million cells from CellxGene database leveraging their cell-type labels mapped to the cell ontology graph from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates competitive generalization and transferability performance over the existing TFMs on biologically important tasks including identifying novel cell types of unseen cells, prediction of cell-type- specific marker genes, and cancer drug responses. Source code and model weights are available at https://github.com/DeepGraphLearning/scCello. 1 Introduction Cells are basic units of all living organisms. Deciphering diverse cell functions through gene expression is a long-standing challenge in life science and yet the essential path towards precision and personalized medicine. In this context, single-cell RNA sequencing (scRNA-seq) has emerged as a pivotal technique to measure the gene expression in individual cells. The vast amount of publicly available scRNA-seq data offers a rich transcriptomic data source [48] for learning cell representations towards various research applications, such as cancer therapy [62] and drug discovery [5]. Recently, severalTranscriptome Foundation Models(TFMs) were developed to improve cell represen- tation learning. They mainly utilize pre-training methods analogous to natural language processing like masked token prediction, treating genes as “tokens” and cells as “sentences” [ 14, 61, 70, 55]. However, the existing TFMs treat cells as independent samples and ignore their cell-type lineages. On the other hand, prior knowledge of the taxonomic relationships of cell types has been made available through the cell ontology graph by Open Biological and Biomedical Ontology Foundry [4]. Effec- tively leveraging the ontology knowledge can improve the quality of the pre-training on large-scale scRNA-seq atlases, which are heterogeneous and encompass hundreds of cell types. This can be done by training the TFM to recognize the inherent ontology relationships among cell types, thereby 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2408.12373v2  [cs.LG]  28 Feb 2025refining the cell representations. For instance, “mature α-β T cell” should be closer to “mature T cells” compared to more general term “T cells” and farther from neurons and astrocytes from the brain (e.g., Tab. 7). To capture this intuition, we propose scCello, a single cell, Cell-ontology guided TFM. scCello learns cell representation by integrating cell type information and cellular ontology relationships into its pre-training framework. scCello’s pre-training framework is structured with three levels of objectives: (1) gene level: a masked token prediction loss to learn gene co-expression patterns, enriching the understanding of gene interactions (Sec. 2.2); (2) intra-cellular level: an ontology- based cell-type coherence loss to encourage cell representations of the same cell type to aggregate, prompting consistency between cells and their types (Sec. 2.3); and (3) inter-cellular level: a relational alignment loss to guide the cell representation learning by consulting the cell-type lineage from the cell ontology graph (Sec. 2.4).. We demonstrate the generalizability and transferability of scCello on 22 million cells from CellxGene. For model generalization, we observe that scCello excels on cell type identification across all datasets in both zero-shot setting (i.e., directly using the pre-trained model) (Sec.4.2.1) and fine-tuning setting (Sec. 4.2.2). In particular, scCello accurately classifies novel cell types by leveraging the ontology graph structure (Sec. 4.3). For transferability, scCello demonstrates competitive performances in pre- dicting cell-type-specific marker genes (Sec. 4.4) and cancer drug responses (Sec. 4.5). Additionally, scCello is robust against batch effects (Sec. 4.6). Finally, we validate our contribution via ablation study (Sec. 4.7). 2 Method Fig. 1 illustrates an overview of scCello. We present the details of individual components below. 2.1 Data Preprocessing Cell ontology graph. Cell ontology is a widely used metadata schema for standard cell type annotations [16]. We downloaded the ontology from Open Biological and Biomedical Ontology Foundry (https://obofoundry.org/). It is structured as an unweighted directed acyclic graph G = ( V, E), where each node v ∈ Vcorresponds to a distinct cell type and each directed edge (u, v) ∈ Edenotes a hierarchical lineage relationship of the form \"is a subtype of\" between cell types (Fig. 1a). To accurately represent the inherently symmetric \"being biologically similar\" relationship between cell types, the directed graph was transformed into an undirected one for subsequent calculation of cellular ontology relationships in Sec. 2.4. scRNA-seq data. The scRNA-seq data were downloaded from CellxGene. After the preprocessing (App. B), we obtained 22 million cells. Each single-cell transcriptome is represented by a sequence of tuples, each containing genes and their expression counts.1 Each sequence was then ordered by the rank of the gene expression values [61] , akin to the sequential ordering of natural languages. Given a batch of B cells, each cell i ∈ {1, . . . , B} was assigned a cell type ontology identifier ci ∈ Vfrom the CellxGene database, to enable mapping between cell and cell ontology. 2.2 Masked Gene Prediction Same as BERT [ 15], scCello predicts a randomly masked gene token in each cell based on its surrounding context in the sequence. This objective LMGP aims to learn the dynamic gene co- expression network. 2.3 Intra-Cellular Ontology Coherence A straightforward approach to encourage learning the cell representations that are coherent to the cell type labels is to apply cross-entropy loss for supervised cell type classification. However, this approach is limited in learning cell representation for the foundation model. Instead, we employed a supervised contrastive loss as our objective LIntra, which directly optimizes the TFM rather than 1scRNA-seq data was from CellxGene database https://cellxgene.cziscience.com/. 2B D Ontology Relationship … Cells BC 𝒉!\t 𝒉#\t Regulatory T cell Genes C…Neuron 112 23 3 𝒛$%𝒛$\tCell Reprs. Distance> > 𝒛$%𝒛&\t𝒛$%𝒛'\t 321 Linear … Cell Type AAB BC C > 𝒛$%𝒛$\t 𝒛$%𝒛&\t 𝒛$%𝒛'\t Native cell…ANucleate cell A ESimilarity? \"Cell Type E\" … … DistanceGenes Knockout Cell Reprs.Marker GeneLogits (b) scRNA-seq data (a) Cell Ontology Graph Cell Reprs.IC50?+Cell Reprs.Cell Type Classification Novel Cell Type ClassificationMarker Gene Prediction 𝒉(\t 𝒛'\t𝒛$%𝒛𝟑\t𝒛$%𝒛&\t ? ? (d) Downstream Tasks A (c) scCello Pre-training Framework 𝒛$\t 𝒛&\t Relational Alignment Cell-Type Coherence >sim(𝐴,𝐴)sim(𝐴,𝐵)sim(𝐴,𝐶)Guide Cancer Drug PredictionCell Type Clustering Batch Integration Figure 1: (a) Cell ontology graph describes taxonomic relationships between cell types. (b) Each cell in scRNA-seq data is represented by gene sequences, and associated with a cell type ontology identifier. (c) The pre-training framework of scCello is structured with three levels of objectives: gene-level masked gene prediction, intra-cellular level cell type coherence and inter-cellular level ontology alignment. For example, as shown in panel b, cells 1, 2, and 3 are labelled with cell type A, B and C. The intra-cellular cell type coherence loss encourages alignment of embedding z1 with hA, z2 with hB, and z3 with hC. The inter-cellular level ontology alignment loss encourages representational learning of cell similaritiesz⊤ i zj between cell i and j to be consistent to the similarity of their corresponding cell types sim(ci, cj) based on the ontology relationships. (d) Downstream tasks enabled by scCello and demonstrated in the study. merely learning through the linear classifier: LIntra = − BX i=1 log   exp(zT i hci/τ) exp(zT i hci/τ) + PB j=1,j̸=i exp(zT i hcj /τ) ! . (1) where zi and hci denote the latent representation of cell i and cell type ci, respectively. This supervised contrastive loss pulls representations of the same class (positives) and repels rep- resentations of different classes (negatives). It often leads to representations that are at least as discriminative as the cross-entropy loss [22]. To reduce the degrees of freedom available for TFM optimization, we introduce a regularization term LReg: LReg = PB i=1 ||Linear(hci) − zi||2 2, (2) where the linear layer is shared across all cells and cell types. Thereby, it constrains the cell type representation space to be an affine transformation of the cell representation space. 32.4 Inter-Cellular Relational Alignment To encourage TFMs to learn inter-cellular ontology relationships, scCello forces cell representations to truthfully reflect the pairwise node structural similarity derived from the cell ontology graph, using a relational alignment objective. This objective constitutes the most important part of scCello. Ontology relationships. To effectively quantify ontology relationships between cell types from the ontology graph, scCello estimates pairwise node structural similarities as proxies using Personalized PageRank (PPR) [20]. PPR is a graph learning algorithm. The PPR score PPR(u, v) estimates the probability for a random walk. It starts from a given target nodeu ∈ Vand terminates at another node v ∈ V. Importantly, this is a context-sensitive structural similarity measure that accounts both direct connections and broader subgraph patterns [ 67]. It also provides robustness against variations in global network structures, such as variable node degrees and clustering coefficients [11]. To improve robustness (as justified in App. A), we transform PPR(·) through a non-linear function to derive the structural similarities sim(·) as ontology relationships tunable by a hyper-parameter threshold s: sim(u, v) = ( ⌊log2(PPR(u,v) s + 1)⌋, if PPR(u, v) ≥ s 1, otherwise . (3) Relational alignment. Cells with closely related cell types tend to be more similar than those with distinct cell types. This observation guides scCello to align the distances between cell representations w.r.t. a target cell, with their structural similarities sim(·) (as shown in Fig. 1c). Specifically, given a batch of B cells, if we consider a target cell i and another cell in the batch j ̸= i, the representation distance zT i zj should reflect their structural similarity sim(ci, cj). Accordingly, a negative sample set Ωi,j = {k|sim(ci, cj) > sim(ci, ck), 1 ≤ k ≤ B} can be produced, where cell pair (i, k) are considered less similar to the cell pair (i, j) and should be contrasted against in the representation space using the objective LInter: LInter = − BX i=1 BX j=1,j̸=i log   exp(zT i zj/τ) exp(zT i zj/τ) + P k∈Ωi,j exp(zT i zk/τ) ! . (4) Notably, ancestor cell types, which can reach the target cell type via the directed \"is a subtype of\" edge on the ontology graph, are structurally distant from the target cell type. Despite being distant, they fall into the same, broader cell type category. Contrasting cells associated with these distant ancestor cell types with the target cell is counter-intuitive. Therefore, scCello explicitly excludes such cells from the negative sample set, avoiding inappropriately pushing away biologically similar cells. This enhances scCello’s capability to discern subtle similarities and differences within the cell types. 2.5 Overall Pre-training Objective During pre-training, we seek to minimize the loss functions of all pre-training tasks simultaneously: θ∗ ← arg min θ LMGP + LInter + LIntra + LReg (5) where θ denotes all learnable parameters in scCello, which adopts transformer stacks as model backbones. We state the detailed information of model architectures in App. D. 3 Related Work The rapid growth of scRNA-seq datasets has opened new avenues for constructing TFMs, enabling transfer learning across various biological downstream tasks. Initial efforts, such as scBERT [73], Exceiver [13] and Geneformer [61], borrows the concept of masked language modeling [15] from natural language processing (NLP) domain for pre-training, by treating cells as sentences and genes as tokens. Concurrently, tGPT [59] and scGPT [14] explored generative modeling [53], and CellLM [76] adapted the idea of contrastive learning [42]. Following the concept of “scaling” towards emergent ability [70] in NLP, scFoundation [25] proposes the largest foundation model at the time in terms of model size and pre-training data size; scHyena [49] scales modeling context window size to the full length of scRNA-seq data with Hyena operator [51] instead of conventionally used transformers. 4scTab [18] is the first to explore large-scale supervised learning mechanism for scRNA-seq pre- training, and is capable of annotating unseen tissue cells for real-world applications. Moreover, SCimilarity [28] and UCE [55] focus on developing a unified latent space as a large-scale reference atlas for querying new cells. Yet, these TFMs mainly treat cells as independent samples during training and ignore their biological ontology relationships. scCello bridges this gap by incorporating cell type relationships derived from the cell ontology graph into TFM pre-training. This strengthens TFMs’ model generalization and transferability capability, as shown in Sec. 4. 4 Experiments As an overview, the following experiments show that, (1) scCello can generalize to unseen cells, and to more difficult settings, such as cells of unseen cell types, tissues, and donors (Sec. 4.2.1); (2) scCello can benefit from fine-tuning on target datasets (Sec. 4.2.2); (3) the structural similarity embedded in scCello helps to classify novel cell types in a zero-shot manner (Sec. 4.3); (4) scCello effectively transfers to different downstream tasks (Sec. 4.4 and Sec. 4.5); (5) scCello is robust to batch effects that arise from different experimental conditions (Sec. 4.6); (6) Each loss component in Eqn. 5 is beneficial to scCello (Sec. 4.7). For every table reported, we used bold to highlight the best performance and results within 0.005 difference from the best. We used underlining to denote the second-best performances. For all metrics, ↑ indicates the higher the better. 4.1 Setups Pre-training and downstream datasets. We collected a large pre-training dataset consisting of 22 million cells along with downstream datasets. In particular, we generated one in-distribution (ID) and six out-of-distribution (OOD) datasets (App. B). The ID dataset is denoted as Did. For the OOD setting, we introduced three scenarios: unseen cell types ({Dct i }2 i=1), unseen cell tissues ({Dts i }2 i=1), and unseen donors ({Ddn i }2 i=1). Each scenario has two datasets. Notably, the OOD donor setting presents more realistic challenges than ID and other OOD settings because of the potential batch effects in the test donors. Pre-training configurations. An Adam optimizer [38] (learning rate: 0.001, weight decay: 0.001, warm-up steps: 3, 333) was used to train the scCello for 40, 000 steps on 4 NVIDIA A100 GPUs on Compute Canada. We used 192 for batch size. More details are introduced in App. D. Baselines. Across all downstream tasks, scCello is benchmarked with leading open-source large- scale TFMs: Geneformer [61], scGPT [14], scTab [18], UCE [55], and three TFM ablations. We also implemented ablated versions of scCello that only differ in the pre-training objectives from scCello: scCello using only the masked gene prediction loss (denoted as MGP), scCello using only the cell type supervised classification (denoted as Sup), and scCello using only the two losses (denoted as MGP+Sup). The three ablated TFMs provide a reference to isolate the effect of implementation details and training configurations. For each task, we also selected state-of-the-art non-TFM methods for fair comparison. Downstream metrics. We evaluated the 3 tasks by the following metrics. (1) Clustering metrics include normalized mutual information (NMI), adjusted rand index (ARI), average silhouette width (ASW), and the average of the 3 scores (AvgBio) to assess both between-cluster separation and within-cluster closeness [14]. The batch integration task (Sec. 4.6) is evaluated by ASWb, graph connectivity (GraphConn) and their average (AvgBatch), along with an overall score (Overall = 0.6×AvgBio + 0.4×AvgBatch) to balance biological relevance and batch consistency following [14]. (2) Classification metrics include accuracy (Acc), Macro F1 and area under the ROC curve (AU- ROC) [50]. (3) Regression task metrics include Pearson correlation coefficient score (PCC) [ 50]. Details for each metric were provided in App. E.1. 4.2 Cell Type Identification 4.2.1 Zero-shot Cell Clustering Results Setup. For the cell type clustering task, TFM baselines and four non-TFM methods were evaluated: (1) raw data expressions of highly variable genes ( abbr., Raw Data) [ 34]; (2) Seurat [ 27]; (3) 5Table 1: Zero-shot cell type clustering on the curated ID and OOD datasets. Method In-Distribution (ID) Out-of-Distribution (OOD) Did Dct1 Dct2 Dts1 Dts2 Ddn1 Ddn2 OOD Avg.↑NMI↑ ARI↑ ASW↑ AvgBio↑ AvgBio↑ AvgBio↑ AvgBio↑ AvgBio↑ AvgBio↑ AvgBio↑ Non-TFM Methods Raw Data 0.566 0.237 0.453 0.419 0.703 0.629 0.540 0.631 0.458 0.460 0.570Seurat 0.648 0.270 0.407 0.442 0.752 0.737 0.587 0.636 0.466 0.489 0.611 Harmony1 0.621 0.261 0.382 0.421 0.432 0.417 0.462 0.515 0.456 0.474 0.459scVI 0.660 0.297 0.464 0.474 0.760 0.725 0.577 0.634 0.478 0.502 0.613 Ontology-Agnostic TFMs Geneformer 0.616 0.261 0.418 0.432 0.689 0.668 0.539 0.597 0.468 0.482 0.574scGPT 0.615 0.258 0.442 0.438 0.707 0.720 0.544 0.627 0.456 0.477 0.589scTab 0.707 0.479 0.544 0.577 0.759 0.726 0.515 0.657 OOM OOM /UCE 0.670 0.304 0.494 0.489 0.772 0.741 0.598 0.670 0.485 0.506 0.629MGP 0.662 0.306 0.451 0.473 0.714 0.740 0.576 0.628 0.488 0.518 0.611Sup 0.703 0.393 0.569 0.555 0.767 0.775 0.605 0.680 0.552 0.573 0.659MGP+Sup 0.661 0.337 0.550 0.516 0.758 0.764 0.610 0.672 0.553 0.570 0.655 Ontology-Enhanced TFMs scCello 0.785 0.558 0.667 0.670 0.769 0.786 0.612 0.705 0.608 0.643 0.687 1 Harmony could be over-corrected w.r.t.batch labels for datasets with many batches [10]. Table 2: Cell type identification using fine-tuned TFMs. Both the classification and clustering performances on the ID dataset Did are reported. Method Classification Clustering Acc↑ Macro F1↑ AvgBio↑ Non-TFM Methods scANVI 0.763 0.490 0.568 Scratch 0.621 0.223 0.544 Ontology-Agnostic TFMs Geneformer 0.747 0.440 0.439 scGPT 0.712 0.344 0.477 scTab 0.778 0.373 0.606 MGP 0.722 0.287 0.607 Sup 0.812 0.363 0.659 MGP+Sup 0.820 0.406 0.607 Ontology-Enhanced TFMs scCello 0.867 0.511 0.694 Table 3: Marker gene prediction, a binary classification task to identify cell- type-specific marker genes. Method Dmk1 Dmk2 Avg.↑AUROC↑ AUROC↑ Non-TFM Methods DET 0.721 0.683 0.702 Ontology-Agnostic TFMs Geneformer 0.452 0.470 0.461 scGPT 0.385 0.387 0.386 scTab 0.672 0.727 0.700 UCE 0.500 0.500 0.500 MGP 0.579 0.629 0.604 Sup 0.699 0.693 0.696 MGP+Sup 0.730 0.730 0.730 Ontology-Enhanced TFMs scCello 0.756 0.729 0.743 Harmony [40] (4) scVI [44]. Cell representations were extracted from the baselines and clustered by Louvain algorithm [6]. We evaluated the clustering performance of each method on both ID dataset Did and OOD datasets Dcond i (cond∈{ct, ts, dn}, i∈{1, 2}). ID and OOD generalization. We reported zero-shot cell type clustering performance in Tab.1, and included all the metrics for all datasets in App. E.2.1 due to space constraint. For both the ID and OOD settings, scCello consistently outperforms all baselines, achieving a 16.1% improvement in AvgBio on the ID dataset and a 12.1% improvement in average AvgBio across the six OOD datasets. Interestingly, while scCello outperforms non-TFM methods by a large margin, Geneformers and scGPT barely surpass these methods. The latter is consistent with previous observations [75]. In the OOD experiments, scCello confers strong generalization capability across unseen cell types tissue, and donors. In cell type clustering, scCello is the second best only trailing UCE by 0.03 and the best method for dataset 1 and 2. The OOD tissue setting highlights scCello’s ability to transfer its learned knowledge to different unseen tissues. Specifically, scCello achieve 0.6 and 0.7 while most methods conferred below 0.6 and 0.7 for the two datasets, respectively. For the unseen OOD donor scenario, most methods perform poorly with AvgBio ranging between 0.45 and 0.55. scCello led the chart achieving AvgBio above 0.6 in both datasets. Overall, scCello showcases strong model generalization capabilities across a range of biological conditions, which is attributable to the integration of cell ontology priors during its TFM pre-training. Indeed, the ablated models namely MGP, Sup, and MGP+Sup conferred lower scores compared to the full model. 6Table 4: Cancer drug response prediction: a regression task to predict the IC50 values of drugs. MethodNon-TFM Methods Ontology-Agnostic TFMs Ontology-Enhanced TFMs DeepCDR scFoundation Geneformer scGPT scTab UCE MGP Sup MGP+SupscCello PCC↑ 0.854 0.882 0.911 0.9190.9130.9220.872 0.915 0.916 0.917 Figure 2: Novel cell type classification on OOD cell type dataset Dct 1 for increasing difficulties. Figure 3: Batch integration on the curated ID and OOD datasets. 4.2.2 Fine-tuning Results Setup. We benchmarked all TFM baselines except UCE for its lack of fine-tuning support. These TFMs were fine-tuned on a subset of our pre-training data with supervised classification loss (details in App. E.2.2). We assessed both classification and clustering performance on the ID dataset Did. We also compared with a non-TFM method, scANVI [72]. Improvement with fine-tuning. In Tab.2, The fine-tuned scCello outperforms other TFMs and non-TFM methods on both classification and clustering metrics, achieving up to 25.9% improvement in Macro F1 over the best baseline. Moreover, scCello without fine-tuning still surpasses the performance of the other fine-tuned methods, further highlighting its superior transferability. 4.3 Novel Cell Type Classification Novel cell type classification aims to label cells of unseen cell types without further fine-tuning. This task is useful for annotating completely new scRNA-seq datasets but infeasible for most of the supervised methods that solely rely on the labels observed in the training data [8, 31, 68]. Leveraging the cell ontology graph that comprises the lineage relations among all of the known cell types, scCello makes this task feasible. Setup. Our goal is to classify new query cells into \"novel cell types\" not seen during pre-training. To do this, we generate representations for both query cells and novel cell types, using similarity measures for classification. This process involves utilizing similarities between TFM-derived representations for the former and biological relationships from the cell ontology graph for the later. Details were described in App. E.3. We benchmarked all TFMs and evaluated them on OOD cell type datasetsDct 1 and Dct 2 . We increased the difficulty of this task by the number of novel cell types (#Cell Types) that exist among the query cells. Specifically, we simulated five difficulty levels, with the number of novel cell types ranging from 10% to 100% of the total cell types. To assess the variance of the performance, we randomly sampled cell type combinations 20 times at each level. OOD generalization. In Fig. 2, scCello led other TFMs by a large margin, achieving up to 76.8% Acc to classify 9 novel cell types (i.e., 10% of the total heldout cell types) and 33.5% Acc to classify up to 87 novel cell types (i.e., 100% of the total heldout cell types) (Tab. 16 and Tab. 17). These results show a significant leap from the existing TFMs, which either do not work or only work for annotating a handful of novel types [68, 45, 66]. 74.4 Marker Gene Prediction Cell-type-specific genes, or marker genes, are highly expressed in a specific cell type but exhibit low expression in others. These genes play a crucial role in delineating cell functions in diverse tissue contexts. Identifying marker genes in less characterized cell types is an ongoing challenge [52]. Setup. We sought to assess whether the pre-trained TFMs can discriminate marker from non-marker genes for any cell type without any supervised fine-tuning. This zero-shot experiment evaluates whether the TFM is able to learn biologically meaningful gene co-expression patterns without supervision. For each cell, we quantified the marker gene potential of each gene by the changes in TFM-generated cell representations after in-silico knockout of the target gene (details in App. E.4). Here we assume that the larger the change the higher the marker gene potential. We discussed the caveat of this approach in Sec. 5. As test data, we used GSE96583 [32] (Dmk 1 ) and GSE130148 [65] (Dmk 2 ) . We obtained the marker gene labels from CellMarker2 [29] and PanglaoDB [21]. We also compared with a non-TFM method, Differential Expression Tests (DET) [60]. Zero-shot transferability. In Tab. 3, scCello outperforms other TFMs, improving upon the second- best method by 1.8% in average AUROC. The inclusion of cell label information during pre-training boosts TFM performance, as evidenced by the strong results of scTab, Sup, MGP+Sup and scCello. This is due to the biological correlation between marker genes and cell types. Furthermore, employing cell ontology graphs further improves the prediction accuracy over MGP+Sup. 4.5 Cancer Drug Response Prediction Developing effective drugs for cancer treatment is challenging due to individual variability in drug responses. Accurately predicting cancer drug responses (CDR) can greatly aid anti-cancer drug development and improve our understanding of cancer biology [43]. Setup. Following the approach of scFoundation [ 26], cell representations were extracted from fixed TFMs and integrated into the DeepCDR [43] pipeline to estimate the half-maximal inhibitory concentration (IC50) values of drugs (details in App. E.6). We benchmarked our method against DeepCDR, scFoundation, and other TFM baselines, using the same pre-processed data as DeepCDR. Zero-shot transferability. In Tab. 4, scCello is among the top 3 along with scGPT and UCE, achieving 7.4% improvement in PCC over the base method DeepCDR. This highlights scCello’s transferability in enhancing specialized task-oriented methods. In particular, it can be used as an powerful feature extractor for diverse downstream tasks. 4.6 Batch Integration The scRNA-seq atlases, assembled from datasets across various labs and conditions, are prone to unwanted technical variations known as batch effects [46]. These effects can significantly affect the generalization ability of TFMs especially because they require pre-training on a massive amount of heterogeneous scRNA-seq data pooled from many studies. Here we sought to evaluate scCello’s robustness to batch effects without fine-tuning. Setup. We adopted the same baselines as in zero-shot cell type clustering (Sec.4.2.1), and followed the evaluation protocol of scGPT [14]. We evaluated on one ID dataset Did and six OOD datasets Dcond i (cond ∈ {ct, ts, dn}, i ∈ {1, 2}) (see complete results of all metrics in App. E.7). Robustness to data noise. Fig. 3 shows that scCello excels in 3 out of 7 datasets, and achieves comparable performance on another 3 datasets. The performance is attributable to the use of cell type information as the ablated baseline MGP conferred much lower batch integration score compared to Sup and scCello. 4.7 Ablation Study Ablation of pre-training losses. Tab. 5 reports the cell type clustering (Sec. 4.2.1) and novel cell type classification (Sec. 4.3) performance of scCello by using full or partial pre-training losses. Removing any of the four losses in Eqn. 5 resulted in decreased performance, corroborating the benefits of the proposed pre-training losses. Notably, removing the inter-cellular ontology relation loss LInter led to 56.1% and 65.3% decrease in terms of Acc. and Macro F1 on novel cell type 8Table 5: Pre-training loss ablation on the cell type clustering and novel cell type classification (abbr., \"clf.\") tasks. Config Cell Type Clustering Novel Cell Type Clf. Dct2 Ddn2 Dct1 AvgBio↑ AvgBio↑ Acc↑ Macro F1↑ Full Loss 0.786 0.643 0.335 0.150 w/oLMGP 0.774(↓1.5%) 0.640(↓0.5%) 0.287(↓14.3%) 0.131(↓12.7%) w/oLInter 0.778(↓1.0%) 0.620(↓3.6%) 0.147(↓56.1%) 0.052(↓65.3%) w/oLIntra 0.730(↓7.1%) 0.626(↓2.6%) 0.280(↓16.4%) 0.118(↓21.3%) w/oLReg 0.764(↓2.8%) 0.638(↓0.8%) 0.296(↓11.6%) 0.134(↓10.7%) Table 6: Overall performance v.s. the number of parameters. Method Perf. Rank #Params (M) Geneformer 6.3 10.3scGPT 6.2 51.3scTab 4.2 9.7UCE 4.8 674.7MGP 6.7 10.3Sup 3.3 10.4MGP+Sup 3.2 10.9 scCello 1.3 10.7 classification task, respectively. This shows the upmost importance of the structurally induced loss and ultimately the use of cell ontology graph information. Parameter efficiency. Tab.6 demonstrates that scCello is highly parameter-efficient, utilizing up to 60 times fewer parameters than the largest existing TFM, UCE, while still achieving the best average performance rankings across all downstream tasks. With an average performance rank of 1.3, scCello consistently ranks first or near the top in nearly every task. Visualization. Visualization and analysis of scCello’s learned cell representations were presented in App. E.8. In short, biologically similar cell types are closer to each other and farther from those dissimilar ones in the t-SNE 2D space (Fig. 11). 5 Discussion and Conclusion Limitation and future work. The cell ontology is constantly revised and expanded. In the future, we plan to investigate more efficient methods for fine-tuning scCello to enable continual learning of updated ontology, rather than retraining the entire model. Additionally, we aim to scale up the model size of scCello to increase its expressiveness and capacity. For the zero-shot marker gene prediction experiments (Sec. 4.4), one caveat is that our in-silico gene knockout approach also detects essential genes such as housekeeping genes [17] and transcription factors that are master regulators [9], which may not necessarily be marker genes. Nonetheless, deletion of these influential genes will also lead to large change of the transcriptome landscape of the cell. We will explore this in future study. Societal impact. This work proposes a novel cell ontology-guided TFM, scCello, to enhance cell representation learning. On the positive side, once pre-trained, scCello can serve as a foundational model capable of facilitating scientific discoveries across various downstream tasks related to cells and cellular processes. However, on the negative side, the pre-training of scCello requires significant computational resources, potentially resulting in substantial carbon dioxide emissions that could contribute to environmental harm. Conclusion. The proposed scCello incorporates cell ontology knowledge into its pre-training process by simultaneously modeling at the gene level, intra-cellular level, and inter-cellular level. We constructed a large-scale cell type identification benchmark to evaluate the model’s generalization capabilities, both in-distribution and out-of-distribution. Our evaluation demonstrates that scCello also exhibits strong transferability, as evidenced by its performance on other biologically meaningful downstream tasks such as zero-shot novel cell type classification and cell-type-specific marker gene prediction. Foundational models are typically heavy on the parameters for them to have sufficient capacity to learn from unlabeled data from scratch. This limits their usage to only fine-tuning tasks as pre-training them is prohibitive without large compute. Our proposed approach provides an efficient way of leveraging the prior knowledge at the pre-training, which led to much smaller parameter size while achieving performance comparable of the TFMs that are 5-60 times bigger. Together, scCello is a knowledge-informed and general purpose deep learning model that can be fine-tuned for a wide array of downstream applications, aiding in the rapid identification of novel cell types, disease-associated genes, and effective cancer drugs. Acknowledgments and Disclosure of Funding The authors would like to thank Chence Shi, Meng Qu, Zhaocheng Zhu, and Sophie Xhonneux for their helpful discussions and comments. We also appreciate all anonymous reviewers for their 9constructive suggestions. This project is supported by Intel-MILA partnership program, the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R&D Project (AI4D-CORE-06). This project was also partially funded by IV ADO Fundamental Research Project grant PRF-2019-3583139727. Y .L. is supported by Canada Research Chair (Tier 2) in Machine Learning for Genomics and Healthcare (CRC-2021-00547) and Natural Sciences and Engineering Research Council(NSERC) Discovery Grant (RGPIN-2016-05174). The computation resource of this project is supported by Mila, Calcul Québec and the Digital Research Alliance of Canada. References [1] Shibla Abdulla, Brian D. Aevermann, Pedro Assis, Seve Badajoz, Sidney M. Bell, Emanuele Bezzi, Batuhan Cakir, Jim Chaffer, Signe Chambers, J. Michael Cherry, Tiffany Chi, Jennifer Chien, Leah Dorman, Pablo Garcia-Nieto, Nayib Gloria, Mim Hastie, Daniel Hegeman, Jason Hilton, Timmy Huang, Amanda Infeld, Ana-Maria Istrate, Ivana Jelic, Kuni Katsuya, Yang-Joon Kim, Karen Liang, Mike Lin, Maximilian Lombardo, Bailey Marshall, Bruce Martin, Fran McDade, Colin Megill, Nikhil Patel, Alexander V . Predeus, Brian Raymor, Behnam Robatmili, Dave Rogers, Erica Rutherford, Dana Sadgat, Andrew Shin, Corinn Small, Trent Smith, Prathap Sridharan, Alexander Tarashansky, Norbert Tavares, Harley Thomas, Andrew Tolopko, Meg Urisko, Joyce Yan, Garabet Yeretssian, Jennifer Zamanian, Arathi Mani, Jonah Cool, and Ambrose J. Carr. Cz cell×gene discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data. bioRxiv, 2023. [2] Peter Angel and Michael Karin. The role of jun, fos and the ap-1 complex in cell-proliferation and transformation. Biochimica et Biophysica Acta (BBA)-Reviews on Cancer, 1072(2-3):129– 157, 1991. [3] Sercan Ö Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In Proceed- ings of the AAAI conference on artificial intelligence, volume 35, pages 6679–6687, 2021. [4] Jonathan Bard, Seung Y Rhee, and Michael Ashburner. An ontology for cell types. Genome biology, 6:1–5, 2005. [5] Nurken Berdigaliyev and Mohamad Aljofan. An overview of drug discovery and development. Future medicinal chemistry, 12(10):939–947, 2020. [6] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008, 2008. [7] Felipe A. Vieira Braga, Gozde Kar, Marijn Berg, Orestes A. Carpaij, Krzysztof Pola ´nski, Lukas M. Simon, Sharon Brouwer, Tomás Gomes, Laura Hesse, Jian Jiang, Eirini Sofia Fasouli, Mirjana Efremova, Roser Vento-Tormo, Carlos Talavera-López, Marnix R. Jonker, Karen Affleck, Subarna Palit, Paulina M. Strzelecka, Helen V . Firth, Krishnaa T. Mahbubani, Ana Cvejic, Kerstin B. Meyer, Kourosh Saeb-Parsy, Marjan A. Luinge, Corry-Anke Brandsma, Wim Timens, Ilias Angelidis, Maximilian Strunz, Gerard H. Koppelman, Antoon J. M. van Oosterhout, Herbert B. Schiller, Fabian J Theis, Maarten van den Berge, Martijn C. Nawijn, and Sarah A. Teichmann. A cellular census of human lungs identifies novel cell states in health and in asthma. Nature Medicine, 25:1153 – 1163, 2019. [8] Maria Brbic, Marinka Zitnik, Sheng Wang, Angela Oliveira Pisco, Russ B. Altman, Spyros Darmanis, and Jure Leskovec. Mars: discovering novel cell types across heterogeneous single- cell experiments. Nature Methods, 17:1200 – 1206, 2020. [9] Sunny Sun-Kin Chan and Michael Kyba. What is a master regulator? Journal of stem cell research & therapy, 3, 2013. [10] Ruben Chazarra-Gil, Stijn van Dongen, Vladimir Yu Kiselev, and Martin Hemberg. Flexible comparison of batch correction methods for single-cell rna-seq using batchbench. Nucleic acids research, 49(7):e42–e42, 2021. 10[11] Fan Chen, Yini Zhang, and Karl Rohe. Targeted sampling from massive block model graphs with personalized pagerank. Journal of the Royal Statistical Society Series B: Statistical Methodology, 82(1):99–126, 2020. [12] David Combe, Christine Largeron, Mathias Géry, and El ˝od Egyed-Zsigmond. I-louvain: An attributed graph clustering method. In Advances in Intelligent Data Analysis XIV: 14th International Symposium, IDA 2015, Saint Etienne. France, October 22-24, 2015. Proceedings 14, pages 181–192. Springer, 2015. [13] William Connell, Umair Khan, and Michael J Keiser. A single-cell gene expression language model. arXiv preprint arXiv:2210.14330, 2022. [14] Haotian Cui, Chloe X. Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. scgpt: toward building a foundation model for single-cell multi-omics using genera- tive ai. Nature methods, 2024. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [16] Alexander D. Diehl, Terrence F. Meehan, Yvonne M. Bradford, Matthew H. Brush, Wasila M. Dahdul, David S. Dougall, Yongqun He, David Osumi-Sutherland, Alan Ruttenberg, Sirarat Sarntivijai, Ceri E. Van Slyke, Nicole A. Vasilevsky, Melissa A. Haendel, Judith A. Blake, and Christopher J. Mungall. The cell ontology 2016: enhanced content, modularization, and ontology interoperability. Journal of biomedical semantics, 7(44), 2016. [17] Eli Eisenberg and Erez Y Levanon. Human housekeeping genes, revisited.TRENDS in Genetics, 29(10):569–574, 2013. [18] Felix Fischer, David S Fischer, Evan Biederstedt, Alexandra-Chloé Villani, and Fabian J Theis. Scaling cross-tissue single-cell annotation models. bioRxiv, 2023. [19] Felix Fischer, David S. Fischer, Evan Biederstedt, Alexandra-Chloé Villani, and Fabian J. Theis. Scaling cross-tissue single-cell annotation models. bioRxiv, 2023. [20] Dániel Fogaras, Balázs Rácz, Károly Csalogány, and Tamás Sarlós. Towards scaling fully personalized pagerank: Algorithms, lower bounds, and experiments. Internet Mathematics, 2(3):333–358, 2005. [21] Oscar Franzén, Li-Ming Gan, and Johan LM Björkegren. Panglaodb: a web server for explo- ration of mouse and human single-cell rna sequencing data. Database, 2019:baz046, 2019. [22] Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised contrastive learning. In International Conference on Machine Learning , pages 3821–3830. PMLR, 2021. [23] Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008. [24] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017. [25] Minsheng Hao, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Le Song, and Xuegong Zhang. Large scale foundation model on single-cell transcriptomics. bioRxiv, pages 2023–05, 2023. [26] Minsheng Hao, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Le Song, and Xuegong Zhang. Large scale foundation model on single-cell transcriptomics. bioRxiv, 2023. [27] Yuhan Hao, Tim Stuart, Madeline H Kowalski, Saket Choudhary, Paul Hoffman, Austin Hartman, Avi Srivastava, Gesmira Molla, Shaista Madad, Carlos Fernandez-Granda, et al. Dictionary learning for integrative, multimodal and scalable single-cell analysis. Nature biotechnology, 42(2):293–304, 2024. 11[28] Graham Heimberg, Tony Kuo, Daryle DePianto, Tobias Heigl, Nathaniel Diamant, Omar Salem, Gabriele Scalia, Tommaso Biancalani, Shannon Turley, Jason Rock, et al. Scalable querying of human cell atlases via a foundational model reveals commonalities across fibrosis-associated macrophages. bioRxiv, pages 2023–07, 2023. [29] Congxue Hu, Tengyue Li, Yingqi Xu, Xinxin Zhang, Feng Li, Jing Bai, Jing Chen, Wenqi Jiang, Kaiyue Yang, Qi Ou, et al. Cellmarker 2.0: an updated database of manually curated cell markers in human/mouse and web tools based on scrna-seq data. Nucleic Acids Research, 51(D1):D870–D876, 2023. [30] Congxue Hu, Tengyue Li, Yingqi Xu, Xinxin Zhang, Feng Li, Jing Bai, Jingrun Chen, Wenqi Jiang, Kaiyue Yang, Qi Ou, Xia Li, Peng Wang, and Yunpeng Zhang. Cellmarker 2.0: an updated database of manually curated cell markers in human/mouse and web tools based on scrna-seq data. Nucleic Acids Research, 51:D870 – D876, 2022. [31] Aleksandr Ianevski, Anil K. Giri, and Tero Aittokallio. Fully-automated and ultra-fast cell-type identification using specific marker combinations from single-cell transcriptomic data. Nature Communications, 13, 2022. [32] Hyun Min Kang, Meena Subramaniam, Sasha Targ, Michelle Nguyen, Lenka Maliskova, Elizabeth McCarthy, Eunice Wan, Simon Wong, Lauren Byrnes, Cristina M Lanata, et al. Multi- plexed droplet single-cell rna-sequencing using natural genetic variation. Nature biotechnology, 36(1):89–94, 2018. [33] Hyun Min Kang, Meena Subramaniam, Sasha Targ, Michelle Ly Thai Nguyen, Lenka Maliskova, Elizabeth E. McCarthy, Eunice Wan, Simon Wong, Lauren E. Byrnes, Cristina M. Lanata, Rachel E. Gate, Sara Mostafavi, Alexander Marson, Noah A. Zaitlen, Lindsey A. Criswell, and Chun Jimmie Ye. Multiplexed droplet single-cell rna-sequencing using natural genetic variation. Nature biotechnology, 36:89 – 94, 2017. [34] Kasia Zofia Kedzierska, Lorin Crawford, Ava Pardis Amini, and Alex X Lu. Assessing the limits of zero-shot foundation models in single-cell biology. bioRxiv, pages 2023–10, 2023. [35] Hadas Keren-Shaul, Ephraim Kenigsberg, Diego Adhemar Jaitin, Eyal David, Franziska Paul, Amos Tanay, and Ido Amit. Mars-seq2. 0: an experimental and analytical pipeline for indexed sorting combined with single-cell rna sequencing. Nature protocols, 14(6):1841–1862, 2019. [36] Shima Khoshraftar and Aijun An. A survey on graph representation learning methods. ACM Transactions on Intelligent Systems and Technology, 15(1):1–55, 2024. [37] Cheorl-Ho Kim. Glycobiology of innate immunology. Springer, 2022. [38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [39] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [40] Ilya Korsunsky, Nghia Millard, Jean Fan, Kamil Slowikowski, Fan Zhang, Kevin Wei, Yuriy Baglaenko, Michael Brenner, Po-ru Loh, and Soumya Raychaudhuri. Fast, sensitive and accurate integration of single-cell data with harmony. Nature methods, 16(12):1289–1296, 2019. [41] Samuel A Lambert, Arttu Jolma, Laura F Campitelli, Pratyush K Das, Yimeng Yin, Mihai Albu, Xiaoting Chen, Jussi Taipale, Timothy R Hughes, and Matthew T Weirauch. The human transcription factors. Cell, 172(4):650–665, 2018. [42] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive representation learning: A framework and review. Ieee Access, 8:193907–193934, 2020. [43] Qiao Liu, Zhiqiang Hu, Rui Jiang, and Mu Zhou. Deepcdr: a hybrid graph convolutional network for predicting cancer drug response. bioRxiv, 2020. 12[44] Romain Lopez, Jeffrey Regier, Michael B Cole, Michael I Jordan, and Nir Yosef. Deep generative modeling for single-cell transcriptomics. Nature methods, 15(12):1053–1058, 2018. [45] Mohammad Lotfollahi, Mohsen Naghipourfar, Malte D. Luecken, Matin Khajavi, Maren Büttner, Marco Wagenstetter, Žiga Avsec, Adam Gayoso, Nir Yosef, Marta Interlandi, Sergei Rybakov, Alexander V . Misharin, and Fabian J Theis. Mapping single-cell data to reference atlases by transfer learning. Nature Biotechnology, 40:121 – 130, 2021. [46] Malte D. Luecken, Maren Büttner, Kridsadakorn Chaichoompu, Anna Danese, Marta Inter- landi, MF Mueller, D Strobl, Luke Zappia, Martin Dugas, Maria Colomé-Tatché, and F Theis. Benchmarking atlas-level data integration in single-cell genomics. Nature Methods, 19:41 – 50, 2020. [47] Malte D Luecken, Maren Büttner, Kridsadakorn Chaichoompu, Anna Danese, Marta Interlandi, Michaela F Müller, Daniel C Strobl, Luke Zappia, Martin Dugas, Maria Colomé-Tatché, et al. Benchmarking atlas-level data integration in single-cell genomics.Nature methods, 19(1):41–50, 2022. [48] Colin Megill, Bruce Martin, Charlotte Weaver, Sidney Bell, Lia Prins, Seve Badajoz, Brian McCandless, Angela Oliveira Pisco, Marcus Kinsella, Fiona Griffin, et al. Cellxgene: a performant, scalable exploration platform for high dimensional sparse matrices. bioRxiv, pages 2021–04, 2021. [49] Gyutaek Oh, Baekgyu Choi, Inkyung Jung, and Jong Chul Ye. schyena: Foundation model for full-length single-cell rna-seq analysis in brain. arXiv preprint arXiv:2310.02713, 2023. [50] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit- learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. [51] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning , pages 28043–28078. PMLR, 2023. [52] Yixuan Qiu, Jiebiao Wang, Jing Lei, and Kathryn Roeder. Identification of cell-type-specific marker genes from co-expression patterns in tissue samples. Bioinformatics, 37(19):3228–3234, 2021. [53] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [54] William M Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846–850, 1971. [55] Yanay Rosen, Yusuf Roohani, Ayush Agrawal, Leon Samotorcan, Tabula Sapiens Consortium, Stephen R Quake, and Jure Leskovec. Universal cell embeddings: A foundation model for cell biology. bioRxiv, pages 2023–11, 2023. [56] Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53–65, 1987. [57] T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993, 2023. [58] Robert Salomon, Dominik Kaczorowski, Fatima Valdes-Mora, Robert E Nordon, Adrian Neild, Nona Farbehi, Nenad Bartonicek, and David Gallego-Ortega. Droplet-based single cell rnaseq tools: a practical guide. Lab on a Chip, 19(10):1706–1727, 2019. [59] Hongru Shen, Jilei Liu, Jiani Hu, Xilin Shen, Chao Zhang, Dan Wu, Mengyao Feng, Meng Yang, Yang Li, Yichen Yang, et al. Generative pretraining from large-scale transcriptomes for single-cell deciphering. Iscience, 26(5), 2023. 13[60] Charlotte Soneson and Mark D Robinson. Bias, robustness and scalability in single-cell differential expression analysis. Nature methods, 15(4):255–261, 2018. [61] Christina V . Theodoris, Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R Al Sayed, Matthew C. Hill, Helene Mantineo, Elizabeth M Brydon, Zexian Zeng, X. Shirley Liu, and Patrick T. Ellinor. Transfer learning enables predictions in network biology. Nature, 618:616– 624, 2023. [62] Ander Urruticoechea, Ramon Alemany, J Balart, Alberto Villanueva, Francesc Vinals, and Gabriel Capella. Recent advances in cancer therapy: an overview. Current pharmaceutical design, 16(1):3–10, 2010. [63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [64] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [65] Felipe A Vieira Braga, Gozde Kar, Marijn Berg, Orestes A Carpaij, Krzysztof Polanski, Lukas M Simon, Sharon Brouwer, Tomás Gomes, Laura Hesse, Jian Jiang, et al. A cellular census of human lungs identifies novel cell states in health and in asthma. Nature medicine, 25(7):1153– 1163, 2019. [66] Hui Wan, Liang Chen, and Min Deng. scemail: Universal and source-free annotation method for scrna-seq data with novel cell-type perception. Genomics, Proteomics & Bioinformatics, 20:939 – 958, 2022. [67] Hanzhi Wang, Zhewei Wei, Junhao Gan, Sibo Wang, and Zengfeng Huang. Personalized pagerank to a target node, revisited. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 657–667, 2020. [68] Wenchuan Wang, Fan Yang, Yuejing Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao. scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data. Nature Machine Intelligence, 4:852 – 866, 2022. [69] Xiliang Wang, Yao He, Qiming Zhang, Xianwen Ren, and Zemin Zhang. Direct comparative analyses of 10x genomics chromium and smart-seq2.Genomics, Proteomics and Bioinformatics, 19(2):253–266, 2021. [70] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [71] F Alexander Wolf, Philipp Angerer, and Fabian J Theis. Scanpy: large-scale single-cell gene expression data analysis. Genome biology, 19:1–5, 2018. [72] Chenling Xu, Romain Lopez, Edouard Mehlman, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models. Molecular systems biology, 17(1):e9620, 2021. [73] Fan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao. scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data. Nature Machine Intelligence, 4(10):852–866, 2022. [74] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, and Xing Xie. Graphformers: Gnn-nested transformers for representation learning on textual graph. Advances in Neural Information Processing Systems , 34:28798– 28810, 2021. [75] Hongyu Zhao, Tianyu Liu, Kexing Li, Yuge Wang, and Hongyu Li. Evaluating the utilities of large language models in single-cell data analysis. 2023. [76] Suyuan Zhao, Jiahuan Zhang, and Zaiqing Nie. Large-scale cell representation learning via divide-and-conquer contrastive learning. arXiv preprint arXiv:2306.04371, 2023. 14random walk path from u to krandom walk path from u to vv k u … vk u…… PPR𝑢,. Native cellNucleate cell Regulatory T cell PPR𝑢,𝑘 PPR𝑢,𝑣 …v k u … Cell Ontology GraphRandom WalkerPersonalized PageRank (PPR) Figure 4: Graphical illustration of applying the Personalized PageRank (PPR) algorithm to cell ontology graph. As explained in App. A, PPR conducts random walks over the ontology graph with respect to a target cell type u, and converges to a steady state when the likelihood of terminating on each node stabilizes into a steady distribution. This likelihood distribution determines the final PPR score PPR(·) and reflects the structural similarity between cell types. A PPR Transformation Personalized PageRank (PPR). Personalized PageRank (PPR) extends the classic PageRank algo- rithm, which Google originally developed to rank web pages in search engines. PageRank conducts this by analyzing large-scale hyperlinked graphs on the web using random walker simulations. Unlike traditional PageRank that assigns a universal score to each web page, PPR customizes these scores. Specifically, individual user preferences during searches are incorporated, so that PPR can focus on web pages particularly relevant to each user. Due to its flexibility and effectiveness, PPR has been widely applied in graph learning across various fields, such as social networks, recommendation systems, and biological data analysis. As illustrated in Fig. 4, this algorithm starts with a predefined preference node (or target node), which is emphasized according to the user’s interests. Subsequently, a random walk is conducted on the graph to facilitate graph traversal. At each step of the walk, there is a fixed probability α that the walker will jump back to the target node from the current node instead of moving to an adjacent node chosen at random. This process of jumping, commonly referred to as \"teleportation\", biases the walk towards subgraphs that are of particular importance to the target node, thus personalizing the results according to user preferences. The walk continues until it reaches a steady state, at which point the likelihood of being on each node stabilizes into a steady-state distribution. These stabilized (a) Distribution of PPR scores.  (b) Distribution of structural similarity. Figure 5: Comparison of the distributions for the PPR scores PPR(·) and the structural similarity sim(·) after the transformation. 15Figure 6: Relationships between the structural similariity sim(·) after PPR transformation and the original PPR scores PPR. Figure 7: Frequency for each target cell type to be associated with other cell types that is at specific levels of structural similarity. probabilities, reflecting both the graph’s structure and the user’s preferences, determine the PPR scores. These scores effectively evaluate each node’s structural similarities and rank them according to their relevance and importance from a personalized perspective. PPR transformation. In scCello, the PPR algorithm is applied to the cell ontology graph to assess the structural similarities among cell types, or to measure their importance relative to a specified target cell type. We implemented PPR using the \"pagerank\" function in NetworkX [ 23] with \"personalization\" as arguments. However, modification is needed to integrate PPR into TFM pre-training. The PPR scores are in real-number format and susceptible to numerical noise. Also, as shown in Fig. 5a, these scores typically exhibit a skewed distribution, concentrated around lower magnitudes. Consequently, setting precise thresholds to differentiate between node similarity and dissimilarity is challenging. Moreover, the vast amount small PPR values may be indistinguishable from noise. To mitigate the effects of numerical noise and skewed magnitudes for the PPR scores, we employ truncation, logarithmic scaling, and discretization as outlined in Eqn. 3. Note that Eqn. 3 defines a monotonic, non-decreasing function that preserves the relative order between nodes. Its minimum value is set to 1 for the least similar cell types. This equation transforms the raw PPR score, PPR(·), into the final structural similarity, sim(·). This transformation ensures that sim(·) accurately reflects pronounced similarities as defined by the cell ontology and avoids emphasizing minor dissimilarities that could mislead during TFM pre-training. Analyses. In Fig. 5, we present a comparison of the distributions for the PPR score, PPR(·), and the transformed structural similarity, sim(·). After transformation, the distribution of sim(·) is less skewed and exhibits clear discretization. This facilitates the setting of definitive thresholds for distinguishing between similarity and dissimilarity among cell types, thereby enabling the effective incorporation of the cell ontology graph in scCello’s pre-training. In addition, we provide detailed insights into the scale of structural similarity, the distribution of these similarities for each cell type, and examples of cell types associated with various levels of structural similarity: (1) Fig. 6 illustrates the correspondence between the structural similarity after PPR transfor- mation and the original PPR scores, showcasing a log-linear relationship as expected. This helps clarify the scaling of structural similarity, which is discretized into integer levels ranging from 1 to 11. (2) Fig. 7 demonstrates how frequently each target cell type is associated with other cell types at specific levels of structural similarity. Consequently, during scCello’s pre-training, a substantial number of negative samples are expected to be utilized in the inter-cellular relational alignment objective, as outlined in Sec. 2.4. 16Table 7: Examples of cell types associated with various levels of structural similarity, sim(·), for specified target cell types. Cell types demonstrated in the cell ontology graph in Fig. 1 are underlined. Target Typesim(·) Corresponding Cell Types T Cell 8 \"gamma-delta T cell\", \"mature T cell\" , \"lymphocyte\" 7 \"mature gamma-delta T cell\", \"α-β T cell\", \"matureα-β T cell\", \"thymocyte\" 6 \"B cell\", \"double-positive,α-β thymocyte\", \"CD8-positive,α-β thymocyte\", \"CD4-positive,α-β T cell\", \"CD8-positive,α-β T cell\", \"double negative thymocyte\" 5 \"dendritic cell\", \"innate lymphoid cell\", \"plasmablast\", \"mononuclear cell\", \"regulatory T cell\", \"memory T cell\", \"myeloid leukocyte\", \"naive T cell\", \"mature B cell\", \"CD4-positive, CD25-positive,α-β regulatory T cell\" . . . . . . 1 \"renal intercalated cell\", \"smooth muscle fiber of ileum\", \"type II pneumocyte\", \"hematopoietic cell\", \"neuron\", \"common lymphoid progenitor\",··· Neuron 7 \"secretory cell\" 6 \"glutamatergic neuron\", \"GABAergic neuron\", \"motor neuron\", \"neural cell\", \"peripheral nervous system neuron\" 5 \"glycinergic neuron\", \"retinal bipolar neuron\", \"native cell\", \"enteric neuron\", \"retina horizontal cell\", \"amacrine cell\", \"neuronal receptor cell\" 4 \"retinal ganglion cell\", \"endocrine cell\", \"neuroendocrine cell\", \"cerebral cortex GABAergic interneuron\", \"muscle cell\", \"somatic cell\" . . . . . . 1 \"germ cell\", \"T cell\", \"tracheal goblet cell\", \"DN3 thymocyte\", \"promonocyte\", \"cerebral cortex endothelial cell\",··· (3) Tab. 7 displays examples of highly similar and dissimilar cell types categorized into various levels of structural similarity, specifically targeting \"T cell\" and \"neuron\" types. B Data Preprocessing Details Download and Preprocessing. We downloaded from CellxGene [1] census version 2023-7-25. We focused on 291 datasets for human scRNA-seq. We preprocessed the dataset by the following steps: (1) Remove non-primary cells. Some data on CellxGene was duplicated due to multiple submissions of the same dataset from different research groups, therefore cells marked as \"non-primary\" were filtered out to prevent label leakage between pre-training and down- stream. (2) Filter out cells not produced by 10x-based [69] sequencing protocols. There are numerous sequencing protocols in CellxGene database besides 10x-based sequencing [69], such as Drop-seq [58] and MARS-seq [ 35]. Only sequencing data from 10x-based sequencing protocols was kept to avoid large variation of data signals [46]. (3) Exclude cancer cells. Cancer cells were highly dissimilar to normal cells and even occupied a large amount in the CellxGene database (nearly 12%). These cells could bring unexpected signals and skew the data, therefore we excluded these cancer cells. To build downstream datasets for out-of-distribution (OOD) generalization evaluation, we first held out two category sets for each of the three settings: unseen cell types, unseen tissues and unseen 17Table 8: Data statistics for our curated pre-training and downstream datasets, where the downstream datasets encompass one ID dataset and six OOD datasets under three different OOD scenarios, including unseen cell types, unseen tissues and unseen donors (Sec. 4.1). The blue colored numbers represent disjoint categories of that column. For example, in the \"cell type\" column, the cell type set in the pre-training data, and the cell type set in the OOD cell type dataset Dct 1 and Dct 1 are disjoint. Dataset #Total Cells #Cell Types #Tissues #Donors #Conditions #Batches Pre-training data 22,293,755 398 140 4,103 55 267 ID datasetDid 22,317 318 132 3,447 54 261 OOD cell type datasetDct1 486,810 87 125 122 35 90 OOD cell type datasetDct2 435,791 87 128 106 40 117 OOD tissue datasetDts1 335,675 186 32 1,801 10 28 OOD tissue datasetDts2 341,681 205 32 2,052 7 25 OOD donor datasetDdn1 2,528,134 439 91 525 36 127 OOD donor datasetDdn2 2,521,868 404 101 525 33 123 In Total / 572 204 5153 / / donors. Each category set were randomly selected with selection ratios 15%, 15% and 10% for the three OOD settings respectively. During the selection, we prohibited any category associated with more than 0.1% of the total pre-processed cells from being selected. This avoids losing too much data for pre-training. After the selection, cells associated with each held category set are collected, resulting in two OOD downstream datasets for each of the three OOD settings. These datasets are denoted as {Dct i }2 i=1 for the OOD cell type setting, {Dts i }2 i=1 for the OOD tissue setting, and {Ddn i }2 i=1 for the OOD donor setting. By excluding cells with at least one property belong to any of the six held category sets, the remaining data is further split into 99.9% as our pre-training data and 0.1% as the in-distribution (ID) downstream dataset Did. This way, our pre-training data and the ID dataset Did share similar data distributions. Data Statistics. We summarize the data statistics for our curated pre-training dataset, one ID dataset and six OOD datasets in Tab. 8. C Discussion on Ontology Graph Modeling Graph Neural Networks (GNNs). One essential component of scCello is to model the cell ontology prior graph. GNNs are essential for managing graph-structured data by using a process called message passing [ 36]. This process lets nodes gather information from their neighbors, capturing their local connections and features, as seen in technologies like GCN [ 39]. Further advancements like GraphSAGE [24] and GAT [64] allow GNNs to handle larger areas of the graph and more complex relationships, useful in fields from social networks to drug discovery. Recently, combining GNNs with transformer models, like GraphFormer [74], has proven effective. This integration allows the models to process both textual and graph data simultaneously, enhancing the understanding of the graph’s structure without needing external measures. Why we left GNNs for future work. While using GNNs in TFM modeling can internalize ontology graph knowledge in TFM modeling and no custom metric like that for PPR transformation is necessary, the cellular ontology graph presents specific challenges that hinder effective direct modeling by GNNs. The key issues is that the ontology graph is extremely sparse with about 2.7k nodes and 3.9k edges, and faces long-distance issues. For example, the average pairwise distance of 398 nodes (i.e., cell types) associated with our pre-training scRNA-seq data is 7.39, and the maximum distance is 18. Therefore, it requires multiple layers of graph propagations (around 7 layers), risking over-smoothing [57] of cell type representations. 18Table 9: Hyper-parameters comparison between TFM baselines (introduced in Sec.4.1) and our TFM scCello. \"The number of\" is denoted with the symbol #. Configuration Geneformer [61] scGPT [14] scTab [19] UCE [55] scCello #Parameters 10,316,196 51,330,049 9,655,628 674,745,857 10,683,654 Total GPUs 12 * V100 (32G) 4 * A100 1 * A100 24 * A100 (80G) 4 * A100 (40G) Training Time 3 days 3 days / 43.5 days 2 days Sequence Length 2,048 1,200 19,331 1,024 [N] 2,048 Gene Mask Ratio 15% / / 20% 15% Batch Size Per GPU 12 32 2,048 6 12 Gradient Accumula- tion Steps 1 1 1 4 4 Effective Batch Size 144 128 2048 576 192 Cell Reprs. Avg. pooling CLS / CLS CLS #Genes in Token V ocabulary 25,424 48,292 19,331 Any protein- coding genes 25,424 #Transformer Layers 6 12 / 33 6 Transformer Layer Hidden Dimension 512 512 / 5,120 512 Transformer Layer Embedding Size 256 512 / 1,280 256 #Transformer Heads 4 8 / 20 4 Transformer Layer Activation Function GeLU ReLU / ReLU ReLU MLP Layer Acti- vation Function ReLU ReLU / GeLU ReLU Dropout 0.02 0.2 / 0.05 0.02 D Implementation Details scRNA-seq Data. scRNA-seq can enable the quantification of gene expression profiles of individual cells. Each cell’s gene expression profile can be described by the set ˆX = {(e1, g1), (e2, g2), . . . ,(eM , gM )}, where ek denotes the expression count of gene gk, with ek ≥ 0. A value of ek = 0 indicates that the gene gk is not expressed or not detected by the sequencing experiment. We use the same gene vocabulary set as [61], with the number of genes M=25, 424. Gene Token Vocabulary. The gene vocabulary set contains both protein-coding genes and miRNA genes. M, the number of genes, is not the same as the number of all tokens in the model vocabulary. scCello has M gene tokens plus three more special tokens [MASK] for masking, [CLS] for the start of a sentence and [PAD] for padding. Rank Value Encoding. Unlike natural languages, which inherently follow a sequential order, scRNA-seq data presents a unique challenge due to the lack of intrinsic order among gene tokens. Therefore, we employ Rank Value Encoding [61] approach to rank genes based on their normalized expression set {(˜ei, gi)}M i=1. Specifically, gene expressions are first normalized by the total count within a cell [71] in a cell-wise manner, and then normalized through gene-specific weighting factors in a gene-wise manner. These factors are adopted from [61], which calculates the non-zero median value of expression of each detected gene across all cells. By design, these factors are assigned to emphasize lowly-expressed but essential genes, such as transcription factors [41], while deprioritizing ubiquitously expressed housekeeping genes [17]. After the normalization and ranking, it results in an ordered sequence of gene identities X = [gπ(1), gπ(2), . . . , gπ(M)] with an index permutation π(·), satisfying ˜eπ(1) ≥ ˜eπ(2) ≥ ··· ≥˜eπ(M). To mitigate memory consumption, zero-expressed genes are removed and the gene sequence is 19Table 10: Metrics used in downstream tasks. Task Metrics Cell Type Clustering (Sec. 4.2.1) NMI, ARI, ASW, AvgBio Cell Type Classification (Sec. 4.2.2) Acc, Macro F1, AvgBio, ∆AvgBio Novel Cell Type Classification (Sec. 4.3) Acc, Macro F1 Marker Gene Prediction (Sec. 4.4) AUROC Cancer Drug Response Prediction (Sec. 4.5) PCC Batch Integration (Sec. 4.6) NMI, ARI, ASW, AvgBio, ASWb, GraphConn, AvgBatch, Overall further truncated with a context length L=2, 048 in practice. This rank-based approach offers better robustness against technical artifacts than directly using the original numerical expressions, which can vary significantly in magnitude across different experimental assays [46]. Cell and Cell Type Representations. Given a pre-training dataset with N cells X = {X1, X2, . . . , XN }, each cell Xi can be mapped to a specific cell type ontology identifier ci ∈ V. For analyzing, scCello denotes cell Xi’s representation as zi and cell type ci’s representation as hci. Masked Gene Prediction. Given a batch of cells {Xi}B i=1, scCello predicts a gene token gk based on the ordered gene sequence context Xi,\\k=[g1, . . . , gk−1, [MASK], gk+1 . . . , gM ] after replacing the token with a special [MASK]. This objective (term as LMGP) aims to capture complex but important gene-gene interactions within one cell, like regulatory mechanisms between transcription factors and other genes: LMGP = −PB k=1 Ei∼Ψ − log p(xi|Xk,\\i) (6) where tokens are masked by a pre-defined distribution Ψ, same as that in BERT [15]. Specifically, 80% selected genes are replaced with [MASK], 10% selected genes are kept the same as its original, and 10% selected genes are replaced with random gene tokens. Model Architecture. scCello utilizes a stack of self-attention transformer encoder layers [ 63], eacg composed of a self-attention and feedforward neural networks. The self-attention mechanism processes the input sequence, effectively capturing interactions between gene tokens. Configuration Hyper-parameters. Besides scCello, we also summarize essential hyper-parameters for TFM baselines in Tab.9 for comparison. It includes pre-training configurations like batch size, sequence length, and training time consumed. It also includes architecture configurations for the transformer model backbone, such as the number of transformer layers and the embedding size of transformer layers. Note that scTab uses TabNet [3] instead of transformer layers as model backbone, therefore its architecture configurations are not recorded in the table. E Downstream Experiment Details E.1 Evaluation Metrics All metrics used in downstream tasks are summarized in Tab. 10 and introduced below. Normalized Mutual Info Score (NMI). The NMI is a metric that quantifies the similarity between two differen clustering assignments or labelings of the same set of samples. We use NMI to compare the cell-type labels, with the cluster indices obtained from applying the Louvain clustering algorithm [12] on the target dataset. We denote the two label assignments of the sameN cell samples as C and K, representig the cell-type labels and the Louvain cluster indices, respectively. The entropy of a label assignment, sayC, is a 20measure of the uncertainty associated with that assignment set. It’s calculated as: H(C) = − |C|X i=1 P(i) logP(i) (7) where |C| is the number of unique cell types and P(i) = |Ci| N is the probability that a randomly selected sample belongs to the class Ci. The entropy H(K) for the cluster indices K is computed similarly, with Q(j) = |Kj| N being the probability of a sample belonging to the cluster Kj: H(K) = − |K|X j=1 Q(j) logQ(j) (8) The mutual information (MI) between C and K qunatifies the amount of information shared between the two label assignments. It is calculated by: MI(C, K) = |C|X i |K|X j R(i, j) log R(i, j) P(i)Q(j) (9) where R(i, j) = |Ci∩Kj| N is the probability that a randomly selected sample belongs to both the class Ci and the cluster Kj. The normalized mutual information (NMI) is defined as: NMI(C, K) = MI(C, K) mean(H(C), H(K)) (10) NMI is a normalized version of MI, scaled by the mean of the entropy terms for cell-type labels and cluster indices. This normalization ensures that NMI values range from 0 to 1, where 0 indicates no correlation between the two label assignments, and 1 represents a perfect match. To obtain the best match between the clusters and the cell-type labels, we performed optimized Louvain clustering over a range of resolutions from 0.1 to 2, in steps of 0.1. The clustering output with the highest NMI score, when compared to the cell-type label set, was selected as the optimal clustering result. The implementation of NMI used in this study was from the scib python library [47]. Adjusted Rand Index Score (ARI). The ARI is another metric used to evaluate the similarity between the clustering assignment and the cell type labels of the same set of samples, similar to the NMI metric. In this context, we similarly denote the cell-type labels as C and the Louvain [12] cluster indices computed on the target dataset as K. The Rand Index (RI) is a measure of the overlap between the two clusterings, C and K. It considers both the correct clustering overlaps and the correct disagreements between the two clusterings [54]. Formally, if we define a as the number of pairs of elements that belong to the same set in both C and K, and b as the number of pairs of elements that are in different sets in C and in different sets in K, the unadjusted RI is given by: RI = a + b CN 2 (11) where N is the total number of cell samples and CN 2 represents the total number of possible pairs in the dataset. However, the unadjusted RI does not account for the possibility of random label assignments leading to correct overlaps by chance. To address this issue, the adjusted RI (ARI) is introduced, which corrects for randomly correct labels by discounting the expected RI of random labelings: ARI = RI − E[RI] max(RI) − E[RI] (12) 21The ARI ranges from 0 to 1, where 0 corresponds to a random labeling, and 1 indicates a perfect match between the two clustering assignments. Similar to NMI, we performed NMI-optimized Louvain clustering to obtain the best match between the clusters and the cell-type labels. Specifically, we executed Louvain clustering over a range of resolutions and selected the clustering output with the highest NMI score when compared to the cell type label set. The implementation of ARI used in this study was from the scib python library [47]. Average Silhouette Width Score (ASW). The silhouette width [ 56] is a metric that evaluates the quality of a clustering solution by quantifying the relationship between the within-clustering distances and the between-cluster distances for each data point. Like the NMI and the ARI, the silouette calculates the similarity between the clustering assignment and the cell type labels of the same set of samples. For each cell sample, the silhouette width is computed based on two scores: (1) a: the mean distance between a sample and all other samples in the same cluster; and (2) b the mean distance between a sample and all samples in the nearest neighboring cluster. The silhouette score si for each sample i is defined as si = b − a max(a, b) (13) The silhouette score ranges from -1 to 1, with higher values indicating that the sample is well-matched to its own cluster and dissimilar to the nearest neighboring cluster. To obtain an overall assessment of the clustering quality, the average silhouette width (ASW) is calculated by averaging the silhouette scores si across all samples. This overall ASW, denoted as ASWo, ranges between -1 and 1, with the following interpretations: • ASW o close to 1: The clusters are dense and well-separated. • ASWo around 0: The clusters overlap, and the between-cluster and within-cluster variability are approximately equal. • ASWo near -1: Strong misclassification has occurred, where the within-cluster variability is greater than the between-cluster variability. To ensure that the final ASW metric falls within the range of 0 to 1, a scaling operation is often applied: ASW = ASWo + 1 2 (14) This scaled ASW value, ranging from 0 to 1, provides a convenient measure for evaluating the quality of the clustering solution, with higher values indicating better separation and cohesion of the clusters. AvgBio. This score combines the three clustering metrics: NMI, ARI and ASW. AvgBio = 1 3(NMI + ARI + ASW) (15) Silhouette Variant Score (ASW b). To evaluate the effectiveness of the batch integration task (Sec. 4.6), a variant of the average silhouette width score (ASW) is employed, referred to as the ASWb. Unlike ASW based on cell type labels, ASWb considers batch labels. This score is designed to assess the degree of batch mixing, where a score of 0 indicates well-mixed batches, and deviations from 0 suggest the presence of a batch effect. We take the absolute value of the original silhouette width score ˜si for sample i based on batch labels: s′ i = |˜si| (16) To ensure higher scores indicate better batch mixing, these scores are scaled by subtracting them from 1. As we expect batches to integrate within cell identity clusters, we compute the ASWb,j score for each cell label j separately, using the following equation: ASWb,j = 1 |Cj| X i∈Cj 1 − s(i)′ (17) 22where Cj = {i|ci = j}N i=1 is the set of cell indices whose cell type label is exactly j. To obtain the final ASW b score, the label-specific ASW b,j scores are averaged across the set of unique cell type labels: ASWb = 1 |V| X j∈V ASWb,j (18) where V represents the set of unique cell type labels. Graph Connectivity (GraphConn). The GraphConn metric is designed to assess whether the k-nearest neighbor ( kNN) graph representation of the integrated data directly connects all cells with the same cell type label. This metric operates on the kNN graph, denoted as GkNN, which is pre-processed by the Scanpy library using the \"scanpy.pp.neighbors\" function. For each cell type label v ∈ V, where V represents the set of cell type labels (Sec. 2), a subset kNN graph GkNN(Vv; Ev) is created. This subset graph contains only cells from the given label v. Using these subset kNN graphs, the GraphConn score is computed as follows: GraphConn = 1 |V| X v∈V |LCC(GkNN(Vv, Ev))| |Vv| (19) Here, |LCC(·)| is the number of nodes in the largest connected component of the graph and |Vv| is the number of nodes with cell type v. The resultant GraphConn score has a range of (0; 1], where a score of 1 indicates that all cells with the same cell type are connected in the integrated kNN graph. The lowest possible score indicates a graph where no cell is connected to any other cell. It’s important to note that the GraphConn score is computed directly on the kNN graph representation of the integrated data. As a result, this metric can be used to evaluate the quality of any integration output, regardless of the specific integration method used. AvgBatch. This score combines two metrics: ASWb and GraphConn. AvgBatch = 1 2(ASWb + GraphConn) (20) Overall. We follow scGPT [14]to calculate a weighted average score of both the batch removal score ASWb and the bio-conservation score AvgBio to balance biological relevance and batch consistency, following the equation: Overall = 0.6 ∗ AvgBio + 0.4 ∗ AvgBatch (21) Accuracy (Acc). In classification tasks like cell type classification (Sec. 4.2.2) and novel cell type classification (Sec. 4.3), we denote the predicted values of thei-th sample as ˆyi and the corresponding true label as yi. Then the accuracy metric is defined as Acc(y, ˆy) = 1 N NX i=1 1 [ˆyi == yi] (22) where the 1 [·] is the indicator function. Macro F1 Score (Macro F1). The F1 Score is essentially defined for binary classification tasks. F1 = 2 Recall−1 + Precision−1 (23) Recall = TP TP + FN (24) Precision = TP TP + FP (25) where TP is the number of true positives, FN the number of false negatives, and TP the number of false positives. The recall is intuitively the ability of the classifier to find all the positive samples; The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. For multi-class classification, macro F1 is defined as the average F1 taken over all different classes. 23ROC AUC Score (AUROC). The Area Under the Receiver Operating Characteristic (AUROC) curve is a metric commonly used to evaluate the performance of binary classification models. It provides a comprehensive measure of the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) across different classification thresholds. In a binary classification task, the model’s output is typically a probability or score that represents the likelihood of a sample belonging to the positive class. By varying the classification threshold, different operating points on the ROC curve can be obtained, where each point represents a specific combination of true positive rate (TPR) and false positive rate (FPR). The ROC curve is created by plotting the TPR (y-axis) against the FPR (x-axis) for different classifi- cation thresholds. The AUROC is then calculated as the area under this ROC curve, providing a single scalar value that summarizes the overall performance of the binary classifier. The AUROC ranges from 0 to 1, with the following interpretations: (1) AUROC=1 indicates perfect classification, where the classifier can perfectly distinguish between the positive and negative classes; (2) AUROC=0.5 indicates random guessing, indicating that the classifier performs no better than a random prediction. The AUROC is a widely used metric because it provides a comprehensive evaluation of the classifier’s performance across all possible classification thresholds. It is invariant to class imbalance and does not require choosing a specific threshold, making it a robust and threshold-agnostic measure. Furthermore, the AUROC has a statistical interpretation as the probability that a randomly chosen positive instance will have a higher predicted probability than a randomly chosen negative instance, which provides a clear interpretation of the metric’s value. Pearson correlation coefficient score (PCC). The PCCis a widely used measure of the linear relationship between two variables. It quantifies the strength and direction of the linear association between the variables, ranging from -1 to 1. The formula for the PCC between two variables, A and B, is given by: rAB = Pn i=1(Ai − B)(Bi − B)qPn i=1(Ai − B)2 qPn i=1(Bi − B)2 where Ai and Bi are the individual observations of variables A and B, respectively. A and B are the sample means of A and B, respectively. n is the number of observations. The numerator represents the covariance between A and B, which measures how much A and B vary together from their respective means. The denominator normalizes the covariance by the product of the standard deviations of A and B, ensuring that the correlation coefficient falls within the range of -1 to 1. The interpretation of this PPC metric is as follows: (1) rAB=1 indicates perfect positive linear correlation (as A increases, B increases proportionally); (2)rAB= −1 indicates perfect negative linear correlation (as A increases, B decreases proportionally); (3) rAB=0 indicates no linear correlation between A and B; (4) 0 < |rAB| < 1 indicates that the strength of the linear correlation increases as the value approaches 1 (either positive or negative). In the context of regression analysis, computing the PCC between each regressor (independent variable) and the target variable can provide insights into the linear relationships between the predictors and the response variable. E.2 Cell Type Identification E.2.1 Zero-shot Identification ( i.e., Cell Type Clustering) Method. We here discuss the experimental details for Sec. 4.2.1. Cell representations extracted from each baseline model are used to compute the k nearest neighbor (kNN) graph using Scanpy’s standard protocols [71]. These representations and the kNN graph are then processed with Louvain clustering algorithms at various resolutions, ranging from 0.1 to 2 in steps of 0.1. The optimized clustering result is determined by the highest gained NMI score achieved across all the resolutions. For implementation, we accelerated Louvain clustering by adopting RAPIDS, a software library that enhances data science pipelines by entirely utilizing NVIDIA GPUs instead of traditional CPUs. Additionally, we conducted ten iterations of dataset down-sampling and reported the averaged NMI, 24Table 11: Full results for the OOD unseen cell type datasets Dct 1 and Dct 2 in the ell type clustering. Method OOD CellType Data (Dct 1 ) OOD CellType Data ( Dct 2 ) NMI↑ ARI↑ ASW↑ AvgBio↑ NMI↑ ARI↑ ASW↑ AvgBio↑ Non-TFM Methods Raw Data 0.864 0.718 0.529 0.703 0.823 0.557 0.505 0.629 Seurat 0.893 0.773 0.590 0.752 0.884 0.723 0.605 0.737 Harmony 0.553 0.241 0.432 0.432 0.594 0.248 0.411 0.417 scVI 0.905 0.797 0.577 0.760 0.889 0.709 0.577 0.725 Ontology-Agnostic TFMs Geneformer 0.846 0.697 0.525 0.689 0.846 0.629 0.530 0.668 scGPT 0.866 0.705 0.551 0.707 0.873 0.724 0.564 0.720 scTab 0.886 0.807 0.584 0.759 0.867 0.754 0.557 0.726 UCE 0.902 0.802 0.612 0.772 0.892 0.695 0.635 0.741 MGP 0.860 0.710 0.573 0.714 0.881 0.745 0.595 0.740 Sup 0.892 0.787 0.621 0.767 0.910 0.793 0.622 0.775 MGP+Sup 0.888 0.775 0.611 0.758 0.901 0.779 0.611 0.764 Ontology-Enhanced TFMs scCello 0.887 0.781 0.640 0.769 0.909 0.817 0.632 0.786 ARI, ASW, and AvgBio scores. This approach significantly reduced the time required to evaluate a dataset, such as Did, from days to just a few minutes. Datasets. As introduced in Sec. 4.2.1, we evaluate one ID dataset ( Did) and six OOD datasets (Dcond i with cond ∈ {ct, ts, dn} and i ∈ {1, 2}) to demonstrate our model’s generalization capabili- ties. These evaluations address various scenarios involving unseen cells for comprehensive testing, including cells with distributions similar to our pre-training dataset, as well as those associated with unseen cell types, tissues, and donors. Hyper-parameters. We used k = 15 neighbors to compute the kNN graph, with node distances calculated using the euclidean distance between cell representations. The Louvain clustering used seed 0 as the random state and treated the kNN graph as unweighted and directed. Performance. In Sec. 4.2.1, Tab. 1 reports only the AvgBio metric for six OOD datasets due to space constraints. Full metrics, including NMI, ARI, and ASW, are detailed in: (1) Tab.11 for the two OOD cell type datasets (Dct 1 and Dct 2 ); (2) Tab. 12 for the two OOD tissue datasets (Dts 1 and Dts 2 ); and (3) Tab. 13 for the two OOD donor datasets (Ddn 1 and Ddn 2 ). E.2.2 Identification with Fine-tuning ( i.e., Cell Type Classification) Method. In this setting, the TFMs are further fine-tuned by adding a simple linear layer atop their model backbones, which transforms the hidden representations into prediction logits. The dimensions of these logits correspond to the number of cell type classes predicted. Importantly, all model parameters, including those of the TFM backbone and the newly added linear layer, are trainable during fine-tuning. The model checkpoint that achieves the highest Macro F1 score on the validation data is then selected for final testing. Datasets. We fine-tuned TFMs on a subset of our curated pre-training data, randomly selecting 90% for training and using the remaining 10% for validation. The final performance was tested on the ID dataset Did, which consists of cell samples never seen during scCello ’s pre-training. We explored two subset sizes, 0.1% and 1% of the pre-training data, to simulate scenarios where 10× more annotated data becomes available. This exploration is meaningful for real-world applications, where annotating data is both costly and time-consuming. 25Table 12: Full results for the OOD unseen tissue datasets Dts 1 and Dts 2 in the cell type clustering. Method OOD Tissue Data (Dts 1 ) OOD Tissue Data ( Dts 2 ) NMI↑ ARI↑ ASW↑ AvgBio↑ NMI↑ ARI↑ ASW↑ AvgBio↑ Non-TFM Methods Raw Data 0.733 0.405 0.481 0.540 0.800 0.585 0.508 0.631 Seurat 0.777 0.497 0.488 0.587 0.813 0.560 0.535 0.636 Harmony 0.649 0.302 0.436 0.462 0.684 0.400 0.460 0.515 scVI 0.774 0.443 0.516 0.577 0.816 0.550 0.537 0.634 Ontology-Agnostic TFMs Geneformer 0.736 0.412 0.468 0.539 0.787 0.499 0.505 0.597 scGPT 0.739 0.407 0.486 0.544 0.794 0.556 0.531 0.627 scTab 0.754 0.492 0.515 0.515 0.815 0.616 0.541 0.657 UCE 0.787 0.476 0.531 0.598 0.836 0.610 0.562 0.670 MGP 0.766 0.472 0.491 0.576 0.802 0.544 0.537 0.628 Sup 0.788 0.502 0.527 0.605 0.838 0.621 0.580 0.680 MGP+Sup 0.789 0.518 0.524 0.610 0.833 0.612 0.573 0.672 Ontology-Enhanced TFMs scCello 0.784 0.519 0.534 0.612 0.839 0.675 0.601 0.705 Table 13: Full results for the OOD unseen donor datasets Ddn 1 and Ddn 2 in the Cell Type Clustering. Note that scTab is OOM on these two datasets. Method OOD Donor Data (Ddn 1 ) OOD Donor Data ( Ddn 2 ) NMI↑ ARI↑ ASW↑ AvgBio↑ NMI↑ ARI↑ ASW↑ AvgBio↑ Non-TFM Methods Raw Data 0.665 0.247 0.462 0.458 0.665 0.251 0.462 0.460 Seurat 0.691 0.294 0.413 0.466 0.711 0.335 0.420 0.489 Harmony 0.679 0.286 0.405 0.456 0.690 0.324 0.408 0.474 scVI 0.699 0.269 0.466 0.478 0.722 0.311 0.471 0.502 Ontology-Agnostic TFMs Geneformer 0.666 0.303 0.434 0.468 0.686 0.327 0.433 0.482 scGPT 0.656 0.259 0.452 0.456 0.677 0.298 0.456 0.477 scTab / / / OOM / / / OOM UCE 0.718 0.245 0.491 0.485 0.737 0.284 0.496 0.506 MGP 0.713 0.294 0.457 0.488 0.734 0.359 0.462 0.518 Sup 0.754 0.357 0.545 0.552 0.768 0.395 0.556 0.573 MGP+Sup 0.754 0.373 0.532 0.553 0.768 0.398 0.544 0.570 Ontology-Enhanced TFMs scCello 0.774 0.426 0.625 0.608 0.794 0.486 0.649 0.643 Hyper-parameters. For scCello, we set the following hyper-parameters for fine-tuning: a learning rate of 5.0 × 10−5, a linear learning rate scheduler with 500 warmup steps, a weight decay of 0.001, and a batch size of 24. The same fine-tuning configuration was applied to the three ablation TFMs pre-trained using scCello’s codebase (MGP, Sup, and MGP+Sup). For other TFM baselines, we searched for the optimal learning rate to report the final performance. Performance. In Sec. 4.2.2, we reported classification and clustering metrics for TFMs fine-tuned with the 0.1$ subset of the pre-training data. Here, we extend our reporting to TFMs fine-tuned with 1% of a pre-training subset that is 10 × larger. We compare performances at these two subset selection ratios in Tab. 14. We observe that, 26Table 14: Cell type identification with fine-tuning evaluated on the ID dataset Did, as the pre-training subset data size for fine-tuning increases from 0.1% to 1% for the subset selection ratio. Methods Cell Type Classification Cell Type Clustering Acc↑ (0.1% → 1%) Macro F1 ↑ (0.1% → 1%) AvgBio ↑ (0.1% → 1%) Ontology-Agnostic TFMs Geneformer 0.747 → 0.872 0.440 → 0.664 0.439 → 0.469 scGPT 0.712 → 0.862 0.344 → 0.636 0.477 → 0.481 scTab 0.778 → 0.773 0.373 → 0.455 0.606 → 0.589 MGP 0.722 → 0.861 0.287 → 0.639 0.607 → 0.631 Sup 0.812 → 0.902 0.363 → 0.718 0.659 → 0.668 MGP+Sup 0.820 → 0.902 0.406 → 0.735 0.607 → 0.667 Ontology-Enhanced TFMs scCello 0.867 → 0.910 0.511 → 0.761 0.694 → 0.699 Known cell types u∈𝒱kn Unknown cell types 𝑣∈𝒱unkn Query cell q … … similarity cell type repr.cell repr. 𝓏q u2 similarity …u4 vPPR𝑢#,𝑣u2 𝒉$# 𝑠𝑞,𝑣 𝑠𝑞,𝑢% 𝑠𝑞,𝑢#𝑠𝑞,𝑢&=𝒛'(𝒉$# 𝑠𝑞,𝑣 Step 1: Known and unknown cell typesStep 2: Similarity of query to known cell typesStep 3: Similarity of query to unknown cell typesStep 4: Decide query cell type 𝒔𝑞,𝒖=𝑠𝑞,𝑢&,𝑠𝑞,𝑢#,…,𝑠𝑞,𝑢𝒱$% 𝒔𝑣,𝒖=PPR𝑢&,𝑣,…,PPR𝑢𝒱$%,𝑣𝑠𝑞,𝑣=SpearmanR(𝒔𝑞,𝒖,𝒔(𝑣,𝒖)) 𝑣∗=arg\tmax+∈𝒱!\"#\"𝑠(𝑞,𝑣) PPR𝑢-,𝑣 PPR𝑢&,𝑣 PPR𝑢%,𝑣… u1 u3u3 u1 Figure 8: Graphical illustration of our approach for classifying novel cell types (i.e., unknown cell types) (introduced in App. E.3). (1) As the size of fine-tuning data increases, all TFMs except scTab show benefits and scCello achieves 48.9% improvement in Macro F1 when the data size gets 10 × larger. scTab’s underperformance may be related to its model capacity, as it employs a TabNet architec- ture [3]—unlike others that use the powerful standard Transformers [63]. (2) Across both the classification and clustering metrics, scCello’s prevails other TFM baselines by a large margin. Remarkably, even when fine-tuned with a smaller 0.1% pre-training subset, scCello surpasses TFMs fine-tuned with a much larger 1% subset, achieving a 3.9% improvement over the best baseline. This underscores scCello ’s superiority, attributed to its cell ontology-guided pre-training. (3) Interestingly, clustering performance does not necessarily correlate directly with classifica- tion performance. For instance, while MGP+Sup outperforms Sup in classification metrics, it does not do so in clustering metrics. This observation underscores the importance to evaluate both the clustering and classification performances for cell type identification with model fine-tuning, which can make the evaluation setting more comprehensive and rigorous. E.3 Novel Cell Type Classification Method. In this task, we define \"known cell types\" Vkn ⊆ Vas the 398 cell types from our labeled pre-training dataset (see dataset statistics Tab. 8). \"Novel cell types\", or \"unknown cell types\" Vunkn ⊆ V, are those present only in the target downstream dataset and not observed during TFM pre-training (Vunkn = V \\ Vkn). Given a new query cell q, we aim to classify it to one of the unknown cell types Vunkn. To solve this problem, we choose to first calculate representations for both the query cell sample and the unknown 27cell types. And then we measure the similarity between the two representations to determine the prediction results vq ∈ Vunkn. Since unknown cell types are absent from the pre-training dataset, their representations cannot be directly obtained from any TFM baselines or our model, despite its ability to learn representations for known cell types. To address this problem, we leverage the known cell typesVkn as a bridge to represent the query cells through the similarity between the cell and cell type representations produced by TFMs, and also represent the unknown cell types using the structural similarity relationships between the known and unknown ones derived from the cell ontology graph. Specifically, our approach is illustrated in Fig. 8 and involves the following steps: (1) Representations for known cell types. Although scCello inherently learns cell type repre- sentations during pre-training, most existing TFMs do not output cell type representations directly. For benchmarking, we propose a protocol to calculate known cell type repre- sentations for general TFMs. Specifically, the representation for each known cell type is calculated by averaging cell representations derived from TFMs across cells belonging to this cell type. We used cell samples from a subset (10%) of our curated pre-training dataset, because the whole 22 million dataset is too large to fit. We denote the known cell type representations as {hu}u∈Vkn , to differentiate with the nota- tion of scCello’s learned cell type representations {hu}u∈Vkn introduced in Sec. 2. For fair comparison, scCello also follows this protocol to generate known cell type representations, instead of using its learned ones. Nevertheless, we emphasize scCello’s capability to conduct this task alone without further accessing reference databases like our pre-training dataset. (2) Similarity vector for a query cell to known cell types. We first derive the cell repre- sentations for the query cell q from TFMs. Then, we estimate the similarity between the query cell q and any known cell type u ∈ Vkn using the cosine similarity between their representations s(q, u) = zT q hu. For all known cell types, this results in a similarity vector: s(q, u) = [d(q, u1), d(q, u2), . . . , d(q, u|Vkn|)] (26) where we define the order of vector indices as u = [u1, u2, . . . , u|Vkn|] satisfying u1 < u2 < ··· < u|Vkn|. (3) Similarity vector for unknown cell types to known cell types. For each unknown cell type v ∈ Vunkn, we estimate the similarity s(v, u) between the unknown v and the known cell types u. To achieve this, we leverage the cell ontology graph to calculate structural proximities as proxies. The proximities are measured using the raw PPR score PPR(u, v), u∈ Vkn, v∈ Vunkn, which is introduced in Sec. 2.4. Therefore, the similarity vector can be represented as: s(v, u) = [PPR(u1, v), PPR(u2, v), . . . ,PPR(u|Vkn|, v)], (27) (4) Align the similarity vectors for the query cell and the unknown cell types. Intuitively, the similarity vector s(q, u) indicates a profiling for the query cell q, with known cell types u as a frame of reference; and the similarity vector s(v, u) conveys similar profiling for an unknown cell type v. Therefore, the more similar the two similarity vectors s(q, u) and s(v, u) is, the higher possibility for the query cell to be alike this unknown cell type. We derive it using Spearman Ratio [50] SpearmanR(·) as the similarity measure: s(q, v) = SpearmanR(s(q, u), s(v, u)). (28) Other formulas for the vector similarity function are available, like the commonly used cosine similarity (i.e., d(q, v) = d(q, u)T s(q, u)). Our approach is not sensitive to the choice of the similarity metric. As shown in Fig. 10, using the dot product as the similarity score led to similar relative performance as in Fig. 2, where scCello generally performs better or on par with other TFMs. Therefore, we used Spearman Ratio throughout the experiments. (5) Select the final answer. The unknown cell type v∗ with the largest distance is selected as the prediction for novel cell type classification: v∗ = arg maxv∈Vunkn s(q, v) (29) 28(a) Acc for Dct 1 (same as Fig. 2, put for comparison).  (b) Macro F1 for Dct 1 . (c) Acc for Dct 2 .  (d) Macro F1 for Dct 2 . Figure 9: Novel cell type classification on two OOD cell type datasets Dct 1 and Dct 1 , using the Spearman Ratio similarity measure to compare the representations of the query cells and the novel cell types (App. E.3). Two metrics Acc and Macro F1 are reported. In real-world applications, our approach is still applicable since almost all cell types are included in the cell ontology graph. But we won’t be able to know whether the newly coming query cells are from unknown cell types Vunkn or known cell types Vkn. Therefore, we can expand the unknown cell type set Vunkn to all the cell type defined in the ontology graph V, and conduct similar processes in our approach. Datasets. We evaluate on OOD cell type datasets Dct 1 and Dct 2 . The cell types in Dct 1 and Dct 2 are already aligned to the cell ontology graph using the ontology identifiers provided by CellxGene database, and are a subset of all the unknown cell types Vunkn. We recognize that the prediction task becomes more challenging as the number of novel cell types increases. Therefore, we constrain the complete unknown cell type set to the cell types occurred in the datasets we used. To further reflect the challenge, we created five difficulty levels, where the number of cell types spanned from 10%, 25%, 50%, 75% to 100% of the total cell type count. For example, if we use 25% cell types in the OOD cell type dataset Dct 1 with a total 87 cell types, the unknown cell types include (87 × 25% ≈ 22) randomly selected cell types from the complete set {ci|Xi ∈ Dct 1 }. To account for potential biases, we randomly sampled 20 distinct combinations of cell types for each difficulty level. Hyper-parameters. The PPR(·) score is calculated using the \"nx.pagerank\" function with alpha hyper-parameter set to 0.9. Performance. The full metrics for both accuracy and macro f1 score on the two OOD cell type datasets Dct 1 and Dct 2 are reported in Fig. 9. Besides plots, the numerical results are also summarized in Tab. 16 and Tab. 17 for reference. 29Table 16: Novel cell type classification results on OOD cell type dataset Dct 1 . Method 10% cell types 25% cell types 50% cell types 75% cell types 100% cell types Acc↑ F1↑ Acc↑ F1↑ Acc↑ F1↑ Acc↑ F1↑ Acc↑ F1↑ Ontology-Agnostic TFMs Geneformer 0.392 0.207 0.226 0.095 0.157 0.050 0.135 0.036 0.123 0.027 scGPT 0.291 0.178 0.148 0.072 0.105 0.041 0.062 0.024 0.052 0.020 scTab 0.380 0.248 0.191 0.096 0.114 0.058 0.088 0.042 0.077 0.035 UCE 0.399 0.253 0.289 0.120 0.205 0.064 0.149 0.040 0.131 0.030 MGP 0.361 0.243 0.233 0.119 0.125 0.048 0.089 0.032 0.076 0.022 Sup 0.464 0.389 0.329 0.200 0.187 0.109 0.139 0.075 0.111 0.055 MGP+Sup 0.556 0.358 0.341 0.172 0.217 0.089 0.193 0.069 0.172 0.056 Ontology-Enhanced TFMs scCello 0.768 0.559 0.547 0.365 0.442 0.246 0.364 0.177 0.335 0.150 Table 17: Novel cell type classification results on OOD cell type dataset Dct 2 . Method 10% cell types 25% cell types 50% cell types 75% cell types 100% cell types Acc↑ F1↑ Acc↑ F1↑ Acc↑ F1↑ Acc↑ F1↑ Acc↑ F1↑ Ontology-Agnostic TFMs Geneformer 0.367 0.213 0.310 0.125 0.107 0.048 0.069 0.027 0.047 0.020 scGPT 0.411 0.223 0.217 0.085 0.108 0.041 0.077 0.031 0.061 0.025 scTab 0.338 0.245 0.175 0.102 0.113 0.053 0.074 0.038 0.049 0.027 UCE 0.339 0.227 0.244 0.119 0.119 0.064 0.056 0.035 0.037 0.027 MGP 0.411 0.270 0.225 0.120 0.114 0.057 0.066 0.036 0.038 0.023 Sup 0.581 0.372 0.325 0.204 0.199 0.112 0.140 0.081 0.108 0.063 MGP+Sup 0.428 0.315 0.228 0.143 0.131 0.082 0.069 0.047 0.061 0.041 Ontology-Enhanced TFMs scCello 0.763 0.500 0.498 0.304 0.364 0.196 0.297 0.149 0.247 0.124 E.4 Marker Gene Prediction Method. We here explain our approach for this task in details. Given a cell’s gene expression profile, we enumerate each gene and attempt to knock it out, either by replacing it with a special [MASK] token or by reducing its expression to zero. The former method is used for Geneformer, MGP, Sup, MGP+Sup, and scCello, while the latter is applied to scGPT, scTab, and UCE. By comparing the cell representations of the mutated expression and those of the original expression, we assess the impact of each gene’s knockout. A greater impact suggests a higher likelihood of the gene being a marker gene. This zero-shot approach requires no further fine-tuning and is particularly useful when additional computational resources or annotated datasets for fine-tuning are unavailable. Notably, we acknowledge the shortage of our method: for house keeping genes ( i.e., non-marker genes), knocking out these genes will also have large impact on the cell because the cell would die [17]. Therefore, a high impact from gene knockout does not necessarily indicate a marker gene, but rather an \"important\" gene. However, this issue is not critical empirically, as the number of well-documented housekeeping genes is about 400, which is small compared to the extensive gene token vocabulary of M = 25, 424. Datasets. As introduced in Sec. 4.4, we used the datasets from GSE96583 [33] and GSE130148 [7]. One the one hand, the GSE96583 dataset Dmk 1 inherently contains five cell subsets associated with 9 cell type classes. The five cell subsets are denoted as \"GSE96583_1\", \"GSE96583_2\", \"GSE96583_3\", \"GSE96583_4\", \"GSE96583_5\", respectively. On the other hand, the GSE130148 dataset Dmk 2 contains 13 cell type classes. The size of these two datasets are summarized in Tab. 21, and their associated cell types are recorded in Tab.20 for demonstration. Additionally, the ground truth cell-type-specific marker genes are originally sourced from two databases: CellMarker2 [29] and PanglaoDB [21]. 30(a) Acc for Dct 1 .  (b) Macro F1 for Dct 1 . (c) Acc for Dct 2 .  (d) Macro F1 for Dct 2 . Figure 10: Novel cell type classification on two OOD cell type datasets Dct 1 and Dct 1 , using the cosine similarity measure to compare the representations of the query cells and the novel cell types (App. E.3). Two metrics Acc and Macro F1 are reported. Performance. In Sec. 4.4, we only reported the average performance across the 5 subsets of GSE96583 (Dmk 1 ) and the individual performance of GSE130148 (Dmk 2 ) in Tab 3. Here, we provide complete results for all five subsets in Tab. 19. E.5 Novel Marker Gene Prediction Method. We adopted a methodology similar to our zero-shot marker gene prediction protocol (see App. E.4) for calculating differences in cell representations through in-silico gene perturbation: (1) For each cell, we calculated the change in cell representations after removing each gene in-silico and selected the top 10% genes with the largest changes, excluding the known marker genes. (2) For each cell type, we identified the 10 most frequent genes among the top 10% across all cells, to obtain 10 candidate novel marker genes. (3) To ensure specificity, we removed genes present in more than one cell type. Datasets. We used the same two datasets (GSE96583 and GSE130148) in Sec. 4.4 for marker gene prediction, where marker gene labels were retrieved from CellMarker2 [30] and PanglaoDB [21]. Performance. As a case study, we followed the above steps to find novel marker genes for two cell types: • For cell type “CD14+ Monocytes”, two genes were found: FOS and LGALS2. FOS is typically expressed in response to stress signals, cytokines, and growth factors [2]; LGALS2 31Table 19: Full results for the five data subsets from GSE96583 ( Dmk 1 ) and one dataset from GSE130148 (Dmk 2 ) in the marker gene prediction task (Sec. 4.4). Method GSE96583 (Dmk1 ) GSE130148 ( Dmk2 ) Avg.↑of Dmk1 andDmk2GSE96583_1 GSE96583_2 GSE96583_3 GSE96583_4 GSE96583_5Avg.↑ AUROC↑AUROC↑ AUROC↑ AUROC↑ AUROC↑ AUROC↑ Non-TFM Methods DET 0.725 0.710 0.737 0.715 0.717 0.721 0.683 0.702 Ontology-Agnostic TFMs Geneformer 0.445 0.447 0.478 0.484 0.408 0.452 0.470 0.461scGPT 0.423 0.387 0.344 0.385 0.388 0.385 0.387 0.386scTab 0.666 0.654 0.689 0.693 0.660 0.672 0.727 0.700UCE 0.502 0.499 0.500 0.499 0.500 0.500 0.500 0.500MGP 0.572 0.560 0.606 0.589 0.567 0.579 0.629 0.604Sup 0.707 0.697 0.694 0.699 0.700 0.699 0.693 0.696MGP+Sup 0.734 0.720 0.739 0.734 0.724 0.730 0.730 0.730 Ontology-Enhanced TFMs scCello 0.767 0.753 0.754 0.748 0.760 0.756 0.729 0.743 Table 20: Cell types for the two marker gene prediction datasets GSE96583 (Dmk 1 ) and GSE130148 (Dmk 2 ). Dataset Cell Types GSE96583 \"Dendritic cells\", \"CD8 T cells\", \"NK cells\", \"B cells\", \"Megakaryocytes\", \"FCGR3A+ Monocytes\", \"CD14+ Monocytes\", \"CD4 T cells\", \"Not Known\" GSE130148 \"Macrophages\", \"T cell\", \"NK cell\", \"Mast cell\", \"Endothelium\", \"Lymphatic\", \"Pulmonary Alveolar Type II\", \"Transformed epithelium’, \"Ciliated\", \"Pulmonary Alveolar Type I\", \"B cell\", \"Fibroblast\", \"Secretory\" is involved in modulating immune responses and inflammatory processes in monocytes [37]. Both are plausible marker gene candidates. • For cell type “Megakaryocytes”, five genes were found: GNG11, TUBB1, H2AC6, CA VIN2, CLU. GNG11 is confirmed as marker genes in literature, TUBB1 is a likely marker, while H2AC6, CA VIN2, and CLU require further investigation. E.6 Cancer Drug Response Prediction Method. In this task, we first compute cell line level representations from scRNA-seq data and drug representations for associated drugs. Both these two representations are then input into the DeepCDR framework for training. Finally, we calculate the PCC between the predicted and actual IC50 values for each drug across all cell lines and report the average performance across all tested drugs. Specifically, for TFMs, single-cell gene expression data are inputted into each model to generate cell- specific representations for each gene. These are then aggregated into cell line-level representations through max-pooling across all genes for each dimension. Conversely, the DeepCDR method uses raw gene expressions, aggregating them directly before max-pooling. Additionally, drugs are represented as graphs and encoded using graph neural networks to obtain drug representations. Datasets. In our experiments, we utilized cell line and drug-paired data pre-processed by Deep- CDR [43], including 223 drugs and 561 cell line bulk gene expression profiles for 697 genes from 31 different cancer types. Among the dataset, 89,585 cell line-drug samples were used for training and 4,729 for testing [26]. Hyper-parameters. We following scFoundation’s implementation to set the parameters in the DeepCDR framework, like “-use_gexp” as True, and both “-use_mut” and “-use_methy” as False. 32Table 21: The number of cell samples (#Cells) for the marker gene prediction datasets GSE96583 (Dmk 1 ) and GSE130148 (Dmk 2 ). Dataset GSE96583_1 GSE96583_2 GSE96583_3 GSE96583_4 GSE96583_5 GSE130148 #Cells 4,246 3,639 14,619 14,446 6,145 10,360 Table 22: The correlation of the ontology structure and the pairwise similarity of known cell type representations Method Spearman R ↑ Non-TFM Methods Raw Data 0.212 Seurat 0.316 Harmony 0.262 Ontology-Agnostic TFMs Geneformer 0.284 scGPT 0.037 scTab 0.209 UCE 0.285 MGP 0.275 Sup 0.229 MGP+Sup 0.238 Ontology-Enhanced TFMs scCello 0.506 Performance. Results are already reported in Tab. 4 in Sec. 4.5. E.7 Batch Integration Method. This batch integration task aims to seamlessly integrate scRNA-seq data from different batches, which can be conducted using the same protocol as cell type clustering. After clustering, model performance is evaluated. Besides using cell type labels and clustering indices from the optimized Louvain algorithm to calculate the preservation of biological signals (NMI, ARI, ASW and AvgBio), this task also use batch labels to measure the removal of batch effects (ASW b and AvgBatch). See App. E.1 for metric calculation details. Datasets. As introduced in Sec. 4.6, all datasets used in the cell type clustering task (Sec. 4.2.1) are evaluated, including one ID dataset Did and six OOD datasets Dcond i (cond ∈ {ct, ts, dn}, i ∈ {1, 2}). Hyper-parameters. We use the same hyper-parameters as that in cell type clustering. Performance. In Sec. 4.6, the Overall score, a weighted average of AvgBio and AvgBatch, is already reported in Fig. 3. Complete results for all metrics are included in Tab. 23 for the ID dataset Did, Tab.24 for the OOD cell type datasets Dct 1 and Dct 2 , Tab.25 for the OOD tissue datasets Dts 1 and Dts 2 , and Tab. 26 for the OOD donor datasets Ddn 1 and Ddn 2 . E.8 Visualization for Learned Cell Representations We calculate known cell type representation as introduced in Sec. E.3, by averaging cell representa- tions for each type on 10% of the pre-training data. Then we apply tSNE to project the known cell type representations to 2D space and visualize in Fig. 11. Highly correlated cell types are clustered together as expected, and dissimilar cell types are distant. 33Figure 11: Visualization for learned cell representations of scCello (introduced in App. E.8). The nodes are different cell types in the pre-training dataset and the edges denote \"is a subtype of\" relationships in cell ontology G. The coordinates of nodes are calculated using tSNE dimensional reduction for cell type representations derived from scCello. As expected, highly ontology-correlated cell type pairs are very close in the latent space, such as myeloid leukocyte and myeloid cell, as well as fibroblast and connective tissue cell. Meanwhile, dissimilar cell type pairs remain distant, such as CD4-positive, alpha-beta T cell and epithelial cell. The highly biologically informative representation space implies scCello’s potential generalization ability to other cell-type-related downstream tasks. We also calculate the Spearman R correlation of the pairwise similarity of known cell type representa- tions and the ontology structure (1 for an edge between two cell types and 0 for no edge between them) in Tab.22. As expected, scCello learned a biologically informative representation space that is much more correlated to the true ontology structure than other methods. This implies scCello’s potential generalization ability to other cell-type-related downstream tasks. We also compare the cell representation distribution for scCello with its ablated version excluding the relational alignment objective (i.e., Eqn. 4), to analyze how the ontology relational alignment loss benefits clustering performance. Following the same visualization method for Fig. 11, visualization results for the ablated model is shown in Fig. 12. Adding the relational alignment loss makes similar cell types clustered more closely and pushes dissimilar types farther apart. Therefore, relational alignment enables scCello to align better with biological intuitions, and produce more effective cell representations as evaluated by broad downstreams. 34Figure 12: Visualization for learned cell representations of scCello ablation without the relational alignment objective. Compared with Fig 11, without relational alignment objective, some highly correlated cell types are not clustered well together. Table 23: Batch integration on ID dataset Did. Method ID Unseen Data (Din) ASWb↑ GraphConn↑ AvgBatch↑ AvgBio↑ Overall↑ Non-TFM Methods Raw Data 0.951 0.806 0.878 0.419 0.603 Seurat 0.829 0.686 0.757 0.442 0.568 Harmony 0.824 0.688 0.756 0.421 0.555 scVI 0.880 0.738 0.809 0.474 0.608 Ontology-Agnostic TFMs Geneformer 0.875 0.676 0.775 0.432 0.569 scGPT 0.887 0.691 0.789 0.438 0.578 scTab 0.917 0.925 0.921 0.577 0.715 UCE 0.906 0.788 0.847 0.489 0.632 MGP 0.870 0.728 0.799 0.473 0.603 Sup 0.885 0.809 0.847 0.555 0.672 MGP+Sup 0.892 0.829 0.860 0.516 0.654 Ontology-Enhanced TFMs scCello 0.834 0.697 0.766 0.670 0.708 35Table 24: Batch integration on OOD cell type datasets Dct 1 and Dct 2 . Method OOD CellType Data (Dct1) OOD CellType Data ( Dct2) ASWb↑ GraphConn↑ AvgBatch↑ AvgBio↑ Overall↑ ASWb↑ GraphConn↑ AvgBatch↑ AvgBio↑ Overall↑ Non-TFM Methods Raw Data 0.934 0.940 0.937 0.703 0.797 0.939 0.895 0.917 0.629 0.744Seurat 0.831 0.928 0.880 0.752 0.803 0.844 0.932 0.888 0.737 0.797Harmony 0.909 0.800 0.855 0.432 0.601 0.898 0.817 0.858 0.417 0.593scVI 0.875 0.959 0.917 0.760 0.823 0.880 0.952 0.916 0.725 0.801 Ontology-Agnostic TFMs Geneformer 0.915 0.907 0.911 0.689 0.778 0.915 0.917 0.916 0.668 0.767scGPT 0.903 0.913 0.908 0.707 0.787 0.896 0.927 0.912 0.720 0.797scTab 0.908 0.904 0.906 0.759 0.818 0.910 0.905 0.908 0.726 0.799UCE 0.867 0.947 0.907 0.772 0.826 0.854 0.946 0.900 0.741 0.805MGP 0.894 0.903 0.899 0.714 0.788 0.925 0.580 0.753 0.740 0.745Sup 0.879 0.944 0.912 0.767 0.825 0.879 0.914 0.897 0.775 0.824MGP+Sup 0.885 0.946 0.916 0.758 0.821 0.885 0.925 0.905 0.764 0.820 Ontology-Enhanced TFMs scCello 0.877 0.911 0.894 0.769 0.819 0.858 0.884 0.871 0.786 0.820 Table 25: Batch integration on OOD tissue datasets Dts 1 and Dts 2 . Method OOD Tissue Data (Dts1) OOD Tissue Data ( Dts2) ASWb↑ GraphConn↑ AvgBatch↑ AvgBio↑ Overall↑ ASWb↑ GraphConn↑ AvgBatch↑ AvgBio↑ Overall↑ Non-TFM Methods Raw Data 0.941 0.792 0.867 0.540 0.671 0.946 0.862 0.904 0.631 0.740Seurat 0.865 0.830 0.847 0.587 0.691 0.867 0.841 0.854 0.636 0.723Harmony 0.905 0.755 0.830 0.462 0.609 0.908 0.744 0.826 0.515 0.639scVI 0.901 0.861 0.881 0.577 0.699 0.910 0.881 0.896 0.634 0.739 Ontology-Agnostic TFMs Geneformer 0.925 0.804 0.865 0.539 0.669 0.924 0.835 0.880 0.597 0.710scGPT 0.916 0.776 0.846 0.544 0.665 0.920 0.826 0.873 0.627 0.725scTab 0.916 0.872 0.894 0.515 0.667 0.917 0.874 0.896 0.657 0.753UCE 0.905 0.864 0.885 0.598 0.713 0.911 0.879 0.895 0.670 0.760MGP 0.887 0.887 0.887 0.576 0.700 0.901 0.815 0.858 0.628 0.720Sup 0.903 0.932 0.918 0.605 0.730 0.899 0.911 0.905 0.680 0.770MGP+Sup 0.900 0.941 0.921 0.610 0.734 0.898 0.922 0.910 0.672 0.767 Ontology-Enhanced TFMs scCello 0.868 0.841 0.855 0.612 0.709 0.884 0.819 0.852 0.705 0.764 Table 26: Batch integration on OOD donor datasets Ddn 1 and Ddn 2 . Method OOD Donor Data (Ddn1 ) OOD Donor Data ( Ddn2 ) ASWb↑ GraphConn↑ AvgBatch↑ AvgBio↑ Overall↑ ASWb↑ GraphConn↑ AvgBatch↑ AvgBio↑ Overall↑ Non-TFM Methods Raw Data 0.945 0.785 0.865 0.458 0.621 0.946 0.787 0.867 0.460 0.623Seurat 0.875 0.759 0.817 0.466 0.606 0.876 0.771 0.824 0.489 0.623Harmony 0.893 0.618 0.756 0.456 0.576 0.891 0.655 0.773 0.474 0.594scVI 0.914 0.831 0.872 0.478 0.636 0.909 0.837 0.873 0.502 0.650 Ontology-Agnostic TFMs Geneformer 0.921 0.763 0.842 0.468 0.618 0.919 0.768 0.844 0.482 0.627scGPT 0.920 0.757 0.839 0.456 0.609 0.920 0.763 0.842 0.477 0.623scTab / / / OOM OOM / / / OOM OOMUCE 0.904 0.665 0.784 0.485 0.605 0.907 0.558 0.733 0.506 0.597MGP 0.910 0.824 0.867 0.488 0.640 0.906 0.814 0.860 0.518 0.655Sup 0.909 0.877 0.893 0.552 0.688 0.902 0.857 0.880 0.573 0.696MGP+Sup 0.910 0.888 0.899 0.553 0.691 0.903 0.869 0.886 0.570 0.696 Ontology-Enhanced TFMs scCello 0.845 0.805 0.825 0.608 0.695 0.849 0.802 0.826 0.643 0.716 36NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: In this work, we propose scCello, a novel TFM designed to leverage cell ontology priors for enhancing cell representation and understanding. This main claim is accurately and clearly stated and emphasized in the abstract and introduction. It reflects our contribution and scopes. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We state several limitations of our work in Sec. 5, such as the lack of continue learning capability for our proposed model scCello, the relative small model scale for scCello, and our downstream approach for the zero-shot marker gene experiment. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs 37Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: We propose a novel cell-ontology guided transcriptomic foundation model sc- Cello in this work. It’s a biological insight driven method and focuses on strong downstream applications. We do not establish theoretical results in this paper. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all the details needed to reproduce our work, such as all experi- mental results for every metric and every dataset, pre-training setups in Sec. 4.1 and App. D, and downstream task settings in App. E. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 38(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Our code and datasets will be released upon acceptance. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more de- tails. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the training and test details are justified in App. D for pre-training, and in App. E for downstreams. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In this work, we provide error bars whenever random sampling on key exper- imental factors is involved. For example, in the novel cell type prediction task, we have random sampling procedures for cell type combinations and we report box plots with error bars to demonstrate the statistical significance of this experiments. 39Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: As indicated in Tab. 9, the pre-training our proposed model requires training for 2 days on 4 × A100 NVIDIA GPUs, each with 40G GPU memory. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow the NeurIPS Code of Ethics throughout the entire project. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? 40Answer: [Yes] Justification: We discuss the social impact in Sec. 5 for both positive and negative influences. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: All the datasets we utilized for both pre-training and downstream tasks are publicly available, and we did not implement explicit safeguards for these datasets. While our proposed model, scCello, aims to advance our understanding of cell representation learning for scientific discovery purposes and carries relatively little inherent risk for misuse, we acknowledge that as an open-source model, we cannot guarantee zero potential for misuse if the methods were to fall into malicious hands. Despite our intentions for beneficial applications, the open availability of scCello introduces a degree of uncertainty regarding potential mishandling or nefarious exploitation that we cannot definitively preclude. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] 41Justification: All baseline methods and datasets are properly credited. Their license and terms of use are properly respected. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: Code and datasets will be released upon acceptance. Clear documentations will be provided along. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We did not perform crowdsourcing experiments and research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 42Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 43",
      "references": [
        "Cz cell×gene discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data.",
        "The role of jun, fos and the ap-1 complex in cell-proliferation and transformation.",
        "Tabnet: Attentive interpretable tabular learning.",
        "An ontology for cell types.",
        "An overview of drug discovery and development.",
        "Fast unfolding of communities in large networks.",
        "A cellular census of human lungs identifies novel cell states in health and in asthma.",
        "Mars: discovering novel cell types across heterogeneous single- cell experiments.",
        "What is a master regulator?",
        "Flexible comparison of batch correction methods for single-cell rna-seq using batchbench.",
        "Targeted sampling from massive block model graphs with personalized pagerank.",
        "I-louvain: An attributed graph clustering method.",
        "A single-cell gene expression language model.",
        "scgpt: toward building a foundation model for single-cell multi-omics using genera- tive ai.",
        "Bert: Pre-training of deep bidirectional transformers for language understanding.",
        "The cell ontology 2016: enhanced content, modularization, and ontology interoperability.",
        "Human housekeeping genes, revisited.TRENDS in Genetics",
        "Scaling cross-tissue single-cell annotation models.",
        "Towards scaling fully personalized pagerank: Algorithms, lower bounds, and experiments.",
        "Panglaodb: a web server for explo- ration of mouse and human single-cell rna sequencing data.",
        "Dissecting supervised contrastive learning.",
        "Exploring network structure, dynamics, and function using networkx.",
        "Inductive representation learning on large graphs.",
        "Large scale foundation model on single-cell transcriptomics.",
        "Dictionary learning for integrative, multimodal and scalable single-cell analysis.",
        "Scalable querying of human cell atlases via a foundational model reveals commonalities across fibrosis-associated macrophages.",
        "Cellmarker 2.0: an updated database of manually curated cell markers in human/mouse and web tools based on scrna-seq data.",
        "Fully-automated and ultra-fast cell-type identification using specific marker combinations from single-cell transcriptomic data.",
        "Multi- plexed droplet single-cell rna-sequencing using natural genetic variation.",
        "Assessing the limits of zero-shot foundation models in single-cell biology.",
        "Mars-seq2. 0: an experimental and analytical pipeline for indexed sorting combined with single-cell rna sequencing.",
        "A survey on graph representation learning methods.",
        "Glycobiology of innate immunology.",
        "Adam: A method for stochastic optimization.",
        "Semi-supervised classification with graph convolutional networks.",
        "Fast, sensitive and accurate integration of single-cell data with harmony.",
        "The human transcription factors.",
        "Contrastive representation learning: A framework and review.",
        "Deepcdr: a hybrid graph convolutional network for predicting cancer drug response.",
        "Deep generative modeling for single-cell transcriptomics.",
        "Mapping single-cell data to reference atlases by transfer learning.",
        "Benchmarking atlas-level data integration in single-cell genomics.",
        "Cellxgene: a performant, scalable exploration platform for high dimensional sparse matrices.",
        "schyena: Foundation model for full-length single-cell rna-seq analysis in brain.",
        "Scikit- learn: Machine learning in python.",
        "Hyena hierarchy: Towards larger convolutional language models.",
        "Identification of cell-type-specific marker genes from co-expression patterns in tissue samples.",
        "Improving language understanding by generative pre-training.",
        "Objective criteria for the evaluation of clustering methods.",
        "Universal cell embeddings: A foundation model for cell biology.",
        "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis.",
        "A survey on oversmoothing in graph neural networks.",
        "Droplet-based single cell rnaseq tools: a practical guide.",
        "Generative pretraining from large-scale transcriptomes for single-cell deciphering.",
        "Bias, robustness and scalability in single-cell differential expression analysis.",
        "Transfer learning enables predictions in network biology.",
        "Recent advances in cancer therapy: an overview.",
        "Attention is all you need.",
        "Graph attention networks.",
        "scemail: Universal and source-free annotation method for scrna-seq data with novel cell-type perception.",
        "Personalized pagerank to a target node, revisited.",
        "scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data.",
        "Direct comparative analyses of 10x genomics chromium and smart-seq2.Genomics, Proteomics and Bioinformatics",
        "Emergent abilities of large language models.",
        "Scanpy: large-scale single-cell gene expression data analysis.",
        "Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models.",
        "Graphformers: Gnn-nested transformers for representation learning on textual graph.",
        "Evaluating the utilities of large language models in single-cell data analysis.",
        "Large-scale cell representation learning via divide-and-conquer contrastive learning."
      ],
      "meta_data": {
        "arxiv_id": "2408.12373v2",
        "authors": [
          "Xinyu Yuan",
          "Zhihao Zhan",
          "Zuobai Zhang",
          "Manqi Zhou",
          "Jianan Zhao",
          "Boyu Han",
          "Yue Li",
          "Jian Tang"
        ],
        "published_date": "2024-08-22T13:15:49Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the limitation that existing transcriptome foundation models (TFMs) pre-train on large scRNA-seq corpora while treating cells as i.i.d. samples and ignoring hierarchical/lineage relationships among cell types encoded in cell ontology graphs. Proposes scCello, a cell-ontology guided TFM that injects ontology priors during pre-training to learn (i) biologically meaningful gene co-expression patterns, (ii) cell-type-coherent cell representations, and (iii) representation geometry consistent with inter-type ontology structure. Key findings: pre-training with ontology-guided objectives improves zero-shot and fine-tuned cell type identification (ID and multiple OOD shifts), enables substantially better zero-shot novel cell type classification by leveraging ontology structure, improves zero-shot marker gene prediction, yields competitive cancer drug response prediction when used as a feature extractor, and shows robustness to batch effects. Achieves strong performance with ~10.7M parameters (parameter-efficient vs much larger TFMs).",
        "methodology": "Backbone: transformer encoder on ranked gene-token sequences (Rank Value Encoding) with context length 2048 and masked gene prediction akin to BERT. Data are mapped to cell ontology identifiers. Pre-training optimizes a joint objective: (1) LMGP masked gene prediction loss to model gene co-expression; (2) intra-cellular cell-type coherence via supervised contrastive loss that pulls cell embedding z_i toward its cell-type embedding h_{c_i} and repels other types; plus (3) regularization LReg constraining cell-type embeddings to be an affine transform of cell embeddings; (4) inter-cellular relational alignment loss LInter that enforces similarities between cell embeddings (z_i^T z_j) to respect ontology-derived structural similarity sim(c_i,c_j). Ontology similarity computed from an undirected version of the cell ontology DAG using Personalized PageRank (PPR) from each type, then transformed via thresholding + log-scaling + discretization to reduce skew/noise; negatives for relational alignment are constructed by comparing sim levels, excluding ancestor types from negatives to avoid pushing away biologically related broad categories.",
        "experimental_setup": "Pre-training: 22,293,755 human scRNA-seq cells from CellxGene (Census 2023-07-25), filtered to primary cells, 10x-based protocols, and excluding cancer cells; 398 cell types mapped to Cell Ontology (OBO Foundry). Training: Adam (lr 1e-3, wd 1e-3, 3333 warmup steps), 40k steps, batch size 192 (effective via grad accumulation), 4×A100 40GB, ~2 days; model ~10.68M params. Benchmarks/tasks: (A) Zero-shot cell type clustering on 1 in-distribution dataset (Did) and 6 OOD datasets spanning unseen cell types (Dct1/2), unseen tissues (Dts1/2), unseen donors (Ddn1/2). Clustering via kNN + Louvain, evaluated by NMI, ARI, ASW, AvgBio. Baselines: TFMs (Geneformer, scGPT, scTab, UCE) and non-TFMs (Raw HVGs, Seurat, Harmony, scVI), plus ablations (MGP only; supervised only; MGP+Sup). (B) Fine-tuned cell type classification on Did: add linear head and fine-tune with supervised loss; metrics Acc, Macro-F1 plus clustering AvgBio; compare to scANVI and TFMs. (C) Zero-shot novel cell type classification: hold out cell types; represent known types via averaged embeddings; represent unknown types via ontology PPR similarities to known types; classify by correlating query-to-known similarity vector with unknown-to-known PPR vector (Spearman; also tested cosine), across difficulty levels (10%–100% novel types) with 20 random draws; metrics Acc, Macro-F1. (D) Zero-shot marker gene prediction: in-silico gene knockout per gene; score by embedding change; labels from CellMarker2 & PanglaoDB; datasets GSE96583 and GSE130148; metric AUROC; compare to differential expression tests. (E) Cancer drug response prediction: fixed TFM features integrated into DeepCDR pipeline to predict IC50; metric PCC; compare to DeepCDR and scFoundation and TFMs. (F) Batch integration robustness: scGPT protocol; metrics ASWb, GraphConn, AvgBatch, Overall. Ablations isolate effect of each loss term.",
        "limitations": "Relies on correctness/coverage and stability of the Cell Ontology; ontology is evolving, so pre-trained alignment may become outdated and currently lacks efficient continual/online updating without retraining. Ontology similarity is approximated via PPR with a hand-designed non-linear discretizing transform and threshold hyperparameter; this introduces design choices and may miss richer graph semantics. Approach assumes availability and accurate mapping of cell-type labels to ontology IDs for pre-training; mislabeled or coarse labels can affect intra/inter objectives. Pre-training data limited to human, 10x-based protocols and excludes cancer cells; generalization to other species, protocols (e.g., Smart-seq2, Drop-seq), and disease contexts is not established. Novel cell type classification method uses known-type prototypes computed from reference data subsets, and uses correlation between embedding similarities and PPR vectors—performance may depend on coverage of known types and ontology connectivity. Marker gene prediction uses in-silico knockout embedding shifts, which can also highlight essential/housekeeping genes or master regulators, not strictly markers; thus biological interpretability is imperfect. Computational cost is still substantial (multi-GPU pre-training) though parameter count is modest; also uses gene ranking/truncation (2048) which may drop low-expressed but relevant genes.",
        "future_research_directions": "Develop continual learning or lightweight fine-tuning methods to update the model as the cell ontology evolves (e.g., adapter-based updates, incremental embedding updates for new ontology nodes) without full retraining. Replace or augment PPR-based similarity with learnable ontology encoders (e.g., GNNs/graph transformers) while addressing sparsity and over-smoothing (e.g., positional encodings, diffusion kernels, residual/JK connections). Extend to multi-omics (ATAC, protein), cross-species ontologies, and broader sequencing protocols; study robustness to label noise and missing/uncertain ontology mappings. Improve marker gene discovery by distinguishing markers from essential genes (e.g., conditioning on specificity, integrating differential expression priors, causal perturbation models) and validate predicted markers experimentally. Scale model capacity and/or context length (full transcriptomes) while keeping parameter efficiency; explore curriculum or hierarchy-aware batching/negative sampling. Expand downstream evaluation to more disease settings (including cancer if reintroduced), cell state trajectories, and perturbation prediction, leveraging ontology and other biological graphs (pathways, GRNs).",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "End-to-End Ontology Learning with Large Language Models",
      "full_text": "End-to-End Ontology Learning with Large Language Models Andy Lo University of Cambridge cyal4@cam.ac.uk Albert Q. Jiang University of Cambridge qj213@cam.ac.uk Wenda Li University of Edinburgh wenda.li@ed.ac.uk Mateja Jamnik University of Cambridge mateja.jamnik@cl.cam.ac.uk Abstract Ontologies are useful for automatic machine processing of domain knowledge as they represent it in a structured format. Yet, constructing ontologies requires substantial manual effort. To automate part of this process, large language models (LLMs) have been applied to solve various subtasks of ontology learning. However, this partial ontology learning does not capture the interactions between subtasks. We address this gap by introducing OLLM, a general and scalable method for building the taxonomic backbone of an ontology from scratch. Rather than fo- cusing on subtasks, like individual relations between entities, we model entire subcomponents of the target ontology by finetuning an LLM with a custom regu- lariser that reduces overfitting on high-frequency concepts. We introduce a novel suite of metrics for evaluating the quality of the generated ontology by measuring its semantic and structural similarity to the ground truth. In contrast to standard syntax-based metrics, our metrics use deep learning techniques to define more robust distance measures between graphs. Both our quantitative and qualitative results on Wikipedia show that OLLM outperforms subtask composition methods, producing more semantically accurate ontologies while maintaining structural integrity. We further demonstrate that our model can be effectively adapted to new domains, like arXiv, needing only a small number of training examples. Our source code and datasets are available at https://github.com/andylolu2/ollm. 1 Introduction An ontology is a formal and structural way of representing domain-specific concepts and their relations [16]. They can be simple (e.g., Wikipedia categories) consisting of concepts and only a small number of types of taxonomic relations(e.g., is-a relationships), or they can be complex (e.g., Schema.org) consisting of axioms or many types of relations. For example, a simple ontology for programming languages might contain two concepts “Dynamically-typed language” and “Python”, and one relation “Dynamically-typed language → Python”, representing the knowledge that Python is a dynamically-typed language. A more complex ontology might contain axioms too, for example, “all programming languages are either dynamically or statically typed”. In this paper, we focus on ontologies with only concepts and taxonomic relations. Compared to typical deep learning models, which represent knowledge implicitly in its weights, ontologies capture knowledge in a structured and explicit manner, making them reliable, easy to edit and human-interpretable. Such benefits of ontologies have led to their wide adoption in practice. For example, Wikipedia categories have been 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2410.23584v1  [cs.LG]  31 Oct 2024Dataset ... ... LLM Mask-regularised loss Backpropagate Target LLM ... Output Sum and prune Gold standard evaluation TrainingEvaluation / Inference Document Concept Is-a relation Figure 1: OLLM: Using annotations of documents with their relevant concepts, we train an LLM to model relevant subgraphs of the target ontology with a custom regulariser. During inference, the generated subgraphs for each document are summed and pruned to give the final output ontology. For evaluation, we measure the similarity between the generated ontology and the ground truth. used for entity ranking [46] and information retrieval [42], or Schema.org [40] is a core component of the Semantic Web [1] initiative. While ontologies are useful, building ontologies often requires substantial manual effort. Ontology learning (OL) is the study of automating the construction of high-quality ontologies at scale. For a simple ontology, this amounts to discovering the concepts and taxonomic relations, usually based on a source corpus. In this paper we aim to develop domain-independent methods for OL that are scalable and produce better ontologies. Traditionally, OL is viewed as a composition of subtasks [3], such as concept discovery and relation extraction. In particular, prior works have demonstrated that state-of-the-art large language models (LLMs) can solve such subtasks effectively [4]. While studying subtasks permits fine-grained analysis and evaluation, it does not directly indicate the subsequent impact on the quality of the final ontology. Moreover, there is potential room for improvement by combining several subtasks into one, such as by modelling concepts and relations in conjunction. In this paper, we instead develop and evaluate methods that construct ontologies in an end-to-end fashion to answer the following research questions: 1. How can we leverage LLMs’ knowledge base to build ontologies from scratch? 2. Does our method scale efficiently to practical problem sizes? 3. How well does our method generalise to new domains? We introduce OLLM, an end-to-end method for using LLMs to construct ontologies at scale. Rather than focusing on individual relations between concepts, we finetune an LLM to model entire sub- components of the target ontology. The output ontology is generated by taking the sum of generated sub-components and applying simple post-processing. An overview of the pipeline is shown in Figure 1. To train OLLM, we collect the categorisation metadata for a subset of Wikipedia articles. We attempt to adapt an LLM to model the relevant categorisation subgraph for a particular Wikipedia article, but discover that direct finetuning leads to poor generalisation due to overfitting to high-level, frequently occurring concepts. Instead, we propose a custom regulariser that reweights each concept based on its frequency of occurrence, which substantially improves generalisation. We evaluate OLLM by measuring the similarity of the generated ontology with the ground truth. Current approaches for comparing ontologies rely on mapping components of the two ontologies onto each other, most commonly by literal text matching [30, 45]. This is unreliable when the two ontologies are not already sufficiently similar. Instead, we propose a suite of evaluation metrics suitable for comparing arbitrary labelled graphs. These metrics compare edges and subgraphs of the two ontologies using pretrained text embedders to test for semantic and structural similarity. Both our quantitative and qualitative results reveal that an LLM can already outperform existing 2extraction-based methods out of the box, and the performance is further improved by finetuning with our custom regulariser. We additionally demonstrate that OLLM can be adapted to build the arXiv ontology using only a small number of training examples, suggesting that our model can be applied to new domains in a data-efficient way. In summary, our contributions are: 1. We constructed two datasets based on Wikipedia and arXiv, which can serve as standard datasets for future work studying end-to-end OL. 2. We created OLLM, a method that utilises LLMs to build ontologies from scratch. OLLM produces high-quality ontologies and serves as a strong baseline for end-to-end OL. 3. We developed new evaluation metrics for assessing the quality of the generated ontologies. 2 Background An ontology is a structured way of representing concepts and relations of a shared conceptualisation, that is, domain knowledge [ 15, 16]. Ontologies can span a wide range of complexities. A fully- fledged ontology might contain concepts, relations, constraints, and axioms that enable complex automated reasoning. In this paper, we focus on the core building blocks of an ontology: concepts and taxonomic relations which represent is-a or is-subclass-of relationships between concepts. In some cases, the is-part-of relation is also considered a taxonomic relation. We treat such an ontology as a rooted labelled directed graph where nodes represent concepts, edges represent taxonomic relations and the root node is the special concept of all concepts. A strict ontology asserts that the taxonomic relation is asymmetric and thus the graph must be acyclic, though in practice some ontologies, such as the Wikipedia ontology studied in this paper, may contain cycles. We therefore do not assume that an ontology graph is necessarily acyclic. Examples of ontologies include WordNet [ 33] with 117,659 concepts and 89,089 taxonomic relations, and the Gene Ontology [2] with 42,255 concepts and 66,810 taxonomic relations. Ontology learning is the automatic extraction of ontological elements [17]. The most studied source of input is unstructured text, though there are also works on semi-structured data like HTML [22]. In this paper, the input is a set of documents, each consisting of some unstructured text. We additionally assume each document is associated with one or more concepts in the ground truth ontology, which we utilise for training. The goal is to reconstruct the ground truth ontology given the set of documents. Prior works view OL as a composition of subtasks, and study each subtask in isolation [ 3, 6]. A typical pipeline for building a simple ontology is to first perform concept discovery (identify the nodes), and then relation extraction (identify the edges) [ 8, 24]. A notable approach for relation extraction is Hearst patterns [ 18]. Hearst patterns are hand-crafted lexico-syntactic patterns that exploit natural language structure to discover taxonomic relations. For example, the pattern “[noun phrase] such as [noun phrase]” matches phrases like “dogs such as chihuahuas”, and thus can be processed by regular expressions to identify the relation “dog → chihuahua”. Hearst patterns suffer from low recall, as the relations must occur in exact configurations to be identified by the rules. Roller et al. [39] suggest smoothing techniques to alleviate this issue though at the cost of lower precision. Recently, language models have been used for OL. REBEL [7] treats relation discovery as a translation task, and finetunes encoder-decoder LLMs to extract both taxonomic and non-taxonomic relations. Babaei Giglou et al. [4] benchmarked a wide family of LLMs for concept and relation discovery, and showed promising results. However, the quadratic complexity of link prediction makes this approach unscalable to large ontologies. We provide more discussion in Appendix A.2.3. There are also proof-of-concept works for building ontologies end-to-end with LLMs. Funk et al. [13] proposes to build an ontology by recursively prompting LLMs, while Trajanoska et al. [44] generate the entire ontology in one completion. However, both studies are limited in the scale of the task and evaluation: they only considered ontologies of up to 1000 concepts and relied on manual qualitative evaluation. We bridge this gap by proposing a method that can scale to practical problem sizes and new metrics for systematic qualitative evaluation. The evaluation of ontologies is an open research area. The main approaches are gold standard evaluation [51], which matches elements of the generated ontology with a predefined target ontology; task-based evaluation [36], which measures the usefulness of the ontology on a specific application; and human evaluation [5, 37]. In this paper, we evaluate by the gold standard metric as it is the most straightforward approach when ground-truth ontology exists. Prior works have considered 3Main topic classiﬁcations Humanities Politics Culture Human behavior Politics by issue Sociology of culture Human activities Politics and race <s>[INST] Title: Hybridity Hybridity, in its most basic sense ... [/INST] Main topic classifications -> Human behavior -> Human activities -> Culture -> Sociology of culture Main topic classifications -> Humanities -> Politics -> Politics by issue -> Politics and race Main topic classifications -> Politics -> Politics by issue -> Politics and race Main topic classifications -> Culture -> Sociology of culture</s> Figure 2: Example subgraph induced for the Wikipedia page “Hybridity” (left), where N = 4and C = {Politics and race, Sociology of culture}. The corresponding training text sequence (right), where text coloured in grey is ignored as training targets, but is still present as context for later tokens. matching concepts [30] and direct or indirect relations [23, 45] by literal text comparison. Others have also considered edit-distance [12] or bag-of-words distributional similarity for text comparison [51]. These techniques for measuring semantic similarity may be considered unreliable and have been superseded by current methods [9]. We instead rely on more modern techniques like pretrained text embedders [10] and graph convolutions [26] to match substructures between the two ontologies. 3 OLLM We now introduce OLLM, our novel, simple and scalable method for end-to-end OL with LLMs. On a high level, OLLM uses an LLM to model concept subgraphs of the target ontology by utilising a linearisation scheme to transform subgraphs into string sequences. In contrast to learning individual edges, modelling subgraphs allows the model to learn higher-order structures, such as the interactions between three or more nodes. To create the training dataset, OLLM relies on the annotations of documents to concepts to generate document-subgraph pairings. Such subgraphs are much smaller than the complete graph, so they can be learned by the model more easily. The generated subgraphs for each document are summed into a weighted graph, and simple post-processing is applied to obtain the final predicted ontology. 3.1 Subgraph modelling Here, we describe the method for creating document-subgraph pairings. Given a document and its associated set of concepts C, we define the relevant pathsas the set of paths of at most length N from the root to any of the concepts in C. The relevant subgraphis the set of nodes (concepts) and edges (taxonomic relations) that occur at least once in the relevant paths. An example is shown in Figure 2 (left). The choice of N is task-specific and we describe our method for choosing N in Section 5.1. To employ LLMs to model the subgraphs, we must linearise the graph into a string sequence. Existing methods for autoregressive graph generation employ BFS [50] or DFS [14] ordering starting at an arbitrary node. We instead choose to linearise the subgraph as a list of relevant paths that produced the subgraph in the first place. We do so over BFS/DFS ordering for three reasons: 1) the subgraph is defined from the relevant paths, which makes them the most natural representation; 2) we hypothesise that the hierarchy of concepts in each path is a desirable inductive bias for the hierarchical nature of an ontology; and 3) the path-based representation is much easier to describe in natural language instructions so that our LLM prompting-based baselines may produce reasonable results without finetuning. The linearisation template can be found in Figure 5 in Appendix A.1.2. 3.2 Post-processing The final output graph is obtained by summing all generated subgraphs for each document and pruning low-weighted components. Given the generated subgraphs G1 = (V1, E1), . . . , Gn = (Vn, En), the raw output graph is defined as Graw = (Vraw, Eraw), where Vraw = ∪n i=1Vn and Eraw = ∪n i=1En. 4Each edge (u, v) ∈ Eraw is additionally weighted by the number of times it occurs in the collection of subgraphs: w(u, v) =Pn i=1 1 [(u, v) ∈ En]. A few simple post-processing steps are then applied to Graw in order to prune it: 1. Self-loop pruning: All edges (u, u) ∈ Eraw are removed. 2. Inverse-edge pruning: For (u, v) ∈ Eraw, if (v, u) ∈ Eraw and w(v, u) > w(u, v), remove (u, v). That is, bidirectional edges are turned into unidirectional ones. 3. Absolute thresholding: Edges in Eraw with weight below the α-th quantile are removed, where 0 ≤ α ≤ 1 is a hyperparameter. This removes edges that are globally less important. 4. Relative thresholding: For each vertexu ∈ Vraw, let e1, . . . , ek be the outgoing edges fromu, sorted by weight in ascending order. Let the cumulative weight be C(ei) =Pi j=1 w(ej)/ Pk j=1 w(ej). The edges {ei | C(ei) ≤ β} are pruned, where 0 ≤ β ≤ 1 is a hyperparameter. This is similar to top-p sampling [19], which we use to remove edges that are less important than their neighbours. 5. Clean up: After pruning all edges, nodes with no incoming or outgoing edges are removed. We choose the hyperparameters α and β by tuning on the validation set (Section 5.1). 4 Evaluating end-to-end OL Ontology evaluation is a hard problem as there are no quantitative definitions of what constitutes a “good ontology”, and metrics generally only capture one aspect (e.g., structure but not semantics) of an ontology. We approach evaluation by treating the ground truth as a proxy for a good ontology, and comparing the generated ontologies against the ground truth. Here, we describe how the ground truth is obtained, and introduce new evaluation metrics that are used for measuring ontology similarity. 4.1 Dataset We collect the datasets for the two ontologies considered in this paper: Wikipedia categories and the arXiv taxonomy. We use Wikipedia for learning and in-domain evaluation, and arXiv for out-of- domain evaluation. To build the Wikipedia dataset, we perform a BFS traversal from its root category “Main topic classifications” up to depth 3. For every category encountered, we retrieve the titles and summaries (the text before the first section) of up to 5000 pages that belong in that category. The source data is obtained from the Wikipedia API.1 The arXiv taxonomy is available from its home page, and the source corpus is constructed from the title and abstract of all the papers uploaded to arXiv in the years 2020–2022 with more than or equal to 10 citations.2 In total, the Wikipedia dataset has 13886 concepts, 28375 taxonomic relations and 362067 documents, while the arXiv dataset has 161 concepts, 166 taxonomic relations and 126001 documents. 3499 2127227 6289 1125 226 393 Train Eval Test (a) Wikipedia 61 381 56 2 21 Train Eval Test (b) arXiv Figure 3: Intersection of concepts among the train, validation and test splits of the datasets. Generating the train and test splits from the datasets is a non-trivial problem. Each train- ing example consists of a document and its rele- vant subgraph (Section 3.1). The naive approach of randomly selecting a subset of document- subgraph pairs for the training likely leads to data leakage as there might be a significant over- lap between subgraphs in the training set and the test set. Instead, we first split the full ontology into train and test graphs, and then generate the training document-subgraph pairs. This ensures that there are sufficiently many unseen concepts (and thus relations) in the test split, as shown in Figure 3. Our method is as follows: 1. Let V top be the set of top-level nodes, that is, children of the root node. Randomly partition V top into train V top train, validation V top val , and test V top test splits in 7:3:10 ratio. 1https://en.wikipedia.org/w/api.php 2Citation counts obtained from https://api.semanticscholar.org/. 52. Let d be the depth on the full graph, that is, the distance of the furthest node from the root. The nodes of the train graph are taken as the union of all the nodes that are within distance d − 1 from any node in V top train, plus V top train and the root. The edges are all the edges in the full graph that have both endpoints in the train graph. Similar applies for V top val and V top test . 4.2 Metrics Many existing methods for comparing ontologies rely on syntactic measures like string edit dis- tance [12] as a proxy for semantic similarity, or require every concept to be tagged with descriptions or documents for distributional semantics comparison [51]. To obtain more robust and general evaluation results, we introduce a suite of similarity metrics that use modern methods like text embeddings [38]. Multiple metrics are used as they trade off between interpretability and comprehensiveness and we aim to make them complementary by capturing different aspects of an ontology. For example, com- paring ontologies by literal text equality is easy to understand but may be unreliable. In Section 5.4, we provide further discussion on evaluation metrics in the context of our experiment results. We denote the ground truth ontology graph as G = (V, E) and the generated graph as G′ = (V ′, E′). Literal F1 While literal text matching is unreliable, it is also the simplest and the most interpretable. We treat this metric as a reference metric for sanity check. The Literal F1 metric [23] is given by the harmonic mean of the precision and recall of the edges: Literal precision = |E ∩ E′| |E′| Literal recall = |E ∩ E′| |E| Fuzzy F1 The Literal F1 metric puts a strong emphasis on using the correct wording, while in prac- tice, we are interested in evaluating the semantics of an ontology. For example, using a synonymous phrase for a concept should not be penalised. We utilise embeddings from a pretrained sentence trans- former [38] and use the cosine similarity of the embeddings to measure semantic similarity. Specifi- cally, let NodeSim(u, u′) ∈ V × V ′ → [−1, 1] be the cosine similarity between the sentence embed- dings for u and u′. The Fuzzy F1 score is obtained from the fuzzy precision and recall, defined as: Fuzzy precision = |{(u′, v′) ∈ E′ | ∃(u, v) ∈ E. NodeSim(u, u′) > t∧ NodeSim(v, v′) > t}| |E′| Fuzzy recall = |{(u, v) ∈ E | ∃(u′, v′) ∈ E′. NodeSim(u, u′) > t∧ NodeSim(v, v′) > t}| |E| where t is the matching threshold. We use all-MiniLM-L6-v2 [38, 47] as the embedding model, and choose t as the median cosine similarity between the synonyms in WordNet [33], computed as 0.436. Continuous F1 With fuzzy comparisons, the matches between the edges of the generated and the ground truth graph are no longer one-to-one. This is problematic: consider two graphs A → B and B ← A → B′, where B and B′ match fuzzily. Such graphs will achieve a perfect Fuzzy F1 score yet they significantly differ. Additionally, we found that the previous metrics fail to provide a useful signal for hyperparameter tuning, particularly for our baselines where the generated graphs are poor. The Continuous F1 metric solves these issues by computing the highest-scoring edge matching between the two graphs, where the similarity score between (u, v) and (u′, v′) is given by min(NodeSim(u, u′), NodeSim(v, v′)). Obtaining such matching is equivalent to solving the linear assignment problem [ 32], which can be computed by the Hungarian algorithm [ 27]. The Continuous F1 score is obtained from the continuous precision and recall, given by: Continuous precision = scont |E′| Continuous recall = scont |E| where scont is the score achieved by the best edge matching. 6(a) Direct finetuning (b) Finetuning with masked loss Figure 4: Per token loss on a test set example of the final model trained with and without the custom masked loss objective. A stronger red colour represents a higher cross-entropy loss. Within the top-level concepts (children of the root) shown here, “Culture” and “Humanities” are in the training set while others are not. Using the masked loss objective improves generalisation on the high-level relations (e.g., “Main topic classifications” → “Academic disciplines”) while maintaining performance on lower-level relations. Graph F1 Instead of individual edges, this metric aims to capture the wider structure of the two graphs. Intuitively, we want to know how concepts are related to their local neighbourhood. We do so by using simple graph convolutions [49] with K = 2to compute graph-aware node embeddings after embedding each node with the pretrained embedder. Such embeddings in G are compared against those in G′ by cosine similarity, and the highest-scoring node matching, similar to the Continuous F1 metric, gives the graph similarity score. The Graph F1 score is computed from the graph precision and recall, defined as: Graph precision = sgraph |V ′| Graph recall = sgraph |V | where sgraph is the score achieved by the best node matching. Motif distance Taking inspiration from classical network analysis, we use network motifs[34, 41] to evaluate the structural integrity of the generated graphs. Network motifs are reoccurring subgraphs in a larger graph, most commonly 3-vertex subgraphs. They are typically indicative of the structural characteristics of the full graph. We define the motif distance as the total variation distance between the distribution of all 3-vertex subgraphs in G and G′. 5 Experiments We design our experiments to answer the following research questions: 1. Does OLLM produce better ontologies than traditional methods by subtask composition? 2. Can OLLM be easily adapted to a new domain? We approach the questions by training OLLM on the Wikipedia dataset, and further transfer the model to arXiv with a small number of arXiv samples. As baselines, we use two relation extraction methods, Hearst patterns [18, 39] and REBEL [7]. Relation extraction depends on successful concept discovery to produce high-quality ontologies. To estimate a ceiling to such baselines, we give the baselines a substantial advantageby providing them with the ground truth concepts in the test graph. The results show that even with such an advantage, OLLM outperforms the baselines on many metrics, demonstrating the potential of OLLM for end-to-end OL (Section 5.3). 75.1 Implementation details Analysing the per-token loss on the test split sequences of a directly finetuned model (Section 3.1) shows that the model tends to memorise high-level relations from the training set, leading to poor generalisation, as shown in Figure 4 (top). The crux of the problem is that low-level relations are substantially more diverse than high-level ones: since we present both types of relations at the same rate to the model, it tends to overfit on high-level relations while underfitting on low-level ones. To alleviate this issue, we introduce a new training objective that randomly masks the loss contribution of frequently occurring relations. Suppose a relation u → v is present n times in the training set. During training, when u → v appears in one of the relevant paths, we mask the loss contribution of the tokens for v with probability max(1 − M/n, 0), where M is a constant for the average number of times a relation is present in the training set. Intuitively, this regulariser ensures that frequent relations are only seen ≈M times as targets throughout training, hence reducing overfitting as shown in Figure 4 (bottom). Note that while v is masked from the target, its tokens are still present in the input sequence as context for later tokens. A concrete training example can be found in Figure 2 (right). We finetune Mistral 7B v0.2 [21] with Low-Rank Adaptation [20] on the masked loss objective. The model is trained on the Wikipedia dataset for two epochs with Adam [ 25]. During inference, the outputs are generated with temperature 0.1 and nucleus sampling [ 19] top-p of 0.9. We include a finetuning baseline without the masked loss objective, denoted as Finetune. To adapt OLLM for arXiv, we further finetune the model on 2048 document-subgraph pairs from arXiv. We initialise new low-rank adaptors and train until the loss stops improving on the validation set. We name these models OLLM (transfer) and Finetune (transfer) for training with and without the masked loss objective, respectively. Full details for the Wikipedia and arXiv experiments can be found in Appendix A.1.2. The hyperparameters for the post-processing steps are tuned by grid search on the validation set. We sweep over α ∈ 1 − geomspace(1/|Eraw|, 1, 21) and β ∈ geomspace(0.1, 1, 21) − 0.1, and use the values that maximise Continuous F1. For Wikipedia, we choose the subgraph modelling path length N = 4as it is the smallest N such that almost all edges (> 99%) occur in at least one relevant subgraph. Such criterion is used since smaller N results in smaller subgraphs, which we expect to be easier to model accurately. We choose N = 3for arXiv for the same reason. 5.2 Baselines We give a brief overview of the baseline methods here (in addition to Finetune and Finetune (transfer)). The full implementation details can be found in Appendix A.1. All baselines produce weighted directed graphs which we apply the same post-processing steps as in OLLM (Section 3.2) to obtain the final predicted graph. Memorisation Simply memorising the train graph is a surprisingly strong baseline due to the overlap between train and test graphs, especially for Wikipedia. The weight of each edge is given by the number of relevant subgraphs in which it appears. Hearst We follow the improved implementation of Hearst patterns by Roller et al. [39]. The authors propose spmi, a method which uses low-rank approximations to smooth the relation matrix so that two concepts can be compared even if there are no direct matches between them. We use the smoothed relation matrix to weigh the relations between the ground truth concepts. The additional hyperparameter for the rank of the smoothed matrix is tuned by grid search over the validation set. REBEL The REBEL-large model [7] is an LLM trained to extract many types of relations from Wikipedia articles. We only take the “subclass of”, “instance of”, “member of” and “part of” relations that were extracted. Similar to Hearst, we find that it fails to find many direct relations between ground truth concepts. The same low-rank smoothing technique is applied to improve recall. Prompting We test the Zero/One/Three-shot performance of instruction-tuned LLMs on the subgraph modelling task described in Section 3.1. To obtain more comparable results, we use Mistral 7B Instruct v0.2, the instruction-tuned version of the base model of OLLM, as the LLM for our prompting baseline. The prompt template used is shown in Figure 6 in Appendix A.1. Finetune To test the effectiveness of our masked-loss objective, we introduce a direct finetuning baseline using the same configuration as OLLM except it is trained without loss masking. 8Table 1: Evaluation metrics of OLLM and baselines on Wikipedia and arXiv. OLLM performs particularly well in modelling semantics, and remains competitive syntactically and structurally. Dataset Method Literal F1 ↑ Fuzzy F1 ↑ Cont. F1 ↑ Graph F1 ↑ Motif Dist. ↓ Wikipedia Memorisation 0.134 0.837 0.314 0.419 0.063 Hearst 0.003 0.538 0.350 0.544 0.163 Rebel 0.004 0.624 0.356 0.072 0.132 Zero-shot 0.007 0.871 0.455 0.639 0.341 One-shot 0.031 0.888 0.477 0.610 0.314 Three-shot 0.031 0.880 0.475 0.622 0.354 Finetune 0.124 0.884 0.470 0.588 0.050 OLLM 0.093 0.915 0.500 0.644 0.080 arXiv Memorisation 0.000 0.207 0.257 0.525 0.037 Hearst 0.000 0.000 0.151 0.553 0.098 Rebel 0.000 0.060 0.281 0.546 0.088 Zero-shot 0.025 0.450 0.237 0.414 0.145 One-shot 0.072 0.460 0.290 0.433 0.293 Three-shot 0.051 0.405 0.212 0.385 0.124 Finetune (transfer) 0.000 0.440 0.225 0.441 0.148 OLLM (transfer) 0.040 0.570 0.357 0.633 0.097 5.3 Results We first evaluate whether OLLM can accurately create ontologies with many concepts and relations, such as the Wikipedia categories. Computationally, OLLM required 12 A100-hours for training and 7 A100-hours for inference to generate an ontology for Wikipedia. This is a modest cost in current standards, which demonstrates the scalability of OLLM for real-world problems. In terms of performance, OLLM produces the most semantically accurate ontology in comparison to our baselines as presented in Table 1. Across all of Fuzzy F1, Continuous F1 and Graph F1, we observe the trend that OLLM scores the best, followed by Finetune and Prompting, and lastly Hearst and REBEL. This is surprising, as it suggests that the combination of LLMs with our subgraph modelling framework is a sufficiently strong inductive bias for LLMs to outperform traditional methods even without finetuning. However, prompting alone is not sufficient to build high-quality ontologies. On the Motif Distance metric, prompting methods score poorly at 0.314–0.354 in comparison to 0.050 and 0.080 for Finetune and OLLM respectively. This shows that using LLMs out-of-the-box for subgraph modelling results in poor structural integrity, though this issue is solved by finetuning. Qualitatively, we observe that OLLM can adhere to the clear, explicit naming style of Wikipedia, even on unseen topics in the test set. For example, it generates “Mathematical categories” and “Groups (mathematics)” under the parent concept “Mathematical structures” to distinguish from the natural language sense of categories and groups (Figure 11c). Such style is not learned by the prompting baselines: Three-shot generated “Elections → France”, while it most likely meant “Elections → Elections in France” (Figure 18c). More sample outputs are shown in Appendix A.4.1. The arXiv task differs from the Wikipedia task as it has much fewer relations, and there is even less overlap between the train and test split. This imposes a great challenge on Finetune and OLLM as they need to generalise with a limited diversity of training samples. Despite such constraints, OLLM is substantially better than other methods in modelling the semantics of the test graph. On the Fuzzy F1, Continuous F1, and Graph F1 metrics, OLLM performs the best among all methods with 0.570, 0.357, and 0.633, significantly higher than the next-best of 0.460, 0.290 and 0.546 respectively. Inspecting the generated ontologies (Appendix A.4.2), we observe that prompting baselines tend to produce repetitive concepts such as “Machine Learning and Artificial Intelligence” and “Artificial Intelligence and Machine Learning” (Figure 27), while Hearst and REBEL put almost all concepts under the same parent concept(s) (Figures 23 and 24). We also found that OLLM’s output for arXiv contains concepts from Wikipedia, but restructured in a way that fits the arXiv ontology. For example, “Life sciences” and “Biological evolution” appear in the Wikipedia training set under the same parent category “Life” with no direct links between them. On the generated graph for arXiv, “Life sciences” is instead promoted to one of the top-level concepts with “Biological Evolution” as one of its children, which better fits the “fields of science” style of the arXiv ontology (Figure 20). This demonstrates that OLLM can adapt to produce a new type of ontology by restructuring its learned concepts, all using just a small number of training samples. 9In summary, OLLM scores the best or is competitive across all metrics in both tasks, with the notable exception of the Literal F1 metric. We attribute this to the fact that Literal F1 is sensitive to factors like casing and choice of words, and generally only measures syntactic similarity. For example, we see that a suboptimal baseline like Memorisation scores the best on this metric with 0.134 on the Wikipedia task. This reflects that syntactic similarity generally does not entail semantic similarity, so syntax-based metrics should not be used as stand-alone measures for ontology quality. 5.4 Meta-evaluation In this section, we analyse the usefulness of our new metrics for measuring graph similarity and discuss the limitations of existing metrics. On the Wikipedia task, Memorisation, despite being clearly the worst in Continuous F1 and Graph F1, performs the best on Literal F1 and the second-best on Motif Distance. This can be attributed to the fact that Literal F1 is sensitive to semantically insignificant syntactic differences such as casing and word form, and thus when the training and test set has non-trivial overlap (Figure 3), it is biased towards methods that overfit. Similarly, as per the method described in Section 4.1, the data splits are constructed with structural symmetry, hence we expect the train and test splits to have a similar graph structure even though the represented concepts are different. As a result, methods that tend to overfit, for example, Memorisation and Finetune, achieve the best scores on Motif Distance. This demonstrates that Literal F1 and Motif Distance only capture syntactic and structural similarity respectively, and thus should not be used as stand-alone metrics for evaluation. Analysing the edge and node matchings found by our Continuous F1 and Graph F1 metrics on arXiv reveals that they successfully capture some human intuition on semantic similarity between the two ontologies. In Figures 9 and 10, we visualise the ontology generated by OLLM and the ground truth and observe that semantically similar components in the two graphs indeed get matched. For example, the “Physics” and “Mathematics” clusters in the generated graph get matched with the “Mathematics” cluster in the ground truth, “Data Analysis” and “Information” get matched with “Statistics”, “Economics” with “Quantitative Finance”, and “Life Sciences” with “Quantitative Biology”. This suggests that our edge/node matching procedure is capturing a “semantic graph isomorphism” that allows one to compare similar components in the two graphs, even if they do not exactly share the same concepts. We believe this example of a semantic mapping from one ontology to another is strong evidence that our metrics are capturing meaningful qualities of the ontologies. 6 Discussion Limitations We only study and evaluate the construction of simple ontologies with only concepts and taxonomic relations. A potential approach to extend OLLM to produce non-taxonomic relations is to add tags indicating the relation type to each edge when linearising the subgraphs for sequence modelling. New evaluation metrics might also be required to handle multiple types of relations. Another limitation is that the taxonomic relations in the generated ontologies are not necessarily transitive due to the existence of cycles. This is a general problem for many OL methods and there are existing works on cycle removal algorithms for cleaning hierarchies [ 43, 52]. We ablate this in Appendix A.2.1 and found that the generated ontology can be made consistent by removing a small number of edges. Furthermore, we were unable to fully control for data contamination as the pretraining dataset of Mistral 7B is not publically known. We do, however, observe that the generated ontologies are sufficiently different from the ground truth, indicating that OLLM is not directly remembering samples from its pretraining stage. Conclusion In this paper, we introduce a general method for building ontologies in an end-to-end fashion. We propose a set of metrics for end-to-end OL that measures the semantic and structural similarity between arbitrary labelled graphs. Our model, OLLM, outperforms traditional subtask composition methods in reconstructing the Wikipedia categories, and can be transferred to build ontologies for arXiv after finetuning on a small number of examples. Using LLMs as the backbone for subgraph modelling opens up exciting avenues for future research. For example, one may generate ontologies from corpora with images using vision language models [11]. 107 Acknowledgements We thank Dr Thomas Sauerwald for suggesting network motifs as a basis for evaluation. AQJ acknowledges the support of a Peterhouse Graduate Studentship. References [1] Grigoris Antoniou and Frank Van Harmelen. A semantic web primer. MIT press, 2004. [2] Michael Ashburner, Catherine A Ball, Judith A Blake, David Botstein, Heather Butler, J Michael Cherry, Allan P Davis, Kara Dolinski, Selina S Dwight, Janan T Eppig, et al. Gene ontology: tool for the unification of biology. Nature genetics, 25(1):25–29, 2000. [3] Muhammad Nabeel Asim, Muhammad Wasim, Muhammad Usman Ghani Khan, Waqar Mah- mood, and Hafiza Mahnoor Abbasi. A survey of ontology learning techniques and applications. Database, 2018:bay101, 2018. [4] Hamed Babaei Giglou, Jennifer D’Souza, and Sören Auer. Llms4ol: Large language models for ontology learning. In International Semantic Web Conference, pages 408–427. Springer, 2023. [5] Janez Brank, Marko Grobelnik, and Dunja Mladenic. A survey of ontology evaluation tech- niques. In Proceedings of the conference on data mining and data warehouses (SiKDD 2005), pages 166–170. Citeseer, 2005. [6] Paul Buitelaar, Philipp Cimiano, and Bernardo Magnini. Ontology learning from text: methods, evaluation and applications, volume 123. IOS press, 2005. [7] Pere-Lluís Huguet Cabot and Roberto Navigli. Rebel: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2370–2381, 2021. [8] Philipp Cimiano and Johanna Völker. Text2onto: A framework for ontology learning and data-driven change discovery. In International conference on application of natural language to information systems, pages 227–238. Springer, 2005. [9] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017. [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [11] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2625–2634, 2015. [12] Marc Ehrig, Peter Haase, Mark Hefke, and Nenad Stojanovi ´c. Similarity for ontologies - a comprehensive framework. In European Conference on Information Systems, 2005. URL https://api.semanticscholar.org/CorpusID:9982461. [13] Maurice Funk, Simon Hosemann, Jean Christoph Jung, and Carsten Lutz. Towards ontology construction with language models. arXiv preprint arXiv:2309.09898, 2023. [14] Nikhil Goyal, Harsh Vardhan Jain, and Sayan Ranu. Graphgen: A scalable approach to domain-agnostic labeled graph generation. In Proceedings of The Web Conference 2020, pages 1253–1263, 2020. [15] Thomas R Gruber. A translation approach to portable ontology specifications. Knowledge acquisition, 5(2):199–220, 1993. [16] Thomas R Gruber. Toward principles for the design of ontologies used for knowledge sharing? International journal of human-computer studies, 43(5-6):907–928, 1995. 11[17] Maryam Hazman, Samhaa R El-Beltagy, and Ahmed Rafea. A survey of ontology learning approaches. International Journal of Computer Applications, 22(9):36–43, 2011. [18] Marti A Hearst. Automated discovery of wordnet relations. WordNet: an electronic lexical database, 2, 1998. [19] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. [20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [21] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [22] Lobna Karoui, Marie-Aude Aufaure, and Nacera Bennacer. Ontology discovery from web pages: Application to tourism. In In the Workshop of Knowledge Discovery and Ontologies. Citeseer, 2004. [23] Vipul Kashyap, Cartic Ramakrishnan, Christopher Thomas, and A. Sheth. Taxaminer: an experimentation framework for automated taxonomy bootstrapping. Int. J. Web Grid Serv., 1: 240–266, 2005. URL https://api.semanticscholar.org/CorpusID:5549251. [24] Neha Kaushik and Niladri Chatterjee. Automatic relationship extraction from agricultural text for ontology construction. Information processing in agriculture, 5(1):60–73, 2018. [25] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [26] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [27] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83–97, 1955. [28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023. [29] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [30] Alexander Maedche and Steffen Staab. Measuring similarity between ontologies. In Asunción Gómez-Pérez and V . Richard Benjamins, editors, Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web, pages 251–263, Berlin, Heidelberg, 2002. Springer Berlin Heidelberg. ISBN 978-3-540-45810-4. [31] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pages 55–60, 2014. [32] Silvano Martello and Paolo Toth. Linear assignment problems. In North-Holland Mathematics Studies, volume 132, pages 259–282. Elsevier, 1987. [33] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11): 39–41, 1995. 12[34] Ron Milo, Shai Shen-Orr, Shalev Itzkovitz, Nadav Kashtan, Dmitri Chklovskii, and Uri Alon. Network motifs: simple building blocks of complex networks. Science, 298(5594):824–827, 2002. [35] Simone Paolo Ponzetto, Michael Strube, et al. Deriving a large scale taxonomy from wikipedia. In AAAI, volume 7, pages 1440–1445, 2007. [36] Robert Porzel and Rainer Malaka. A task-based approach for ontology evaluation. In ECAI Workshop on Ontology Learning and Population, Valencia, Spain, volume 1. Citeseer Valencia, Spain, 2004. [37] Joe Raad and Christophe Cruz. A survey on ontology evaluation methods. In Proceedings of the International Conference on Knowledge Engineering and Ontology Development, part of the 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, 2015. [38] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL http://arxiv.org/ abs/1908.10084. [39] Stephen Roller, Douwe Kiela, and Maximilian Nickel. Hearst patterns revisited: Automatic hypernym detection from large text corpora. arXiv preprint arXiv:1806.03191, 2018. [40] Schema.org. Schema.org, 2011. URL https://www.schema.org/. [41] Shai S Shen-Orr, Ron Milo, Shmoolik Mangan, and Uri Alon. Network motifs in the transcrip- tional regulation network of escherichia coli. Nature genetics, 31(1):64–68, 2002. [42] Philipp Sorg and Philipp Cimiano. Exploiting wikipedia for cross-lingual and multilingual information retrieval. Data & Knowledge Engineering, 74:26–45, 2012. [43] Jiankai Sun, Deepak Ajwani, Patrick K Nicholson, Alessandra Sala, and Srinivasan Parthasarathy. Breaking cycles in noisy hierarchies. In Proceedings of the 2017 ACM on Web Science Conference, pages 151–160, 2017. [44] Milena Trajanoska, Riste Stojanov, and Dimitar Trajanov. Enhancing knowledge graph con- struction using large language models. arXiv preprint arXiv:2305.04676, 2023. [45] Pucktada Treeratpituk, Madian Khabsa, and C. Lee Giles. Graph-based approach to auto- matic taxonomy generation (grabtax). ArXiv, abs/1307.1718, 2013. URL https://api. semanticscholar.org/CorpusID:8625171. [46] Anne-Marie Vercoustre, Jovan Pehcevski, and James A Thom. Using wikipedia categories and links in entity ranking. In Focused Access to XML Documents: 6th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2007 Dagstuhl Castle, Germany, December 17-19, 2007. Selected Papers 6, pages 321–335. Springer, 2008. [47] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776–5788, 2020. [48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022. [49] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861–6871. PMLR, 2019. [50] Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generat- ing realistic graphs with deep auto-regressive models. In International conference on machine learning, pages 5708–5717. PMLR, 2018. 13[51] Elias Zavitsanos, Georgios Paliouras, and George A. V ouros. Gold standard evaluation of ontology learning methods through ontology transformation and alignment. IEEE Trans- actions on Knowledge and Data Engineering, 23:1635–1648, 2011. URL https://api. semanticscholar.org/CorpusID:15607684. [52] Torsten Zesch and Iryna Gurevych. Analysis of the wikipedia category graph for nlp applications. In Proceedings of the Second Workshop on TextGraphs: Graph-Based Algorithms for Natural Language Processing, pages 1–8, 2007. 14A Appendix / supplemental material A.1 Experiment details A.1.1 Wikipedia Some prior works that use Wikipedia categories as a dataset perform additional filtering of concepts considered as “meta-categories” mainly used for page management [35]. We instead decided not to further filter the source data to minimise external bias. We note that it is often not clear-cut whether a Wikipedia category is just for page management. For example, the Wikipedia categories of the form “Lists of [subject]” refer to the special type of articles where the main body is a bullet point/table listing of the subject, which is a useful concept in the Wikipedia domain. A.1.2 OLLM For the Wikipedia experiment, we use Mistral 7B v0.2 (not instruction-tuned) [21] as the base model. We attach LoRA [20] adaptors to all attention and feed-forward layers with parameters r = 32and α = 16. The model is trained for 2 epochs (≈ 17K steps) with batch size 16, context length 2048, and is optimised with Adam using a constant learning rate of 1e-5 with warm-up from zero for the first 100 steps. Finetune uses the same configuration. Training takes 12 A100-hours. For the arXiv experiment, we further finetune the model trained on Wikipedia with masked loss objective on 2048 document-subgraph pairs from the arXiv training set. We merge the LoRA adaptors from the Wikipedia experiment and initialise new ones with r = 8and α = 8. The model is trained with batch size 16 and Adam with constant learning rate 3e-6 and warp-up from zero for the first 10 steps. Training terminates when the loss stops improving on the evaluation set, which happened at step 288. Finetune (transfer) uses the same configuration. Early stopping happened at step 192. For both experiments, we finetune the model with the instruction template similar to that of Mistral 7B instruct v0.2. The format is shown below: <s>[INST]\\ Title: {{ title }} {{ abstract }}[/INST]\\ {% for path in paths %} {{ path | join(\" -> \") }} {% endfor %}\\ </s> Figure 5: Linearisation template for OLLM training. For inference, we use the vLLM [28] server which achieves a throughput of ≈ 10 documents per second. Inference on the validation and test splits of both datasets takes 12 A100-hours in total. A.1.3 Hearst The Hearst baseline follows the implementation by Roller et al. [39]. Using the tokenization, part- of-speech tagging, lemmatisation, and token regex functionality of the CoreNLP pipeline [ 31], taxonomic relations are extracted according to the 28 Hearst patterns used by the authors. Processing all documents takes 10 CPU-hours. Following the spmi method, low-rank smoothing is applied to the relation matrix to allow com- parison between any two concepts even if they are not directly related by an extracted rela- tion. The rank of the smoothed matrix, r, is a hyperparameter which we tune by sweeping over r ∈ {5, 10, 15, 20, 25, 50, 100, 150, 200, 250} on the validation set. This defines a dense weighted graph as the raw output. Unfortunately, computing Continuous F1 on a dense graph is very slow, especially for Wikipedia. This is because the Hungarian algorithm used for solving the optimal matching between edges has time complexity O(N3), where N is the number of edges. To bypass this issue, we perform a pre-filtering step of only exporting the top 10|V | weighted edges in the smoothed relation matrix, where |V | is the number of nodes in the graph. For the datasets considered, 15this density of edges is still much higher than that of the ground truth, and thus, we expect this to have minimal impact on the final output after post-processing. A.1.4 REBEL We use REBEL-large [7] in the implementation. The model is an encoder-decoder transformer based on BART-large [29] with 406M parameters. We sample the model with the default configuration used by Cabot and Navigli [7]. The model is trained to predict 220 types of relations, most of which are not taxonomic relations. We filter the extracted relations and only keep those tagged with “subclass of”, “instance of”, “member of”, and “part of” relation types. The same low-rank smoothing method as Hearst is applied to the raw extractions. Processing all documents takes 3 A100-hours. A.1.5 Prompting To obtain more comparable results, we use Mistral 7B Instruct v0.2, the instruction-tuned version of the base model of OLLM, as the LLM for our prompting baseline. For One-shot and Three-shot, we randomly sample examples from the training set for each query. The output is parsed using regex and results that do not match the regex are discarded. We perform manual prompt engineering by inspecting individual responses. The final prompt template is shown in Figure 6. The total inference cost for all prompting baselines is ≈ 50 A100-hours. 16The following is an article’s title and abstract. Your task is to assign this article to suitable category hierarchy. A category is typically represented by a word or a short phrase, representing broader topics/concepts that the article is about. A category hierarchy represented by a collection of paths from the generic root category \"Main topic classifications\" to a specific category suitable for the article. The topics titles should become more and more specific as you move from the root to the leaf. {% if examples|length > 0 %} {% for example in examples %} ### EXAMPLE {{ loop.index }} ### ### ARTICLE ### Title: {{ example[’title’] }} {{ example[’abstract’] }} ### END ARTICLE ### {% for path in example[’paths’] %} {{ path | join(\" -> \") }} {% endfor %} ### END EXAMPLE {{ loop.index }} ### {% endfor %} {% else %} You must answer in the format of: Main topic classifications -> Broad topic 1 -> Subtopic 1 -> ... -> Most specific topic 1 Main topic classifications -> Borad topic 2 -> Subtopic 2 -> ... -> Most specific topic 2 ... {% endif %} ### ARTICLE ### Title: {{ title }} {{ abstract }} ### END ARTICLE ### Provide a category hierarchy for the above article. \\ {% if examples|length > 0 %} Use the same format as the examples above. {% else %} Use the format described above. {% endif %} Figure 6: Prompt template used for the Zero/One/Three-shot baselines. A.1.6 Hyperparameters The raw generated outputs of all methods are post-processed with the same scheme as described in Section 3.2. The best hyperparameters for the post-processing step found by grid search on the validation are reported in Table 2. 17Table 2: Values of the best hyperparameters found by grid search. r is the rank of the low-rank smoothing, only applicable to Hearst and REBEL. α = β = 0means no edges are pruned from the raw output apart from self-loop and inverse edge removal. Dataset Method α β r Wikipedia Memorisation 0 0.058489 - Hearst 0.786685 0 5 REBEL 0.872544 0 20 Zero-shot 0.976781 0.298107 - One-shot 0.990906 0.346684 - Three-shot 0.991955 0.530957 - Finetune 0.883848 0.058489 - OLLM 0.974330 0.025893 - arXiv Memorisation 0.340246 0 - Hearst 0.595878 0 150 REBEL 0.836685 0 100 Zero-shot 0.999896 0.346684 - One-shot 0.999611 0.401187 - Three-shot 0.999851 0.298107 - Finetune (transfer) 0.988129 0.346684 - OLLM (transfer) 0.983681 0.123872 - A.2 Ablations In this section, we present the results of our ablations regarding output consistency, the benefits of more advanced prompting techniques, and a comparison against LLMs4OL [4]. A.2.1 Consistency A common assumption of taxonomic relations is its transitivity and anti-symmetry. One limitation of many OL methods, including OLLM, is that they do not guarantee that the generated ontology is cycle- free, leading to inconsistent taxonomic relations. To achieve consistency, generic post-processing techniques [43] can be applied to remove such cycles. We analysed the ontologies generated by OLLM and found only 97 simple cycles in Wikipedia and none in arXiv. Using the greedy algorithm of repeatedly removing the edge that breaks the most simple cycles (a heuristic to the smallest set of edges whose removal makes the graph acyclic), we prune all such cycles and make the ontology consistent by removing just 26 of 10414 edges in Wikipedia. This is surprising considering we did not explicitly optimise our model to satisfy consistency. A.2.2 Chain of thought prompting More sophisticated prompting techniques, such as chain-of-thought (CoT) [ 48] have been shown to bring significant improvements in LLM inference. We explore whether we can establish strong baselines here by employing CoT in our prompting methods. We extend the zero-shot prompting method such that prediction now involves two rounds of inference: In the first round, we ask the model to describe the possible relevant concepts for the input document and to explain its reasoning. Then, we ask the model to predict the subgraph in the specified format given the additional, self-generated context. The prompts used are shown below: We tested the CoT method on Wikipedia and found no significant difference from basic zero-shot prompting, as shown in Table 3. We attribute this to the fact that CoT prompting primarily aims to improve logic and reasoning. We hypothesise that the performance in OL is more dependent on the model’s understanding of natural language than its ability to perform multi-step reasoning, hence we do not observe any significant improvement from CoT. 18The following is an article’s title and abstract. Briefly break down the topics ( both specific and general concepts) relevant to this article. Explain your reasoning step by step. ### ARTICLE ### Title: {{ title }} {{ abstract }} ### END ARTICLE ### Figure 7: Chain-of-thought first prompt Your task now is to assign this article to a suitable category hierarchy. A category is typically represented by a word or a short phrase, representing broader topics/concepts that the article is about. A category hierarchy is represented by a collection of paths from the generic root category \"Main topic classifications\" to a specific category suitable for the article. The topic titles should become more and more specific as you move from the root to the leaf. You must answer in the format of: Main topic classifications -> Broad topic 1 -> Subtopic 1 -> ... -> Most specific topic 1 Main topic classifications -> Broad topic 2 -> Subtopic 2 -> ... -> Most specific topic 2 ... Figure 8: Chain-of-thought second prompt A.2.3 Comparison against LLMs4OL In this ablation, we evaluate whether the improvement by OLLM is due to the improved methodology (end-to-end modelling) or simply due to the use of LLMs. One way to construct ontologies with LLMs proposed by LLMs4OL is to first prompt LLMs for possible concepts in a document, then link prediction by prompting for a yes/no response. Unfortunately, constructing a baseline from such two subtasks is non-trivial. We encountered significant scalability issues in the link prediction stage as it required O(n2) inferences. We make two modifications to overcome such limitation: 1. After the concept discovery stage, we only discard all but the n most frequent concepts to limit the number of inferences required during link prediction, where n is the number of concepts in the ground truth. 2. Instead of using zero-shot Mistral 7B as the link predictor, we use a finetuned BERT as the link predictor as it runs much faster. Given that LLMs4OL demonstrated that finetuned models perform much better than zero-shot inference on link prediction, we expect the finetuned BERT to be at least as good, if not better, than zero-shot Mistral 7B on this subtask. We design this ablation such that it is comparable to zero-shot end-to-end modelling: both use zero-shot Mistral 7B as the backbone, just utilised in different ways. We tested this method on Wikipedia and found that it is worse than zero-shot end-to-end modelling on all metrics except Motif Distance, as shown in Table 4. This is evidence that our end-to-end modelling approach is a clear improvement over traditional subtask-based OL. Not only does LLMs4OL suffer from significant scalability bottlenecks thus unlikely to be scalable to solve large problems, its performance is also worse. The results suggest that we can more effectively and efficiently leverage the capabilities of LLMs beyond just solving subtasks, such as by predicting subgraphs. 19Table 3: Comparison of Zero-shot with and without chain-of-thought prompting. There is no significant difference in performance. Dataset Method Literal F1 ↑ Fuzzy F1 ↑ Cont. F1 ↑ Graph F1 ↑ Motif Dist. ↓ Wikipedia Zero-shot 0.007 0.871 0.455 0.639 0.341 Zero-shot CoT 0.007 0.873 0.449 0.635 0.357 Table 4: Comparison of Zero-shot end-to-end modelling and LLMs4OL-style modelling with zero- shot concept discovery and fine-tuned BERT link prediction. LLMs4OL generally performs worse than zero-shot. Dataset Method Literal F1 ↑ Fuzzy F1 ↑ Cont. F1 ↑ Graph F1 ↑ Motif Dist. ↓ Wikipedia Zero-shot 0.007 0.871 0.455 0.639 0.341 LLMs4OL 0.003 0.841 0.428 0.482 0.092 A.3 Visualising evaluation metrics A.3.1 Visualisation of node matching in Graph F1 Neurons and Cognition Neuroscience General EconomicsEconometrics Mathematics Mathematical Physics Quantum Algebra Algebraic Geometry Dynamical Systems Representation Theory Diﬀerential Geometry Combinatorics History and Overview K-Theory and Homology Probability Number Theory Logic Algebraic T opology Symplectic Geometry Spectral Theory Operator Algebras Statistics Theory Metric Geometry Optimization and Control General T opology Commutative Algebra Information Theory Complex Variables Numerical Analysis Category Theory Classical Analysis and ODEs Group Theory Rings and Algebras General Mathematics Geometric T opology Analysis of PDEs Functional Analysis Electrical Engineering and Systems Science Computational Physics Quantum Physics Mathematical Systems Theory Information Theory Machine Learning Information Systems Mathematics Mathematical Logic and Foundations Mathematical Logic Signal Processing Data Analysis Mathematical Systems and Control Analysis Information Systems Theory Mathematical Physics Physics Diﬀerential Equations Condensed Matter Mathematics and Society Partial Diﬀerential Equations Time Series Analysis Data Mining Statistical Inference Statistical Mechanics Molecular Networks Machine Learning Main topic classiﬁcations Quantitative Biology Quantitative Finance Statistics Main topic classiﬁcations Genomics Tissues and Organs Cell Behavior Quantitative Methods Subcellular Processes Biomolecules Populations and Evolution Other Quantitative Biology Health Computational Finance Risk Management T rading and Market Microstructure Statistical Finance Pricing of Securities Portfolio Management General FinanceMathematical Finance Artiﬁcial Intelligence Applications Other Statistics Computation Methodology Information Economics Life Sciences Biological Data Science Biological Data Biological Evolution Economics and Finance Information Retrieval Information T echnology Figure 9: Highest scoring node matching from the Graph F1 metric between the ontology generated by OLLM (teal) and the ground truth ontology (black). The matching between nodes is shown in red, where the opacity of the edge indicates the similarity score (weaker links are more transparent). Visually, the matching defines a clear alignment of the two graphs: from the centre to the left we have the Mathematics-related concepts; at the top right we have Biology-related concepts; and at the bottom right we have Economics-related concepts. 20A.3.2 Visualisation of edge matching in Continuous F1 Neurons and Cognition General Economics Mathematics Commutative Algebra Algebraic Geometry Analysis of PDEs Algebraic T opology Classical Analysis and ODEs Combinatorics Category Theory Complex Variables Diﬀerential Geometry Dynamical Systems Functional Analysis General Mathematics General T opology Group Theory Geometric T opology History and Overview Information Theory K-Theory and Homology Logic Metric Geometry Mathematical Physics Numerical Analysis Number Theory Operator Algebras Optimization and Control Probability Quantum Algebra Rings and Algebras Representation Theory Symplectic Geometry Spectral Theory Statistics Theory Time Series Analysis Diﬀerential Equations Machine Learning Information Systems Theory Artiﬁcial Intelligence Partial Diﬀerential Equations Mathematical Systems Theory Signal Processing Mathematics and Society Condensed Matter Electrical Engineering and Systems Science Mathematical Logic Analysis Mathematical Physics Computational Physics Information Theory Mathematical Systems and Control Physics Quantum Physics Information Information Systems Molecular Networks Machine Learning Main topic classiﬁcations Quantitative Biology Quantitative Finance Statistics Mathematics Life Sciences Economics Data Analysis Computational Finance Genomics Tissues and Organs Risk Management Cell Behavior Quantitative Methods T rading and Market Microstructure Biomolecules Other Quantitative Biology Populations and Evolution Subcellular Processes Biological Data Science Neuroscience Biological Data Biological Evolution Statistical Finance General Finance Mathematical Finance Portfolio Management Pricing of Securities Statistical Mechanics Econometrics Economics and Finance Mathematical Logic and Foundations Applications Other Statistics Computation Methodology Data Mining Information T echnology Information Retrieval Health Statistical Inference Main topic classiﬁcations Figure 10: Highest scoring edge matching from the Continuous F1 metric between the ontology generated by OLLM (teal) and the ground truth ontology (black). The matching between edges is shown in red, where the opacity of the edge indicates the similarity score (weaker links are more transparent). Visually, the matching defines a clear alignment of the two graphs: in the bottom left and centre we have the Mathematics-related concepts; at the right we have Biology-related concepts; and at the top left we have Economics-related concepts. A.4 Visualisation of generated ontologies A.4.1 Wikipedia We include some generated outputs for Wikipedia here. Since the full generated output is too large to visualise, we plot subgraphs of the output instead. We sample the subgraphs by the following method: 1. Pick a random node in the generated graph. 2. Get the induced subgraph by the 1-hop neighbourhood of the chosen node. 3. Include the shortest path from the root “Main topic classifications” to the chosen node if such path exists. 4. Repeat from step 1 if the subgraph has more than 30 nodes or less than 5 nodes. We apply the filtering step (step 4) as subgraphs with too many nodes are difficult to inspect manually, and those with too few are uninformative. For Hearst, we choose the filtering upper bound to be 50 nodes as we fail to find subgraphs smaller than 30 nodes quickly. We additionally colour each edge black if it occurs literally in the training graph, blue if it occurs literally in the test graph, and red otherwise. 21Biology Biology organizations Biological evolution Biographies of biologists Biological research Biotechnology Biology organisations Biological research stubs Biologists Biology lists Biological classiﬁcation Biology stubs Biological concepts Biology-related lists Main topic classiﬁcations Life (a) Biology Main topic classiﬁcations Language Language reform Language law Language teaching Language advocacy Language policy Language rights Language advocacy organizations Language legislation Language laws Language planning Language policy by country Language policy organizations Language standardization Language policy stubs Multilingual education (b) Language policy Main topic classiﬁcationsMathematicsMathematical structures Mathematical objects T opological spacesAlgebraic structures Groups (mathematics) Algebras Ordered sets Mathematical categories (c) Mathematical structures Figure 11: Sub-ontologies for Wikipedia generated by OLLM, centred on various topics. 22Main topic classiﬁcations Energy Energy economics Energy accidents and incidents Energy policy Energy consumption Energy conservation Alternative energy economy Renewable energy economy Energy economists Energy companies Energy policy organizations Energy crises Fuel economy Carbon ﬁnance (a) Energy economics Main topic classiﬁcations Politics Political activism Internet activism Internet activism stubs Internet activists Internet activism organizations (b) Internet activism Mathematical concepts Proof theory Mathematical proofs Theorems Mathematical theorems Theorem proofs Concepts Theories Philosophical theories Theories of history Scientiﬁc theories Theories of everything Social theories Mathematical theories Real numbers Linguistic theories and hypotheses Main topic classiﬁcations (c) Theories Figure 12: Sub-ontologies for Wikipedia generated by Finetune, centred on various topics. 23Historical objects Archaeological artifacts Culture Artiﬁcial objects Exonumia Genetically modiﬁed organisms Satellites Works of art Buildings and structures Steles Furniture Synthetic biologyFlags Coins Ceremonial objects Machines Goods (economics) Vehicles Main topic classiﬁcations (a) Aritificial objects Main topic classiﬁcationsHuman behaviorDeceptionFraud Conﬁdence tricks Fake news Internet fraud (b) Fraud Main topic classiﬁcationsReligion Nature and religion Animism Nature spirits Fire in religion Nature deities Water and religion Animals in religion Light and religion Sanamahism Sacred natural sites (c) Nature and religion Figure 13: Sub-ontologies for Wikipedia generated by Memorisation, centred on various topics. 24Animals Social sciences Sources Explorers Languages Places Materials Leisure Information Suicides Entities World Health Organization Nurses Human rights organizations Weapons Earth sciences Action games Wikis Women Republicans Skeptics Millennium Development Goals Concepts Social networks Venues Plants Drugs Government (a) Drugs Animals Social sciences Sources Explorers Languages Places Materials Leisure Information Suicides Entities World Health Organization Nurses Human rights organizations Weapons Earth sciences Action games Wikis Women Republicans Skeptics Millennium Development Goals Concepts Social networks Venues Plants Government (b) Government People Explorers Social sciences Sources Languages Places Materials Leisure Animals Information Entities Suicides World Health Organization Nurses Drugs Human rights organizations Weapons Earth sciences Action games Government Wikis Women Republicans Skeptics Millennium Development Goals Concepts Social networks VenuesPlants Society Dentists Outer space Religious studies Women's health Games Construction Professional wrestling Scientists Muslims Voyager program Chinese people Companies Government of Vietnam Animatronics Mathematicians Street theatre Polish people American people Linguistics (c) Society Figure 14: Sub-ontologies for Wikipedia generated by Hearst, centred on various topics. 25Elections Geometry Experimental mathematics Nauruan people Computational mathematics Irish people Combinatorics Vietnamese people British people Circassian peopleEngineers Without Borders Algebra Portuguese people Order theory Chinese people Game theory KDE Japanese people Azerbaijani people Elementary mathematics Recreational mathematics Polish people Medical software Lithuanian people Arithmetic Mathematical software Sustainable Development Goals Main topic classiﬁcations (a) Elections Social sciences Philosophy of culture Geometry Experimental mathematics Pedagogy Computational mathematics Combinatorics Algebra Natural language processing Order theory Speech recognitionPhilosophy of mind Game theory Computer engineering Money Recreational mathematics Elementary mathematics Medical software Arithmetic Mathematical software Secularism Main topic classiﬁcations (b) Money Main topic classiﬁcations Vocal music Geometry Life Elementary mathematics Recreational mathematics Experimental mathematics Game theory Algebra Medical software Computational mathematics Combinatorics Arithmetic Mathematical software Marketing Order theory Engineering (c) V ocal music Figure 15: Sub-ontologies for Wikipedia generated by REBEL, centred on various topics. 26Main topic classiﬁcations Social Issues Criminal Justice SystemCorrections Prisons Capital Punishment Law Enforcement Agencies T rials Prisons and Prisoners Police Misconduct Corrections and RehabilitationPrisons and Penitentiaries Sentencing T rials and Sentencing Law Enforcement Prisons and Corrections (a) Criminal Justice System Main topic classiﬁcations People Government and Politics Athletes Denmark Danish Politics Copenhagen Danish Government Danish Football University of Copenhagen Danish History Regions Political History Greenland (b) Denmark Main topic classiﬁcations Computer Science Natural Language Processing Machine Learning Supervised Learning Pattern Recognition Deep Learning Neural Networks (c) Machine Learning Figure 16: Sub-ontologies for Wikipedia generated by Zero-shot, centred on various topics. 27Competitions World Championships Events Main topic classiﬁcations Sports Running Football Athletics Long-distance running T rack and ﬁeld Countries T rack and Field (a) Athletics Criminal law International law Main topic classiﬁcations Law Administrative law Legal studies Comparative law Nationality law JurisprudenceLegal history Constitutional law (b) Legal studies Main topic classiﬁcations Science Biology Physiology Muscle physiology Neurophysiology Cardiovascular system Human physiology Metabolism Respiratory system (c) Physiology Figure 17: Sub-ontologies for Wikipedia generated by One-shot, centred on various topics. 28Main topic classiﬁcations T echnology Space technology Aerospace technology Satellites Space exploration Spacecraft Rocket technology (a) Aerospace technology Main topic classiﬁcations T echnology Artiﬁcial intelligence and machine learning Natural language processing Speech recognition Neural networks Knowledge representation and reasoningComputer vision Knowledge representation Machine learning (b) Artificial intelligence and machine learning Elections VotingVoting systems United States elections Campaigns India General elections State elections Australia Local elections France Political ﬁgures Opinion polling Electoral commissions Voter turnout By-elections Voting methods Electoral systems Election administration Parliamentary elections Presidential elections Provincial elections History United States Main topic classiﬁcations (c) Elections Figure 18: Sub-ontologies for Wikipedia generated by Three-shot, centred on various topics. 29A.4.2 arXiv Mathematics Mathematical PhysicsQuantum Algebra Algebraic Geometry Dynamical Systems Representation Theory Diﬀerential Geometry Combinatorics History and Overview K-Theory and Homology Probability Number Theory LogicAlgebraic T opology Symplectic Geometry Spectral Theory Operator Algebras Statistics Theory Metric Geometry Optimization and Control General T opology Commutative Algebra Information Theory Complex Variables Numerical AnalysisCategory Theory Classical Analysis and ODEs Group Theory Rings and Algebras General Mathematics Geometric T opology Analysis of PDEs Functional Analysis Main topic classiﬁcations Quantitative Biology Quantitative Finance Statistics Neurons and Cognition Molecular Networks Genomics Tissues and Organs Cell Behavior Quantitative Methods Subcellular Processes Biomolecules Populations and Evolution Other Quantitative Biology General Economics Computational Finance Risk Management T rading and Market Microstructure Statistical Finance Pricing of Securities Portfolio Management General Finance Mathematical Finance Machine Learning Applications Other Statistics Computation Methodology Figure 19: Ground truth test split ontology for arXiv 30Information Systems Information Systems Theory Data Analysis Statistical Inference Data Mining Time Series Analysis Physics Condensed Matter Computational Physics Quantum Physics Statistical Mechanics Electrical Engineering and Systems Science Signal Processing Mathematics Mathematical Systems Theory Mathematical Physics Mathematical Logic and Foundations Mathematics and Society Mathematical Systems and Control Mathematical Logic Artiﬁcial Intelligence Machine Learning Economics Econometrics Economics and Finance Life Sciences Neuroscience Biological Data Biological Evolution Biological Data Science Information Information Retrieval Information T echnology Main topic classiﬁcations Health Analysis Information Theory Diﬀerential Equations Partial Diﬀerential Equations Figure 20: Ontology for arXiv generated by OLLM Main topic classiﬁcations Physics Economy Humanities Artiﬁcial Intelligence Life Sciences Methods in Physics Nonlinear Science Quantum Physics Statistical Mechanics Biological Physics Condensed Matter Computational Physics Fluid Dynamics Optics Linguistics Anthropology Medical Humanities Politics Biological Sciences Biology EconomicsEconomics and Finance InformationInformation Theory Figure 21: Ontology for arXiv generated by Finetune 31Physics Mathematical Physics General Relativity and Quantum Cosmology Astrophysics High Energy Physics - Theory High Energy Physics - Phenomenology Quantum Physics Condensed Matter Nuclear Theory Instrumentation and Detectors High Energy Physics - Lattice Atomic Physics Optics High Energy Physics - ExperimentFluid DynamicsNonlinear Sciences Computational Physics Plasma Physics Physics and Society Nuclear Experiment Applied Physics Biological Physics Chemical Physics Instrumentation and Methods for Astrophysics Cosmology and Nongalactic Astrophysics High Energy Astrophysical Phenomena Astrophysics of Galaxies Solar and Stellar Astrophysics Earth and Planetary Astrophysics Mesoscale and Nanoscale Physics Statistical Mechanics Quantum Gases Strongly Correlated Electrons Superconductivity Disordered Systems and Neural Networks Materials Science Soft Condensed Matter Main topic classiﬁcations Electrical Engineering and Systems Science Economics Image and Video Processing Signal Processing Systems and Control Audio and Speech Processing Figure 22: Ontology for arXiv generated by Memorisation 32Cell BehaviorAlgebraic Geometry Tissues and Organs Algebraic T opology Classical Analysis and ODEs General Mathematics Statistics Theory Symplectic Geometry Quantitative Methods Neurons and Cognition K-Theory and Homology Mathematics Geometric T opology Operator AlgebrasRisk Management Pricing of Securities Statistics Metric Geometry Biomolecules Molecular Networks Mathematical Finance Computation Dynamical Systems Group Theory Commutative Algebra General Economics Analysis of PDEs Diﬀerential Geometry Rings and Algebras General T opology Mathematical Physics T rading and Market Microstructure Number Theory Computational Finance Quantum Algebra Subcellular Processes Quantitative Finance Statistical Finance Category Theory Representation Theory Probability Spectral Theory Logic Complex Variables Combinatorics Populations and Evolution Main topic classiﬁcations Quantitative Biology Portfolio Management Other Statistics Information Theory Numerical Analysis General Finance Other Quantitative Biology Optimization and Control Functional Analysis Genomics History and Overview Applications Machine Learning Methodology Figure 23: Ontology for arXiv generated by Hearst 33General Finance Algebraic T opology Quantitative Finance Genomics Operator Algebras Analysis of PDEs Rings and Algebras Number Theory Representation Theory Mathematical Finance Symplectic Geometry Quantitative Methods Probability Statistical Finance Populations and Evolution Geometric T opology Classical Analysis and ODEs Pricing of Securities Methodology Numerical Analysis T rading and Market Microstructure Cell Behavior Metric Geometry Tissues and Organs History and Overview Information Theory Optimization and Control Other Quantitative Biology Mathematics Quantitative Biology Category TheoryStatistics Theory Portfolio Management Subcellular Processes General Mathematics Other Statistics Diﬀerential GeometryK-Theory and Homology Biomolecules Computation Machine Learning Algebraic Geometry Statistics Combinatorics Molecular Networks Spectral Theory Complex Variables General Economics Risk Management Mathematical Physics Dynamical Systems Neurons and Cognition Group Theory Logic Quantum Algebra Commutative Algebra Computational Finance General T opology Functional Analysis Main topic classiﬁcations Applications Figure 24: Ontology for arXiv generated by REBEL Machine Learning Deep Learning Neural Networks Main topic classiﬁcations Physics Health and Medicine Communications T echnology Computer Science Machine Learning and Artiﬁcial Intelligence Artiﬁcial Intelligence and Machine Learning Machine Learning and AI Mathematics Artiﬁcial Intelligence Applied Mathematics Optimization Theoretical Mathematics Probability Theory and Statistics Analysis Functional Analysis Figure 25: Ontology for arXiv generated by Zero-shot 34Machine Learning Deep Learning Neural Networks Main topic classiﬁcations Physics Computer Science Mathematics Science Mathematical Physics Theoretical Physics Artiﬁcial Intelligence and Machine Learning Machine Learning and AI Artiﬁcial Intelligence Applied Mathematics Functional Analysis Optimization Mathematical Analysis Probability Theory and Statistics Analysis EngineeringElectrical Engineering Figure 26: Ontology for arXiv generated by One-shot Machine Learning Deep Learning Main topic classiﬁcations Physics Electrical Engineering and Systems Science Computer Science Machine Learning and Artiﬁcial Intelligence Artiﬁcial Intelligence and Machine LearningEngineering Mathematics Engineering and T echnology Machine Learning and AI Artiﬁcial Intelligence Applied Mathematics Mathematical Physics Probability and Statistics Analysis Figure 27: Ontology for arXiv generated by Three-shot 35NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: The main claims of this paper are: 1. OLLM is an effective method for building ontologies from scratch, and 2. the evaluation metrics we introduce are robust and useful for gold standard evaluation of ontologies. We justify the first claim by demonstrating that OLLM outperforms our baseline methods on Wikipedia and arXiv according to our metrics. We justify the second claim by showing that an existing metric (Literal F1) can score the sub-optimal Memorisation solution highly, while our metrics are not subject to this issue. Our new metrics also suggest results that align with our qualitative analysis. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations and the possible resolutions in Section 6. This includes the scope of the work, where we only study ontologies with concepts and taxonomic relations, the inability to guarantee taxonomic relation transitivity, and the inability to control for data leakage from the pretraining stage of the LLM base model. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best 36judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not include theoretical results. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe the data collection procedure in Section 4.1 and the full experiment details in Appendix A.1. We also include the code and dataset in the supplementary material. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 37(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code and data used for this project are provided in the supplementary material. The code includes a README which details the steps for reproducing our results. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We give all the experimental details in Appendix A.1. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We did not perform repeated experiments due to compute constraints, therefore there are no error bars. Guidelines: 38• The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We describe the compute requirements for each experiment in Appendix A.1. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper does not involve human subjects and does not use data that is not already in the public domain. We clearly describe our data collection procedure in Section 4.1. Our method is highly specialised in building ontologies and solving related tasks, thus having minimal societal impact or any potentially harmful consequences. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? 39Answer: [NA] Justification: Our method is highly specialised in building ontologies and solving related tasks only. We do not expect our work to have a wider impact than improving the quality of existing or new ontologies. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. The trained model is specific to building ontologies only. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The two datasets collected in this project use data from Wikipedia and arXiv, which are in the public domain under the CC BY-SA 4.0 and CC0 1.0 Deed licenses respectively. The REBEL-large model is available under CC BY-NC-SA 4.0 license. 40Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We release the dataset and code for this paper. The data collection procedure is clearly described in Section 4.1. The training procedure is clearly described in Appendix A.1. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? 41Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 42",
      "references": [
        "A semantic web primer.",
        "Gene ontology: tool for the unification of biology.",
        "A survey of ontology learning techniques and applications.",
        "Llms4ol: Large language models for ontology learning.",
        "A survey of ontology evaluation tech- niques.",
        "Ontology learning from text: methods, evaluation and applications, volume 123.",
        "Rebel: Relation extraction by end-to-end language generation.",
        "Text2onto: A framework for ontology learning and data-driven change discovery.",
        "Supervised learning of universal sentence representations from natural language inference data.",
        "Bert: Pre-training of deep bidirectional transformers for language understanding.",
        "Long-term recurrent convolutional networks for visual recognition and description.",
        "Similarity for ontologies - a comprehensive framework.",
        "Towards ontology construction with language models.",
        "Graphgen: A scalable approach to domain-agnostic labeled graph generation.",
        "A translation approach to portable ontology specifications.",
        "Toward principles for the design of ontologies used for knowledge sharing?",
        "A survey of ontology learning approaches.",
        "Automated discovery of wordnet relations.",
        "The curious case of neural text degeneration.",
        "Lora: Low-rank adaptation of large language models.",
        "Mistral 7b.",
        "Ontology discovery from web pages: Application to tourism.",
        "Taxaminer: an experimentation framework for automated taxonomy bootstrapping.",
        "Automatic relationship extraction from agricultural text for ontology construction.",
        "Adam: A method for stochastic optimization.",
        "Semi-supervised classification with graph convolutional networks.",
        "The hungarian method for the assignment problem.",
        "Efficient memory management for large language model serving with pagedattention.",
        "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
        "Measuring similarity between ontologies.",
        "The stanford corenlp natural language processing toolkit.",
        "Linear assignment problems.",
        "Wordnet: a lexical database for english.",
        "Network motifs: simple building blocks of complex networks.",
        "Deriving a large scale taxonomy from wikipedia.",
        "A task-based approach for ontology evaluation.",
        "A survey on ontology evaluation methods.",
        "Sentence-bert: Sentence embeddings using siamese bert- networks.",
        "Hearst patterns revisited: Automatic hypernym detection from large text corpora.",
        "Schema.org.",
        "Network motifs in the transcrip- tional regulation network of escherichia coli.",
        "Exploiting wikipedia for cross-lingual and multilingual information retrieval.",
        "Breaking cycles in noisy hierarchies.",
        "Enhancing knowledge graph con- struction using large language models.",
        "Graph-based approach to auto- matic taxonomy generation (grabtax).",
        "Using wikipedia categories and links in entity ranking.",
        "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.",
        "Chain-of-thought prompting elicits reasoning in large language models.",
        "Simplifying graph convolutional networks.",
        "Graphrnn: Generat- ing realistic graphs with deep auto-regressive models.",
        "Gold standard evaluation of ontology learning methods through ontology transformation and alignment.",
        "Analysis of the wikipedia category graph for nlp applications."
      ],
      "meta_data": {
        "arxiv_id": "2410.23584v1",
        "authors": [
          "Andy Lo",
          "Albert Q. Jiang",
          "Wenda Li",
          "Mateja Jamnik"
        ],
        "published_date": "2024-10-31T02:52:39Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses end-to-end ontology learning (taxonomic backbone: concepts + is-a edges) from a document corpus, arguing subtask pipelines miss interactions and scale poorly. Introduces OLLM, which finetunes an LLM to generate per-document ontology subgraphs (relevant paths from a root to annotated concepts), then aggregates and prunes them into a global ontology. Proposes a frequency-aware masked-loss regulariser to reduce overfitting to high-frequency/high-level concepts and improve generalisation to unseen top-level branches. Releases two datasets (large-scale Wikipedia categories; smaller arXiv taxonomy) with train/val/test splits designed to reduce leakage by splitting the ontology graph first. Introduces a suite of semantic/structural ontology similarity metrics for arbitrary labelled graphs (Fuzzy F1, Continuous F1 via Hungarian matching, Graph F1 via GCN embeddings, Motif distance) that are more robust than literal string matching. Shows OLLM outperforms relation-extraction/subtask-composition baselines and can be adapted to a new domain (arXiv) with few training examples.",
        "methodology": "Formulates ontology as a rooted labelled directed graph; goal is to reconstruct the ground-truth taxonomy given documents and document-to-concept annotations (for training). For each document, constructs a relevant subgraph: all paths of length ≤N from root to any annotated concept; induced nodes/edges form training target. Linearises subgraph as a list of root-to-leaf paths (path-based sequence) and finetunes an autoregressive LLM (Mistral 7B) with LoRA. Introduces masked-loss regulariser: if relation u→v appears n times in training, tokens for v are masked as targets with probability max(1−M/n,0) so frequent relations contribute ~M times, reducing overfitting to high-level edges. Inference: generate subgraph per document (temperature 0.1, top-p 0.9), union edges across documents into weighted graph by occurrence counts, then prune via (1) self-loop removal, (2) inverse-edge resolution by higher weight, (3) global quantile threshold α, (4) per-node cumulative-weight threshold β (top-p style), (5) remove isolated nodes. Hyperparameters α,β tuned on validation by maximising Continuous F1. Evaluation metrics: Literal F1; Fuzzy F1 using SBERT cosine threshold; Continuous F1 computes maximum-weight edge matching using Hungarian algorithm on edge similarities min(NodeSim endpoints); Graph F1 computes graph-aware node embeddings via 2-layer simple graph convolution on text embeddings then Hungarian node matching; Motif distance is TV distance over 3-node motif distributions.",
        "experimental_setup": "Datasets: (1) Wikipedia categories: BFS from root \"Main topic classifications\" to depth 3; for each category, collect up to 5000 pages with title + lead summary via Wikipedia API. Result: 13,886 concepts, 28,375 taxonomic relations, 362,067 documents. (2) arXiv taxonomy: taxonomy from arXiv; corpus is title+abstract of papers from 2020–2022 with ≥10 citations (citations via Semantic Scholar API). Result: 161 concepts, 166 relations, 126,001 documents. Splits: reduce leakage by first partitioning the ontology graph using top-level children of the root (7:3:10 train:val:test at top-level), then take nodes within distance d−1 from each split’s top-level nodes to form split graphs; generate document–subgraph pairs afterward, yielding many unseen concepts in test. Subgraph depth N chosen as smallest such that >99% edges appear in at least one relevant subgraph (N=4 Wikipedia, N=3 arXiv). Models/baselines: OLLM (Mistral 7B v0.2 + LoRA r=32 α=16, 2 epochs, Adam lr 1e-5; masked loss); Finetune baseline without masking. Transfer: further finetune on 2048 arXiv pairs with new LoRA (r=8 α=8), early stopping. Baselines: Memorisation of train graph; Hearst patterns (Roller et al.) with SPMI + low-rank smoothing; REBEL-large relation extraction filtered to taxonomic-like relations + smoothing; Prompting with Mistral 7B Instruct (0/1/3-shot). All outputs post-processed with same pruning; α,β selected by grid search. Evaluation: compare predicted vs ground-truth test graph using Literal/Fuzzy/Continuous/Graph F1 and Motif distance; qualitative inspection of sampled subgraphs. Compute: Wikipedia training ~12 A100-hours; inference ~7 A100-hours; prompting baselines ~50 A100-hours.",
        "limitations": "Scope limited to simple taxonomies (concepts + taxonomic relations); does not handle axioms/constraints or rich relation types. Generated graphs may contain cycles, violating strict taxonomy properties (transitivity/antisymmetry); needs cycle-removal/cleanup post-processing (though few cycles observed). Reliance on document-to-concept annotations for training may limit applicability where such labels are unavailable or noisy. Evaluation assumes the ground-truth ontology is a proxy for quality; metrics may still be imperfect and some (Literal F1, Motif distance) can be misleading under split symmetries/overlap. Potential data contamination from unknown LLM pretraining corpus cannot be fully ruled out. No repeated runs/error bars; some baselines required approximations (smoothing, pre-filtering dense edges) for tractability.",
        "future_research_directions": "Extend OLLM beyond taxonomic backbones to multi-relation ontologies (add relation-type tags in linearisation; develop metrics for heterogeneous graphs) and potentially axioms/constraints via structured decoding or neuro-symbolic post-processing. Improve consistency guarantees (cycle-free DAG enforcement during decoding; differentiable constraints; principled hierarchy-cleaning). Reduce dependence on concept annotations by weak supervision, self-training, retrieval-augmented labeling, or joint concept discovery + structure learning. Explore more scalable/global aggregation and uncertainty-aware edge weighting, and incorporate evidence from multiple modalities (vision-language models for corpora with images). Strengthen evaluation with task-based/human studies, robustness analyses, and statistical significance across seeds; investigate learned graph similarity metrics or alignment methods. Study contamination detection and domain generalisation, including more diverse domains and low-resource settings.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding",
      "full_text": "Published as a conference paper at ICLR 2022 ONTO PROTEIN : P ROTEIN PRETRAINING WITH GENE ONTOLOGY EMBEDDING Ningyu Zhang1,2,3∗ Zhen Bi2,3∗ Xiaozhuan Liang2,3∗ Siyuan Cheng2,3∗ Haosen Hong4 Shumin Deng1,3 Qiang Zhang1,4 Jiazhang Lian4 Huajun Chen1,3,4† 1College of Computer Science and Technology, Zhejiang University 2School of Software Technology, Zhejiang University 3Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies 4Hangzhou Innovation Center, Zhejiang University {zhangningyu,bizhen zju,liangxiaozhuan,22151070}@zju.edu.cn {231sm,12028071,jzlian,qiang.zhang.cs,huajunsir}@zju.edu.cn ABSTRACT Self-supervised protein language models have proved their effectiveness in learn- ing the proteins representations. With the increasing computational power, cur- rent protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve re- markable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowl- edge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we proposeOntoProtein, the ﬁrst general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1. 1 I NTRODUCTION Protein science, the fundamental macromolecules governing biology and life itself, has led to re- markable advances in understanding the disease therapies and human health (Vig et al. (2021)). As a sequence of amino acids, protein can be viewed precisely as a language, indicating that they may be modeled using neural networks that have been developed for natural language processing (NLP). Recent self-supervised pre-trained protein language models (PLMs) such as ESM (Rao et al. (2021b)), ProteinBERT (Brandes et al. (2021)), ProtTrans (Elnaggar et al. (2020)) which can learn powerful protein representations, have achieved promising results in understanding the structure and functionality of the protein. Yet existing PLMs for protein representation learning generally cannot sufﬁciently capture the biology factual knowledge, which is crucial for many protein tasks but is usually sparse and has diverse and complex forms in sequence. By contrast, knowledge graphs (KGs) from gene ontology 2 contain extensive biology structural facts, and knowledge embedding (KE) approaches (Bordes et al. (2013), Zheng et al. (2021)) can efﬁciently embed them into continuous vectors of entities and relations. For example, as shown in Figure 1, without knowing PEX5 has speciﬁc biological processes and cellular components, it ∗Equal contribution and shared co-ﬁrst authorship. †Corresponding author. 1Code and datasets are available in https://github.com/zjunlp/OntoProtein. 2http://geneontology.org/ 1 arXiv:2201.11147v6  [q-bio.BM]  3 Jun 2022Published as a conference paper at ICLR 2022 Figure 1: Left: A protein example with biology knowledge (molecular function, biological pro- cess and cellular component): K+ (potassium ion) Cyclic nucleotide-gated cation channel protein. Right: The corresponding sub-graph regarding K+ carrier proteins in ProteinKG25. Yellownodes are protein sequences and blue nodes are GO (Gene Ontology) entities with biological descriptions. is challenging to recognize its interaction with other proteins. Furthermore, since protein’s shape determines its function , it is more convenient for models to identify protein’s functions with the prior knowledge of protein functions having similar shapes. Hence, considering rich knowledge can lead to better protein representation and beneﬁts various biology applications, e.g., protein contact prediction, protein function prediction, and protein-protein interaction prediction. However, differ- ent from knowledge-enhanced approaches in NLP (Zhang et al. (2019b), Wang et al. (2021b), Wang et al. (2021a)) , protein sequence and gene ontology are two different types of data. Note that protein sequence is composed of amino acids while gene ontology is a knowledge graph with text descrip- tion; thus, severe issues of structured knowledge encoding and heterogeneous information fusion remain. In this paper, we take the ﬁrst to propose protein pre-training with gene ontology embedding (OntoProtein), which is the ﬁrst general framework to integrate external knowledge graphs into protein pre-training. We propose a hybrid encoder to represent language text and protein sequence and introduce contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and the protein sequence embedding during pre-training. For the KE objective, we encode the node descriptions (go annotations) as their corresponding entity embeddings and then optimize them following vanilla KE approaches (Bordes et al. (2013)). We further leverage gene ontology of molecular function, cellular component, and biological process and introduce a knowledge-aware negative sampling method for the KE objective. For the MLM (Mask Language Modeling) objective, we follow the approach of existing protein pre-training approaches (Rao et al. (2021b)). OntoProtein has the following strengths: (1) OntoProtein inherits the strong ability of protein understanding from PLMs with the MLM ob- ject. (2) OntoProtein can integrate biology knowledge into protein representation with the supervi- sion from KG by the KE object. (3) OntoProtein constitutes a model-agnostic method and is readily pluggable into a wide range of protein tasks without additional inference overhead since we do not modify model architecture but add new training objectives. For pre-training and evaluating OntoProtein, we need a knowledge graph with large-scale biology knowledge facts aligned with protein sequences. Therefore, we constructProteinKG25, which con- tains about 612,483 entities, 4,990,097 triples, and aligned node descriptions from GO annotations. To the best of our knowledge, it is the ﬁrst large-scale KG dataset to facilitate protein pre-training. We deliver data splits for both the inductive and the transductive settings to promote future research. To summarize, our contribution is three-fold: (1) We propose OntoProtein, the ﬁrst knowledge- enhanced protein pre-training approach that brings promising improvements to a wide range of pro- tein tasks. (2) By contrastive learning with knowledge-aware sampling to jointly optimize knowl- edge and protein embedding, OntoProtein shows its effectiveness in widespread downstream tasks, including protein function prediction, protein-protein interaction prediction, contact prediction, and so on. (3) We construct and release the ProteinKG25, a novel large-scale KG dataset, promoting the 2Published as a conference paper at ICLR 2022 Figure 2: Overview of our proposed OntoProtein, which jointly optimize knowledge graph embed- ding and masked protein model (Best viewed in color.). research on protein language pre-training. (4) We conduct extensive experiments in widespread pro- tein tasks, including TAPE benchmark, protein-protein interaction prediction, and protein function prediction, which demonstrate the effectiveness of our proposed approach. 2 M ETHODOLOGIES We begin to introduce our approach of protein pre-training with ontology embedding (OntoProtein), as shown in Figure 2. OntoProtein incorporates external knowledge from Gene Ontology (Go) into language representations by jointly optimizing two objectives. We will ﬁrst introduce the hybrid encoder, masked protein modeling, and knowledge encoder, and then we will present the details of contrastive learning with knowledge-aware negative sampling. Finally, we will illustrate the overall pre-training objects. 2.1 H YBRID ENCODER We ﬁrst introduce the hybrid encoder to represent protein and GO knowledge. For the protein encoder, we use the pre-trained ProtBert from Elnaggar et al. (2020). ProtBert is pre-trained using the BERT architecture with UniRef100 datasets. Compared to BERT Devlin et al. (2019), ProtBert encodes amino acid sequences into token level or sentence level representations, which can be used for downstream protein tasks such as contacts prediction tasks. The encoder takes a protein sequence of Ntokens (x1,...,x N ) as inputs, and computes contextualized amnio acid representationHi Protein and sequence representationHProtein via mean pooling. To bridge the gap between text and protein, we utilize afﬁne transformation (an extra linear layer) to project those representation to the same space. We will discuss details of learning protein representation in Section Mask Protein Modeling. For the Go encoder, we leverage BERT (Devlin et al. (2019)), a Transformer (Vaswani et al. (2017)) based text encoder for biological descriptions in Gene Ontology entities. Speciﬁcally, we utilize the pre-trained language model from (Gu et al. (2020)) 3. The encoder takes a sequence of N tokens (x1,...,x N ) as inputs, and computes Go representations HGO ∈RN×d by averaging all the token embeddings. 3https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext 3Published as a conference paper at ICLR 2022 Since the relations in Gene Ontology are important for representing the knowledge of biology fea- tures, thus, we utilize a relation encoder with the random initialization, and those embeddings of relations will be optimized and updated during pre-training. 2.2 K NOWLEDGE EMBEDDING We leverage the knowledge embedding (KE) objective to obtain representations in the pre-training process since Gene Ontology is actually a factual knowledge graph. Similar to Bordes et al. (2013), we use distributed representations to encode entities and relations. The knowledge graph here con- sists of lots of triples to describe relational facts. We deﬁne a triplet as (h,r,t), where hand tare head and tail entities, ris the relation whose type usually is pre-deﬁned in the schema 4. Note that there are two different types of nodeseGO and eprotein in our knowledge graph. eGO is denoted as nodes that exist in the gene ontology, such as molecular function or cellular component nodes, and eGO can be described by annotation texts. eprotein is the protein node that links to the gene ontology, and we also represent eprotein with amnio acids sequences. Concretely, the triplets in this knowledge graph can be divided into two groups, tripleGO2GO and tripleProtein 2GO. To integrate multi-modal descriptions into the same semantic space and address the heterogeneous information fusion issue, we utilize hybrid encoders introduced in the previous Section. Note that protein en- coder and GO encoder represent protein sequence and GO annotations separately. 2.3 M ASKED PROTEIN MODELING We use masked protein modeling to optimize protein representations. The masked protein modeling is similar to masked language modeling (MLM). During model pre-training, we use a 15% proba- bility to mask each token (amino acid) and leverage a cross-entropy loss ℓMLM to estimate these masked tokens. We initialize our model with the pre-trained model of ProtBert and regard ℓMLM as one of the overall objectives of OntoProtein by jointly training KE (knowledge embedding) and MLM. Our approach is model-agnostic, and other pre-trained models can also be leveraged. 2.4 C ONTRASTIVE LEARNING WITH KNOWLEDGE -AWARE NEGATIVE SAMPLING Knowledge embedding (KE) is to learn low-dimensional representations for entities and relations, and contrastive estimation represents a scalable and effective method for inferring connectivity pat- terns. Note that a crucial aspect of contrastive learning approaches is the choice of corruption distri- bution that generates hard negative samples, which force the embedding model to learn discrimina- tive representations and ﬁnd critical characteristics of observed data. However, previous approaches either employ too simple corruption distributions, i.e., uniform, yielding easy uninformative neg- atives, or sophisticated adversarial distributions with challenging optimization schemes. Thus, in this paper, we propose contrastive learning with knowledge-aware negative sampling, an inexpen- sive negative sampling strategy that utilizes the rich GO knowledge to sample negative samples. Formally, the KE objective can be deﬁned as: ℓKE = −log σ(γ−d(h,t)) − n∑ i=1 1 nlog σ(d(h′ i,t′ i) −γ) (1) (h′ i,t′ i) is the negative sample, in which head or tail entities are random sampled to construct the corrupt triples. n is the number of negative samples, σ is the sigmoid function, and γ means the margin. dis the scoring function, and we use TransE (Bordes et al. (2013)) for simplicity, where dr(h,t) =∥h+ r−t∥ (2) Speciﬁcally, we deﬁne triple sets and entity sets as T and E, all triplets are divided into two groups. If the head entity is protein node and the tail entity is GO node, we denote the triple asTprotein−GO. Similarly, if head and tail entities are both GO nodes, we denote them asTGO−GO. As Gene Ontol- ogy describes the knowledge of the biological domain concerning three aspects, all entities in Gene Ontology belong to MFO (Molecular Function), CCO (Cellular Component), or BPO (Biological Process). 4The schema of the knowledge graph can be found in Appendix A.1 4Published as a conference paper at ICLR 2022 Figure 3: Top: Data Distribution of GO Terms. Bottom: Statistics of Protein-GO Term. To avoid plain negative samples, for those TGO−GO triples, we sample triples by replacing entities with the same aspect (MFO, CCO, BPO)5. Finally, we deﬁne the negative triple setsT′and positive triple as (h,r,t), the negative sampling process can be described as follows: T ′ GO−GO(h,r,t) = {(h′,r,t) |h′∈E′,h ∈E′}∪{(h,r,t′) |t′∈E′,t ∈E′} T ′ Protein −GO(h,r,t) = {(h,r,t′) |t′∈E′} (3) where E′∈{EMFO ,ECCO ,EBPO }, and we only replace the tail entities for TProtein −GO triples. 2.5 P RE-TRAINING OBJECTIVE We adopt the mask protein modeling object and knowledge embedding objective to construct the overall object of the OntoProtein. We jointly optimize the overall object as follows: ℓ= αℓKE + ℓMLM (4) where αis the hyper-parameter. Our approach can be embedded into existing ﬁne-tuning scenarios. 3 E XPERIMENT Extensive experiments have been conducted to prove the effectiveness of our approach. In the pre- training stage, we construct a new knowledge graph dataset that consists of Gene Ontology and public annotated proteins. Our proposed model is pre-trained with this dataset and evaluated in several downstream tasks. We evaluate OntoProtein in protein function prediction, protein-protein interaction and TAPE benchmark (Rao et al. (2019)). 3.1 D ATASETS Pre-training Dataset To incorporate Gene Ontology knowledge into language models, we build a new pre-training dataset called ProteinKG25 6, which is a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms7 and proteins entities. Gene Ontology 5For Tprotein−GO triples, it is also intuitive to replace the proteins with their homologous proteins to gen- erate hard negative triples, and we leave this for future works. 6https://zjunlp.github.io/project/ProteinKG25/ 7The structure of GO can be described in terms of a graph, where each GO term is a node, and the relation- ships between the terms are edges between the nodes. 5Published as a conference paper at ICLR 2022 Method Structure Evolutionary Engineering SS-Q3 SS-Q8 Contact Homology Fluorescene Stability LSTM 0.75 0.59 0.26 0.26 0.67 0.69 TAPE Transformer 0.73 0.59 0.25 0.21 0.68 0.73 ResNet 0.75 0.58 0.25 0.17 0.21 0.73 MSA Transformer - 0.73 0.49 - - - ProtBert 0.81 0.67 0.35 0.29 0.61 0.82 OntoProtein 0.82 0.68 0.40 0.24 0.66 0.75 Table 1: Results on TAPE Benchmark. SS is a secondary structure task that evaluates in CB513. In contact prediction, we test medium- and long-range using P@L/2 metrics. In protein engineering tasks, we test ﬂuorescence and stability prediction using spearman’sρmetric. consists of a set of GO terms (or concepts) with relations that operate between them, e.g., molecular function terms describe activities that occur at the molecular level. A GO annotation is a statement about the function of a particular gene or gene product, e.g., the gene product “cytochrome c” can be described by the molecular function oxidoreductase activity. Due to the connection between Gene Ontology and Gene Annotations, we combine the two structures into a uniﬁed knowledge graph. For each GO term in Gene Ontology, we align it to its corresponding name and description and concatenate them by a colon as an entire description. For each protein in Gene annotation, we align it to the Swiss-Prot 8, a protein knowledge database, and extract its corresponding sequence as its description. In ProteinKG25, there exists 4,990,097 triples, including 4,879,951 Tprotein−GO and 110,146 TGO−GO triples. Figure 3 illustrate the statistics of our ProteinKG25. Detailed construction procedure and analysis of pre-train datasets can be found in Appendix A.1. Downstream Task Dataset We use TAPE as the benchmark (Rao et al. (2019)) to evaluate protein representation learning. There are three types of tasks in TAPE, including structure, evolutionary, and engineering for proteins. Following Rao et al. (2021a), we select 6 representative datasets including secondary structure (SS), contact prediction to evaluate OntoProtein. Protein-protein interactions (PPI) are physical contacts of high speciﬁcity established between two or more protein molecules; we regard PPI as a sequence classiﬁcation task and use three datasets with different sizes for evaluation. STRING is built by Lv et al. (2021), which contains 15,335 proteins and 593,397 PPIs. We also use SHS27k and SHS148k, which are generated by Chen et al. (2019). Protein function prediction aims to assign biological or biochemical roles to proteins, and we also regard this task as a sequence classiﬁcation task. We build a new evaluation dataset based on our ProteinKG25 following the standard CAFA protocol (Zhou et al. (2019)). Speciﬁcally, we design two evaluation settings, the transductive setting and the inductive setting, which simulate two sce- narios of gene annotation in reality. In the transductive setting, the model can generate embeddings of unseen protein entities with entity descriptions. On the contrary, for the inductive setting, those entities have occurred in the pre-training stage. The detailed construction of the dataset can be found in Appendix A.1. As shown in Figure 3, proteins are, on average, annotated by 2 terms in CCO, 4 in MFO, and 3 in BPO, indicating that protein function prediction can be viewed as a multi-label problem. Notably, we notice that leaf GO terms tend to have more speciﬁc concepts than non-leaf GO terms. Meanwhile, there exists a challenging long-tail issue for the function prediction task. 3.2 R ESULTS TAPE B ENCHMARK Baselines In TAPE, we evaluate our OntoProtein compared with ﬁve baselines. The ﬁrst is the model with LSTM encoding of the input amino acid sequence, which provides a simple baseline. The second is TAPE Transformer that provides a basic transformer baseline. We further select ResNet from He et al. (2016) as a baseline. The forth is the MSA Transformer (Rao et al. (2021a)). 8https://www.uniprot.org/ 6Published as a conference paper at ICLR 2022 SHS27k SHS148k STRING Methods BFS DFS BFS DFS BFS DFS DPPI 41.43 46.12 52.12 52.03 56.68 66.82 DNN-PPI 48.90 54.34 57.40 58.42 53.05 64.94 PIPR 44.48 57.80 61.83 63.98 55.65 67.45 GNN-PPI 63.81 74.72 71.37 82.67 78.37 91.07 GNN-PPI (ProtBert) 70.94 73.36 70.32 78.86 67.61 87.44 GNN-PPI (OntoProtein)† 72.26 78.89 75.23 77.52 76.71 91.45 Table 2: Protein-Protein Interaction Prediction Results. Breath-First Search (BFS) and Depth-First Search (DFS) are strategies that split the training and testing PPI datasets. Transductive Inductive Method BPO MFO CCO BPO MFO CCO ProtBert 0.58 0.13 8.47 0.64 0.33 9.27 OntoProtein 0.62 0.13 8.46 0.66 0.25 8.37 Table 3: Protein Function Prediction Results on three sub-sets with two settings. BPO refers to Biological Process, MFO refers to Molecular Function, and CCO refers to Cellular Component. Note that MSA Transformer takes advantage of multiple sequence alignments (MSAs) and is the current state-of-the-art approach. Finally, we use ProtBert (Elnaggar et al. (2020)) with 30 layers of BERT encoder, which is the largest pre-trained model among baselines. Results We detail the experimental result on TAPE in Table 1. Concretely, we notice that On- toProtein yields better performance in all token level tests. For the second structure (SS-Q3 and SS-Q8) and contact prediction, OntoProtein outperforms TAPE Transformer and ProtBert, show- ing that it can beneﬁt from those informative biology knowledge graphs in pre-training. Moreover, OntoProtein can achieve comparable performance with MSA transformer. Note that our proposed OntoProtein does not leverage the information from MSAs. However, with external gene ontology knowledge injection, OntoProtein can obtain promising performance. In sequence level tasks, On- toProtein can achieve better performance than ProtBert in ﬂuorescence prediction. However, we observe that OntoProtein does not perform well in protein engineering, homology, and stability pre- diction, which are all regression tasks. We think this is due to the lack of sequence-level objectives in our pre-training object, and we leave this for future work. PROTEIN -PROTEIN INTERACTION Baselines We choose four representative methods as baselines for protein-protein interaction. PIPR (Chen et al. (2019)), DNN-PPI (Li et al. (2018)) and DPPI (Hashemifar et al. (2018)) are deep learning based methods. GNN-PPI (Lv et al. (2021)) is a graph neural network based method for better inter-novel-protein interaction prediction. To evaluate our OntoProtein, we replace the initial protein embedding part of GNN-PPI with ProtBERT and OntoProtein as baselines. Results From Table 2, we observe that the performance of OntoProtein is better than PIPR, which demonstrates that external structure knowledge can be beneﬁcial for protein-protein interaction pre- diction. We also notice th at our method can achieve promising improvement in smaller dataset SHS2K, even outperforming GNN-PPI and GNN-PPI (ProtBert). With a larger size of datasets, OntoProtein can still obtain comparable performance to GNN-PPI and GNN-PPI (ProtBert). PROTEIN FUNCTION PREDICTION Baselines For simplicity, we leverage Seq2Vec (Littmann et al. (2021)) as the backbone for fair comparison and initialize embeddings with ProtBert and our OntoProtein. Note that our approach is model-agnostic, and other backbones can also be leveraged. 7Published as a conference paper at ICLR 2022 6 ≤seq <12 12 ≤seq <24 24 ≤seq P@L P@L/2 P@L/5 P@L P@L/2 P@L/5 P@L P@L/2 P@L/5 TAPE Transformer 0.28 0.35 0.46 0.19 0.25 0.33 0.17 0.20 0.24 LSTM 0.26 0.36 0.49 0.20 0.26 0.34 0.20 0.23 0.27 ResNet 0.25 0.34 0.46 0.18 0.25 0.35 0.10 0.13 0.17 ProtBert 0.30 0.40 0.52 0.27 0.35 0.47 0.20 0.26 0.34 OntoProtein 0.37 0.46 0.57 0.32 0.40 0.50 0.24 0.31 0.39 Table 4: Ablation study of contact prediction. seq refers to the sequence length between amino acids. “P@K” is precision for the top Kcontacts and Lis the length of the protein. Figure 4: We randomly select a protein from the contact test dataset for visual analysis. Left: We visualize the 7th head in the last attention layer in OntoProtein. Right: It is the contact label matrix. Results We split the test sets into three subsets (BPO, MFO, and CCO) and evaluate the perfor- mance of models separately. From Table 3, we notice that our OntoProtein can yield a 4% im- provement with transductive setting and 2% advancement with inductive setting in BPO, further demonstrating the effectiveness of our proposed approach. We also observe that OntoProtein ob- tain comparable performance in other subsets. Note that there exists a severe long-tail issue in the dataset, and knowledge injecting may affect the representation learning for the head but weaken the tail representation, thus cause performance degradation. We leave this for future works. 3.3 A NALYSIS Table 4 illustrates a detailed experimental analysis on the contact prediction. To further analyze the model’s performance, we conduct experiments to probe the performance of different sequences. Speciﬁcally, protein sequence lengths from short-range (6 ≤seq <12) to long-range (24 ≤seq) are tested with three metrics (P@L, P@L/2, P@L/5). We choose several basic algorithms such as LSTM and TAPE transformer as baselines. For fairness, ProtBert is also leveraged for comparison. It can be seen that the performance of OntoProtein exceeds all other methods in all test settings, which is reasonable because the knowledge injected from Gene Ontology is beneﬁcial. Further, we random sample a protein instance from the test dataset and analyze its attention weight of OntoProtein. We conduct visualization analysis as shown in Figure 4 to compare the contacts among amino acids with the contact label matrix. 3.4 D ISCUSSION Applying techniques from NLP to proteins opens new opportunities to extract information from proteins in a self-supervised, data-driven way. Here we show for the ﬁrst time that injecting external knowledge from gene ontology can help to learn protein representation better, thus, boosting the downstream protein tasks. However, the gains in our proposed OntoProtein compared to previous pre-trained models using large-scale corpus is still relatively small. Note that the knowledge graph ProteinKG25 can only cover a small subset of all proteins, thus, limiting the advancement. We will continue to maintain the knowledge graph by adding new facts from Gene Ontology. Besides, previous studies (Liu et al. (2020); Zhang et al. (2021a)) indicate that not all external knowledge are 8Published as a conference paper at ICLR 2022 beneﬁcial for downstream tasks, and it is necessary to investigate when and how to inject external knowledge into pre-trained models effectively. Finally, our proposed approach can be viewed as jointly pre-training human language and protein (the language of life). Our motivation is to crack the language of life’s code with gene knowledge injected protein pre-training. Our work is but a small step in this direction. 4 R ELATED WORK 4.1 P RE-TRAINED LANGUAGE MODELS Up to now, various efforts have been devoted to exploring large-scale PTMs, either for NLP (Peters et al. (2018); Devlin et al. (2019)), or for CV (Tan & Bansal (2019)). Fine-tuning large-scale PTMs such as ELMo (Peters et al. (2018)), GPT3 (Brown et al. (2020)), BERT (Devlin et al. (2019)), XLNet (Yang et al. (2019)) UniLM (Dong et al. (2019)) for speciﬁc AI tasks instead of learning models from scratch has also become a consensus (Han et al. (2021)). Apart from the of large scale language models for natural language processing, there has been considerable interest in develop- ing similar models for proteins (Xiao et al. (2021); Rives et al. (2021)). Rao et al. (2021a) is the ﬁrst to study protein Transformer language models, demonstrating that information about residue- residue contacts can be recovered from the learned representations by linear projections supervised with protein structures. Vig et al. (2021) performs an extensive analysis of Transformer attention, identifying correspondences to biologically relevant features, and also ﬁnds that different layers of the model are responsible for learning different features. Elnaggar et al. (2020) proposes ProtTrans, which explores the limits of up-scaling language models trained on proteins as well as protein se- quence databases and compares the effects of auto-regressive and auto-encoding pre-training upon the success of the subsequent supervised training. Human-curated or domain-speciﬁc knowledge is essential for downstream tasks, which is extensively studied such as Himmelstein & Baranzini (2015), Smaili et al. (2018), Smaili et al. (2019), Hao et al. (2020), Ioannidis et al. (2020) . How- ever these pre-training methods do not explicitly consider external knowledge like our proposed OntoProtein. 4.2 K NOWLEDGE -ENHANCED LANGUAGE MODELS Background knowledge has been considered as an indispensable part of language understanding ((Zhang et al., 2021a; Deng et al., 2021; Li et al., 2021; Zhang et al., 2019a; Yu et al., 2020; Zhu et al., 2021; Zhang et al., 2021b; Chen et al., 2021; Zhang et al., 2022b; Silvestri et al., 2021; Zhang et al., 2021c; Yao et al., 2022; Zhang et al., 2022a)), which has inspired knowledge-enhanced models including ERNIE (Tsinghua) (Zhang et al. (2019b)), ERNIE (Baidu) (Sun et al. (2019)), KnowBERT (Peters et al. (2019)), WKLM (Xiong et al. (2020)), LUKE (Yamada et al. (2020)), KEPLER (Wang et al. (2021b)), K-BERT (Liu et al. (2020)), K-Adaptor (Wang et al. (2021a)), and CoLAKE (Sun et al. (2020)). ERNIE (Zhang et al. (2019b)) injects relational knowledge into the pre-trained model BERT, which aligns entities from Wikipedia to facts in WikiData. KEPLER (Wang et al. (2021b)) jointly optimizes knowledge embedding and pre-trained language representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also effectively learn KE through the abundant information in the text. Inspired by these works, we propose OntoProtein that integrates external knowledge graphs into protein pre-training. To the best of our knowledge, we are the ﬁrst to inject gene ontology knowledge into protein language models. 5 C ONCLUSION AND FUTURE WORK In this paper, we take the ﬁrst step to integrating external factual knowledge from gene ontology into protein language models. We present protein pretraining with gene ontology embedding (On- toProtein), which is the ﬁrst general framework to integrate external knowledge graphs into protein pre-training. Experimental results on widespread protein tasks demonstrate that efﬁcient knowledge injection helps understand and uncover the grammar of life. Besides, OntoProtein is compatible with the model parameters of lots of pre-trained protein language models, which means that users can directly adopt the available pre-trained parameters on OntoProtein without modifying the ar- 9Published as a conference paper at ICLR 2022 chitecture. These positive results point to future work in (1) improving OntoProtein by injecting more informative knowledge with gene ontology selection; (2) extending this approach to sequence generating tasks for protein design. ACKNOWLEDGMENTS We want to express gratitude to the anonymous reviewers for their hard work and kind comments. This work is funded by NSFCU19B2027/NSFC91846204, National Key R&D Program of China (Funding No.SQ2018YFC000004), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduc- tion Programme (2021A-156-G). REPRODUCIBILITY STATEMENT Our code and datasets are all available in the https://github.com/zjunlp/ OntoProtein for reproducibility. Hyper-parameters are provided in the Appendix A.3. REFERENCES Antoine Bordes, Nicolas Usunier, Alberto Garc ´ıa-Dur´an, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Christopher J. C. Burges, L ´eon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States , pp. 2787–2795, 2013. URL https://proceedings.neurips.cc/paper/2013/hash/ 1cecc7a77928ca8133fa24680a88d2f9-Abstract.html. Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. Proteinbert: A univer- sal deep-learning model of protein sequence and function. bioRxiv, 2021. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan- dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Muhao Chen, Chelsea J.-T. Ju, Guangyu Zhou, Xuelu Chen, Tianran Zhang, Kai-Wei Chang, Carlo Zaniolo, and Wei Wang. Multifaceted protein-protein interaction prediction based on siamese residual RCNN. Bioinform., 35(14):i305–i314, 2019. doi: 10.1093/bioinformatics/btz328. URL https://doi.org/10.1093/bioinformatics/btz328. Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. CoRR, abs/2104.07650, 2021. URL https://arxiv.org/abs/ 2104.07650. Shumin Deng, Ningyu Zhang, Luoqiu Li, Chen Hui, Huaixiao Tou, Mosha Chen, Fei Huang, and Huajun Chen. Ontoed: Low-resource event detection with ontology embedding. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, Au- gust 1-6, 2021, pp. 2828–2839. Association for Computational Linguistics, 2021. doi: 10.18653/ v1/2021.acl-long.220. URL https://doi.org/10.18653/v1/2021.acl-long.220. 10Published as a conference paper at ICLR 2022 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pp. 4171– 4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Uniﬁed language model pre-training for natural language understanding and generation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz- imer, Florence d’Alch ´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu- ral Information Processing Systems 32: Annual Conference on Neural Information Pro- cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pp. 13042–13054, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html. Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing. CoRR, abs/2007.06225, 2020. URL https: //arxiv.org/abs/2007.06225. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-speciﬁc language model pretraining for biomedical natural language processing, 2020. Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. Pre-trained models: Past, present and future. CoRR, abs/2106.07139, 2021. URL https://arxiv.org/ abs/2106.07139. Junheng Hao, Chelsea J-T Ju, Muhao Chen, Yizhou Sun, Carlo Zaniolo, and Wei Wang. Bio-joie: Joint representation learning of biological knowledge bases. In Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, pp. 1–10, 2020. Somaye Hashemifar, Behnam Neyshabur, Aly A. Khan, and Jinbo Xu. Predicting protein-protein interactions through sequence-based deep learning. Bioinform., 34(17):i802–i810, 2018. doi: 10.1093/bioinformatics/bty573. URL https://doi.org/10.1093/bioinformatics/ bty573. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pp. 770–778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90. Daniel S Himmelstein and Sergio E Baranzini. Heterogeneous network edge prediction: a data integration approach to prioritize disease-associated genes. PLoS computational biology, 11(7): e1004259, 2015. Vassilis N Ioannidis, Xiang Song, Saurav Manchanda, Mufei Li, Xiaoqin Pan, Da Zheng, Xia Ning, Xiangxiang Zeng, and George Karypis. Drkg-drug repurposing knowledge graph for covid-19, 2020. Chengxi Li, Feiyu Gao, Jiajun Bu, Lu Xu, Xiang Chen, Yu Gu, Zirui Shao, Qi Zheng, Ningyu Zhang, Yongpan Wang, and Zhi Yu. Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspect-based sentiment analysis. CoRR, abs/2109.08306, 2021. URL https://arxiv. org/abs/2109.08306. Hang Li, Xiu-Jun Gong, Hua Yu, and Chang Zhou. Deep neural network based predictions of protein interactions using primary sequences. Molecules, 23(8):1923, 2018. 11Published as a conference paper at ICLR 2022 Maria Littmann, Michael Heinzinger, Christian Dallago, Tobias Olenyi, and Burkhard Rost. Em- beddings from deep learning transfer go annotations beyond homology. Scientiﬁc reports, 11(1): 1–14, 2021. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-BERT: en- abling language representation with knowledge graph. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelli- gence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 2901–2908. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/5681. Guofeng Lv, Zhiqiang Hu, Yanguang Bi, and Shaoting Zhang. Learning unknown from corre- lations: Graph neural network for inter-novel-protein interaction prediction. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pp. 3677–3683. ijcai.org, 2021. doi: 10.24963/ijcai.2021/506. URL https://doi.org/10.24963/ijcai.2021/506. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Pro- cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pp. 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers) , pp. 2227–2237. Association for Computational Linguistics, 2018. doi: 10.18653/v1/n18-1202. URL https: //doi.org/10.18653/v1/n18-1202. Matthew E. Peters, Mark Neumann, Robert L. Logan IV , Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. Knowledge enhanced contextual word representations. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 43– 54. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1005. URLhttps: //doi.org/10.18653/v1/D19-1005. Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John F. Canny, Pieter Abbeel, and Yun S. Song. Evaluating protein transfer learning with TAPE. In Hanna M. Wal- lach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 9686–9698, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/37f65c068b7723cd7809ee2d31d7861c-Abstract.html. Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. MSA transformer. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8844–8856. PMLR, 2021a. URL http://proceedings.mlr.press/v139/rao21a.html. Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer protein language models are unsupervised structure learners. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021b. URL https://openreview.net/forum?id=fylclEqgvgd. 12Published as a conference paper at ICLR 2022 Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc. Natl. Acad. Sci. USA, 118(15):e2016239118, 2021. doi: 10.1073/pnas.2016239118. URL https://doi. org/10.1073/pnas.2016239118. Mattia Silvestri, Michele Lombardi, and Michela Milano. Injecting domain knowledge in neural net- works: A controlled experiment on a constrained problem. In Peter J. Stuckey (ed.), Integration of Constraint Programming, Artiﬁcial Intelligence, and Operations Research - 18th International Conference, CPAIOR 2021, Vienna, Austria, July 5-8, 2021, Proceedings, volume 12735 of Lec- ture Notes in Computer Science, pp. 266–282. Springer, 2021. doi: 10.1007/978-3-030-78230-6\\ 17. URL https://doi.org/10.1007/978-3-030-78230-6_17 . Fatima Zohra Smaili, Xin Gao, and Robert Hoehndorf. Onto2vec: joint vector-based representation of biological entities and their ontology-based annotations.Bioinformatics, 34(13):i52–i60, 2018. Fatima Zohra Smaili, Xin Gao, and Robert Hoehndorf. Opa2vec: combining formal and informal content of biomedical ontologies to improve similarity-based prediction. Bioinformatics, 35(12): 2133–2140, 2019. Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang. Colake: Contextualized language and knowledge embedding. In Donia Scott, N ´uria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pp. 3660–3670. In- ternational Committee on Computational Linguistics, 2020. doi: 10.18653/v1/2020.coling-main. 327. URL https://doi.org/10.18653/v1/2020.coling-main.327. Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. ERNIE: enhanced representation through knowledge integration. CoRR, abs/1904.09223, 2019. URL http://arxiv.org/abs/1904.09223. Hao Tan and Mohit Bansal. LXMERT: learning cross-modality encoder representations from trans- formers. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pp. 5099–5110. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1514. URL https://doi.org/10.18653/v1/D19-1514. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. Bertology meets biology: Interpreting attention in protein language models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https://openreview.net/forum?id= YWtLZvLmud7. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou. K-adapter: Infusing knowledge into pre-trained models with adapters. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 , volume ACL/IJCNLP 2021 of Findings of ACL, pp. 1405–1418. Association for Computational Linguistics, 2021a. doi: 10.18653/v1/2021.ﬁndings-acl.121. URL https://doi.org/10. 18653/v1/2021.findings-acl.121. 13Published as a conference paper at ICLR 2022 Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. KEPLER: A uniﬁed model for knowledge embedding and pre-trained language representa- tion. Trans. Assoc. Comput. Linguistics, 9:176–194, 2021b. URL https://transacl.org/ ojs/index.php/tacl/article/view/2447. Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, and Jie Tang. Modeling protein using large- scale pretrain language model. CoRR, abs/2108.07435, 2021. URL https://arxiv.org/ abs/2108.07435. Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. Pretrained encyclope- dia: Weakly supervised knowledge-pretrained language model. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe- view.net, 2020. URL https://openreview.net/forum?id=BJlzm64tDH. Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. LUKE: deep contextualized entity representations with entity-aware self-attention. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.),Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 6442–6454. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.523. URL https://doi.org/10.18653/v1/2020.emnlp-main.523. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5754–5764, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html. Yunzhi Yao, Shaohan Huang, Ningyu Zhang, Li Dong, Furu Wei, and Huajun Chen. Kformer: Knowledge injection in transformer feed-forward layers. CoRR, abs/2201.05742, 2022. URL https://arxiv.org/abs/2201.05742. Haiyang Yu, Ningyu Zhang, Shumin Deng, Hongbin Ye, Wei Zhang, and Huajun Chen. Bridg- ing text and knowledge with multi-prototype embedding for few-shot relational triple extraction. In Donia Scott, N ´uria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), Decem- ber 8-13, 2020 , pp. 6399–6410. International Committee on Computational Linguistics, 2020. doi: 10.18653/v1/2020.coling-main.563. URL https://doi.org/10.18653/v1/2020. coling-main.563. Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang, and Huajun Chen. Long-tail relation extraction via knowledge graph embeddings and graph convolution net- works. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 3016–3025. Association for Computational Linguistics, 2019a. doi: 10.18653/v1/n19-1306. URL https://doi.org/10.18653/v1/n19-1306. Ningyu Zhang, Shumin Deng, Xu Cheng, Xi Chen, Yichi Zhang, Wei Zhang, and Huajun Chen. Drop redundant, shrink irrelevant: Selective knowledge injection for language pretraining. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 , pp. 4007– 4014. ijcai.org, 2021a. doi: 10.24963/ijcai.2021/552. URL https://doi.org/10.24963/ ijcai.2021/552. Ningyu Zhang, Qianghuai Jia, Shumin Deng, Xiang Chen, Hongbin Ye, Hui Chen, Huaixiao Tou, Gang Huang, Zhao Wang, Nengwei Hua, and Huajun Chen. Alicg: Fine-grained and evolvable conceptual graph construction for semantic search at alibaba. In Feida Zhu, Beng Chin Ooi, and Chunyan Miao (eds.), KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Dis- covery and Data Mining, Virtual Event, Singapore, August 14-18, 2021 , pp. 3895–3905. ACM, 14Published as a conference paper at ICLR 2022 2021b. doi: 10.1145/3447548.3467057. URL https://doi.org/10.1145/3447548. 3467057. Ningyu Zhang, Hongbin Ye, Jiacheng Yang, Shumin Deng, Chuanqi Tan, Mosha Chen, Songfang Huang, Fei Huang, and Huajun Chen. LOGEN: few-shot logical knowledge-conditioned text generation with self-training. CoRR, abs/2112.01404, 2021c. URL https://arxiv.org/ abs/2112.01404. Ningyu Zhang, Xin Xie, Xiang Chen, Shumin Deng, Chuanqi Tan, Fei Huang, Xu Cheng, and Huajun Chen. Reasoning through memorization: Nearest neighbor knowledge graph embeddings. CoRR, abs/2201.05575, 2022a. URL https://arxiv.org/abs/2201.05575. Ningyu Zhang, Xin Xu, Liankuan Tao, Haiyang Yu, Hongbin Ye, Xin Xie, Xiang Chen, Zhoubo Li, Lei Li, Xiaozhuan Liang, Yunzhi Yao, Shumin Deng, Zhenru Zhang, Chuanqi Tan, Fei Huang, Guozhou Zheng, and Huajun Chen. Deepke: A deep learning based knowledge extraction toolkit for knowledge base population. CoRR, abs/2201.03335, 2022b. URL https://arxiv.org/ abs/2201.03335. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: en- hanced language representation with informative entities. In Anna Korhonen, David R. Traum, and Llu´ıs M`arquez (eds.), Proceedings of the 57th Conference of the Association for Computa- tional Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pp. 1441–1451. Association for Computational Linguistics, 2019b. doi: 10.18653/v1/p19-1139. URL https://doi.org/10.18653/v1/p19-1139. Hengyi Zheng, Rui Wen, Xi Chen, Yifan Yang, Yunyan Zhang, Ziheng Zhang, Ningyu Zhang, Bin Qin, Xu Ming, and Yefeng Zheng. PRGC: potential relation and global correspondence based joint relational triple extraction. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguis- tics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 6225–6235. Associa- tion for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.486. URL https: //doi.org/10.18653/v1/2021.acl-long.486. Naihui Zhou, Yuxiang Jiang, Timothy R Bergquist, Alexandra J Lee, Balint Z Kacsoh, Alex W Crocker, Kimberley A Lewis, George Georghiou, Huy N Nguyen, Md Naﬁz Hamid, et al. The cafa challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens. Genome biology, 20(1):1–23, 2019. Yushan Zhu, Huaixiao Tou, Wen Zhang, Ganqiang Ye, Hui Chen, Ningyu Zhang, and Huajun Chen. Knowledge perceived multi-modal pretraining in e-commerce. CoRR, abs/2109.00895, 2021. URL https://arxiv.org/abs/2109.00895. A A PPENDIX A.1 C ONSTRUCTION OF PROTEIN KG25 To incorporate Gene Ontology knowledge into language models and train OntoProtein, we construct ProteinKG25, a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and proteins entities. We design two evaluation schemes, the transductive and the inductive settings, which simulate two scenarios of gene annotation in reality. We use the latest Gene Ontology and Gene Annotations released in April 2020. Gene Ontology de- picts the relation between GO terms using GO-GO triplet, and Gene Annotation depicts the relations between protein and GO term using Protein-GO triplet. Due to this connectivity, we combine the two structures into a uniﬁed knowledge graph. For each GO term in Gene Ontology, we align it to its corresponding name and description and concatenate them by a colon as an entire description. For each protein in Gene annotation, we align it to the Swiss-Prot and extract its corresponding se- quence as its description. The ﬁnal KG contains 4,990,097 triplets (4,879,951 Protein-GO triplets and 110,146 GO-GO triplets), 612,483 entities (565,254 proteins and 47,229 GO terms) and 31 relations. 15Published as a conference paper at ICLR 2022 Due to the tree-like hierarchical structure of Gene Ontology, we deﬁne the depth of GO terms using the shortest path from current GO terms to the root node of GO terms. The distribution of Protein- GO triplets with respect to the depth of GO term is shown at the bottom of Figure 3. Usually, the deeper a GO term is located, the more concrete deﬁnition the GO term has, e.g., the small molecule biosynthetic process is the child node of the biosynthetic process. We notice that the number of gene annotations involved with leaf GO terms is a relatively small percentage in all three types of ontologies. We think there are two possibilities: (1) The complexity of annotations of some concrete GO terms, e.g., the identiﬁcation of whether a protein is involved in the hexose biosynthetic process. (2) The relatively small number of proteins in nature involved with some speciﬁc GO terms is intrinsic to these GO terms. We further pre-process the ProteinKG25 dataset as follows. We observe that the relations of Protein- Go have long-tailed distribution, and mostly focused on involved in, part of and enables. Such data distribution will seriously affect the protein feature embedding during pre-training, so we pre- process our dataset to add more precise and ﬁne-grained relations. Figure 5 illustrates the relation distribution of the original model and the distribution after being pre-poccessed. We search for GO terms whose frequency of occurrence in ProteinKG25 is the top 10 in MF and CC, top 20 in BP, then we form a new type of Protein2GOrelation by their corresponding relationship such as parts of cytoplasm. A.2 D OWNSTREAM TASK DEFINITION We list the detailed deﬁnition of downstream tasks and its corresponding or similar tasks in nature language processing. • Secondary Structure Prediction is a token-level task and similar to NER (Name Entity Recognition). Each token (amino acid) xi is mapped to a label yi ∈ {Helix,Strand,Other }. • Contact Predictionis a token-level matching task. Each token (amino acid) pairxi , xj of sequence (protein) xis mapped to a label yij ∈{0,1}. • Remote Homology Detectionis a sequence-level classiﬁcation task. Each input sequence (protein) x is mapped to a label y ∈ {1,..., 1195}which represents different possible protein folds. • Fluorescence Landscape Predictionand Stability Landscape Predictionare regression tasks where each sequence (protein) xis mapped to a label y∈R . • Protein Protein Interfaceis a sequence-level matching task. Each sequence (protein) pair xi , xj is mapped to a label yij ∈{0,1}. • Protein Function Predictionis a sequence-level classiﬁcation task or a knowledge graph completion task to prediction link of a protein to the Gene Ontology. A.3 E XPERIMENTAL SETTINGS This section details the training procedures and hyperparameters for each of the datasets. We utilize Pytorch (Paszke et al. (2019)) to conduct experiments with Nvidia V100 GPUs. In pre-training of OntoProtein, similar to Elnaggar et al. (2020), we use the same training protocol such as optimizer, learning rate schedule on BERT model. We set γ to 12.0 and the number of negative sampling to 128 in Equation 1. A.4 D ATAFLOW OF ONTO PROTEIN We use batches (protein-seq, protein-go, go-go) to jointly train the model, which is shown in Figure 7. 16Published as a conference paper at ICLR 2022 is_a part_of regulates has_part negatively_regulates positively_regulates occurs_in happens_during ends_during enables involved_in acts_upstream_of_or_within colocalizes_with contributes_to NOT|enables NOT|involved_in NOT|part_of acts_upstream_of NOT|colocalizes_with NOT|acts_upstream_of_or_within acts_upstream_of_positive_effect acts_upstream_of_or_within_positive_effect is_active_in acts_upstream_of_negative_effect acts_upstream_of_or_within_negative_effect NOT|contributes_to NOT|acts_upstream_of_or_within_negative_effect located_in NOT|located_in NOT|is_active_in NOT|acts_upstream_of enables_hydrolase_activity involved_in_signal_transduction involved_in_metabolic_process enables_metal_ion_binding enables_catalytic_activity enables_nucleotide_binding part_of_membrane part_of_integral_component_of_membrane involved_in_cellular_response_to_DNA_damage_stimulus part_of_mitochondrion part_of_extracellular_region enables_RNA_binding part_of_cytoplasm enables_DNA_binding part_of_nucleus enables_transferase_activity involved_in_proteolysis involved_in_methylation involved_in_lipid_metabolic_process part_of_cytosol involved_in_carbohydrate_metabolic_process involved_in_cellular_amino_acid_biosynthetic_process involved_in_transmembrane_transport involved_in_regulation_of_transcription,_DNA-templated involved_in_ion_transport part_of_plastid involved_in_phosphorylation involved_in_cell_cycle involved_in_cell_division involved_in_protein_transport involved_in_translation NOT|involved_in_tRNA_processing enables_structural_constituent_of_ribosome part_of_ribosome 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 1e6 is_a part_of regulates has_part negatively_regulates positively_regulates occurs_in happens_during ends_during enables involved_in acts_upstream_of_or_within colocalizes_with contributes_to NOT|enables NOT|involved_in NOT|part_of acts_upstream_of NOT|colocalizes_with NOT|acts_upstream_of_or_within acts_upstream_of_positive_effect acts_upstream_of_or_within_positive_effect is_active_in acts_upstream_of_negative_effect acts_upstream_of_or_within_negative_effect NOT|contributes_to NOT|acts_upstream_of_or_within_negative_effect located_in NOT|located_in NOT|is_active_in NOT|acts_upstream_of enables_hydrolase_activity involved_in_signal_transduction involved_in_metabolic_process enables_metal_ion_binding enables_catalytic_activity enables_nucleotide_binding part_of_membrane part_of_integral_component_of_membrane involved_in_cellular_response_to_DNA_damage_stimulus part_of_mitochondrion part_of_extracellular_region enables_RNA_binding part_of_cytoplasm enables_DNA_binding part_of_nucleus enables_transferase_activity involved_in_proteolysis involved_in_methylation involved_in_lipid_metabolic_process part_of_cytosol involved_in_carbohydrate_metabolic_process involved_in_cellular_amino_acid_biosynthetic_process involved_in_transmembrane_transport involved_in_regulation_of_transcription,_DNA-templated involved_in_ion_transport part_of_plastid involved_in_phosphorylation involved_in_cell_cycle involved_in_cell_division involved_in_protein_transport involved_in_translation NOT|involved_in_tRNA_processing enables_structural_constituent_of_ribosome part_of_ribosome 0 100000 200000 300000 400000 500000 600000 Figure 5: Top: Initial relation distribution. Bottom: Pre-processed relation distribution. time training set validation set test set Apr, 2020 Aug, 2020 Apr, 2021 Figure 6: The timeline of the three Gene Annotation datasets. To generate pre-training set and evaluation set of protein function prediction, we choose three Gene Annotation datasets in different periods. 17Published as a conference paper at ICLR 2022 Entity Type Relation Type Molecular function enables, contributes to Cellular component located in, part of, is active in, colocalizes with acts upstream of or within, involved in, acts upstream of, Biological process acts upstream of positive effect, acts upstream of negative effect, acts upstream of or within positive effect, acts upstream of or within negative effect Table 5: Entity categories of GO terms and speciﬁc relation types of these entity categories. Task epoch batch size warmup ratio learning rate frozen bert optimizer ss3 5 32 0.08 3e-5 False AdamW ss8 5 32 0.08 3e-5 False AdamW stability 5 32 0.08 3e-5 False AdamW ﬂuorescence: 25 64 0.0 3e-5 True AdamW remote homology 10 64 0.08 3e-5 False AdamW contact 10 8 0.08 3e-5 False AdamW Table 6: Hyper-parameters for the downstream task. Figure 7: The dataﬂow of OntoProtein. TERM DESCRIPTION GO annotation A statement about the function of a particular gene GO term A standard vocabulary term for biological function annotation GO statement A speciﬁc deﬁnition of GO term Table 7: Terms descriptions in Gene Ontology. GO annotations are created by associating a gene or gene product with a GO term. Four pieces of information uniquely identify a GO annotation: Gene Product, Go term, Reference, Evidence. And each Go term contains a description text, and this is Go statement. 18",
      "references": [
        "Translating embeddings for modeling multi-relational data.",
        "Proteinbert: A univer- sal deep-learning model of protein sequence and function.",
        "Language models are few-shot learners.",
        "Multifaceted protein-protein interaction prediction based on siamese residual RCNN.",
        "Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction.",
        "Ontoed: Low-resource event detection with ontology embedding.",
        "BERT: pre-training of deep bidirectional transformers for language understanding.",
        "Uniﬁed language model pre-training for natural language understanding and generation.",
        "Prottrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing.",
        "Domain-speciﬁc language model pretraining for biomedical natural language processing, 2020.",
        "Pre-trained models: Past, present and future.",
        "Bio-joie: Joint representation learning of biological knowledge bases.",
        "Predicting protein-protein interactions through sequence-based deep learning.",
        "Deep residual learning for image recognition.",
        "Heterogeneous network edge prediction: a data integration approach to prioritize disease-associated genes.",
        "Drkg-drug repurposing knowledge graph for covid-19, 2020.",
        "Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspect-based sentiment analysis.",
        "Deep neural network based predictions of protein interactions using primary sequences.",
        "Em- beddings from deep learning transfer go annotations beyond homology.",
        "K-BERT: en- abling language representation with knowledge graph.",
        "Learning unknown from corre- lations: Graph neural network for inter-novel-protein interaction prediction.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Deep contextualized word representations.",
        "Knowledge enhanced contextual word representations.",
        "Evaluating protein transfer learning with TAPE.",
        "MSA transformer.",
        "Transformer protein language models are unsupervised structure learners.",
        "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences.",
        "Injecting domain knowledge in neural net- works: A controlled experiment on a constrained problem.",
        "Onto2vec: joint vector-based representation of biological entities and their ontology-based annotations.",
        "Opa2vec: combining formal and informal content of biomedical ontologies to improve similarity-based prediction.",
        "Colake: Contextualized language and knowledge embedding.",
        "ERNIE: enhanced representation through knowledge integration.",
        "LXMERT: learning cross-modality encoder representations from trans- formers.",
        "Attention is all you need.",
        "Bertology meets biology: Interpreting attention in protein language models.",
        "K-adapter: Infusing knowledge into pre-trained models with adapters.",
        "KEPLER: A uniﬁed model for knowledge embedding and pre-trained language representa- tion.",
        "Modeling protein using large- scale pretrain language model.",
        "Pretrained encyclope- dia: Weakly supervised knowledge-pretrained language model.",
        "LUKE: deep contextualized entity representations with entity-aware self-attention.",
        "Xlnet: Generalized autoregressive pretraining for language understanding.",
        "Kformer: Knowledge injection in transformer feed-forward layers.",
        "Bridg- ing text and knowledge with multi-prototype embedding for few-shot relational triple extraction.",
        "Long-tail relation extraction via knowledge graph embeddings and graph convolution net- works.",
        "Drop redundant, shrink irrelevant: Selective knowledge injection for language pretraining.",
        "Alicg: Fine-grained and evolvable conceptual graph construction for semantic search at alibaba.",
        "LOGEN: few-shot logical knowledge-conditioned text generation with self-training.",
        "Reasoning through memorization: Nearest neighbor knowledge graph embeddings.",
        "Deepke: A deep learning based knowledge extraction toolkit for knowledge base population.",
        "ERNIE: en- hanced language representation with informative entities.",
        "PRGC: potential relation and global correspondence based joint relational triple extraction.",
        "The cafa challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens.",
        "Knowledge perceived multi-modal pretraining in e-commerce."
      ],
      "meta_data": {
        "arxiv_id": "2201.11147v6",
        "authors": [
          "Ningyu Zhang",
          "Zhen Bi",
          "Xiaozhuan Liang",
          "Siyuan Cheng",
          "Haosen Hong",
          "Shumin Deng",
          "Jiazhang Lian",
          "Qiang Zhang",
          "Huajun Chen"
        ],
        "published_date": "2022-01-23T14:49:49Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes OntoProtein, a general, model-agnostic framework to inject structured biological knowledge from Gene Ontology (GO) into protein language model (PLM) pretraining. Constructs and releases ProteinKG25, a large-scale GO–protein knowledge graph (≈612k entities, ≈5.0M triples) with aligned text (GO descriptions) and sequences (protein amino acids). Introduces joint pretraining with masked protein modeling plus a knowledge-embedding objective and a knowledge-aware negative sampling strategy for contrastive learning, yielding improvements over PLM baselines on TAPE token-level tasks and better results on protein–protein interaction (PPI) and protein function prediction.",
        "methodology": "Hybrid dual-encoder setup: (1) protein encoder initialized from ProtBert (BERT-style Transformer over amino-acid tokens) producing token and pooled sequence representations; (2) GO text encoder initialized from PubMedBERT to embed GO term names/descriptions; (3) learned relation embeddings. Aligns protein and GO embedding spaces via an affine projection. Pretraining optimizes (a) MLM-style masked protein modeling loss over sequences; and (b) TransE-style knowledge embedding loss over KG triples using a margin-based logistic contrastive objective. Novel knowledge-aware negative sampling: for GO–GO triples, corrupt head/tail by sampling replacement GO terms constrained to the same ontology aspect (MFO/CCO/BPO) to create harder negatives; for protein–GO triples, corrupt only the GO tail similarly. Overall loss: L = α L_KE + L_MLM.",
        "experimental_setup": "Pretraining data: ProteinKG25 built from April 2020 GO + Gene Annotations aligned to Swiss-Prot sequences; 4,990,097 triples (≈4,879,951 protein–GO and ≈110,146 GO–GO), 612,483 entities (≈565,254 proteins, 47,229 GO terms), 31 relations; GO terms have concatenated name:description text. KE training uses γ=12 and 128 negatives. Evaluation: (1) TAPE benchmark (6 tasks): secondary structure (SS-Q3, SS-Q8 on CB513), contact prediction (medium/long-range P@L/2), remote homology, fluorescence and stability (Spearman ρ). Baselines: LSTM, TAPE Transformer, ResNet, MSA Transformer, ProtBert. (2) PPI prediction on STRING (15,335 proteins, 593,397 PPIs), SHS27k, SHS148k with BFS/DFS splits; baselines: DPPI, DNN-PPI, PIPR, GNN-PPI; also GNN-PPI initialized with ProtBert vs OntoProtein embeddings. (3) Protein function prediction as multilabel sequence classification using a CAFA-style dataset derived from ProteinKG25 with both transductive and inductive splits; evaluated separately on BPO/MFO/CCO subsets. Additional analysis: contact prediction performance stratified by sequence separation buckets (6–12, 12–24, ≥24) using P@L, P@L/2, P@L/5; attention visualization comparisons to contact maps.",
        "limitations": "Gains over strong large-corpus PLMs are modest; ProteinKG25 covers only a subset of all proteins, limiting knowledge injection breadth. Pretraining objective is dominated by token-level MLM + KG embedding; lack of explicit sequence-level objectives likely harms regression/sequence-level tasks (e.g., stability, some engineering/homology results). Negative sampling is constrained and may not fully exploit protein-side hard negatives (authors note homologous-protein corruption is not used). Long-tail distribution of GO terms/relations and sparse annotations can cause uneven benefits—potentially improving head concepts while degrading tail performance (noted in function prediction). Uses TransE scoring (simple translational model) which may underfit complex GO relational structure; relies on alignment quality of GO texts and protein sequences from external databases.",
        "future_research_directions": "Expand and continuously update ProteinKG25 with broader protein coverage and richer GO/annotation facts; study selective/filtered knowledge injection to determine when/which ontology facts help specific downstream tasks. Add sequence-level or task-aligned pretraining objectives (e.g., contrastive sequence-level, regression-aware, protein family/disorder signals) to improve engineering/regression tasks. Explore stronger KG embedding models (e.g., RotatE/ComplEx, path-based or hierarchical GO-aware scoring) and better multimodal fusion beyond affine projection. Improve hard negative sampling using protein biology (e.g., homologous proteins, structural similarity, taxonomic constraints) and curriculum/adversarial sampling. Extend to generative protein design tasks (sequence generation conditioned on GO/function) and to other biological KGs (pathways, drug–target, interactions) for multi-knowledge pretraining.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Periodic schema reorganization for KG-from-text under distant supervision is currently governed by *heuristic* split/merge decisions (prototype clustering, thresholding, one-off validation). In a streaming pipeline, however, schema edits are proposed *adaptively* (based on the data seen so far) and evaluated repeatedly. This adaptivity makes standard held-out validation/bootstrapping unreliable: even if each edit is “validated”, the system can still accept a harmful merge/split due to implicit multiple testing and peeking, leading to irreversible schema drift (catastrophic collapses) and unstable downstream querying.\n\n**Focused gap:** How can we perform *online* relation-schema split/merge repair under distant supervision with an **anytime-valid do-no-harm guarantee**—i.e., control the probability of accepting a harmful schema edit despite adaptive proposal generation—without requiring new human labels?",
    "method": "### Anytime-Valid Risk-Controlled Split–Merge Repair (AVaRiSM)\nWe refine RiSM-ProtoRE by replacing fixed-time bootstrap gating with **anytime-valid confidence sequences** for schema-edit decisions, making the “do-no-harm” claim statistically meaningful under adaptive, repeated edit proposals.\n\n**Core idea:** Treat each candidate edit (merge or split) as a sequential decision problem on a calibration stream. Let \\(\\Delta\\in[-1,1]\\) be the per-instance change in a chosen correctness proxy when applying the edit vs. keeping the current schema (e.g., proxy correctness of relation assignment, plus optional KG constraint satisfaction). Maintain a **time-uniform lower confidence bound** \\(LCB_t\\) on \\(\\mathbb{E}[\\Delta]\\). Accept an edit only when \\(LCB_t \\ge -\\epsilon\\). This provides an anytime-valid “non-degradation” gate that remains valid even if we:\n- generate edits based on the current model/state,\n- check the bound multiple times,\n- stop early as soon as evidence is sufficient.\n\n**What changes vs. RiSM-ProtoRE (novelty):**\n1) **Anytime-valid decision rule** (confidence sequences with log-log stitching) instead of fixed-time bootstrap/t-tests. This directly targets the core deployment pathology: *adaptive repeated testing*.\n2) **Edit-as-audit abstraction:** every proposed edit must pass an online audit with a global risk budget (alpha-spending) across many edits, making schema evolution comparable to controlled experimentation.\n3) **Multi-evidence proxy (optional but Python-feasible):** correctness proxy can combine (a) nearest-prototype agreement, (b) DS bag-consistency, and (c) soft type-constraint satisfaction into a single bounded score—so acceptance is not driven by embedding geometry alone.\n\n**Algorithm (periodic, streaming-friendly):**\n- Maintain relation classes \\(c\\), prototypes \\(p_c\\), and smoothed head/tail type-pair distributions \\(q_c\\).\n- Every \\(M\\) batches:\n  - **Propose merges** using geometry+types: \\(\\cos(p_i,p_j)\\exp(-\\lambda\\mathrm{JSD}(q_i,q_j))\\).\n  - **Propose splits** for high-dispersion classes using 2-way spherical k-means on \\([\\text{embedding} \\| \\text{typepair-onehot}]\\).\n  - **Audit each edit** on a calibration stream \\(B_{cal}\\): compute per-instance \\(\\Delta\\) in proxy correctness and compute a *confidence sequence* lower bound \\(LCB\\). Accept iff \\(LCB\\ge -\\epsilon\\), consuming risk budget \\(\\alpha_e\\) (alpha-spending).\n\nThis yields schema changes that are both bidirectional (split+merge) and **statistically safe under adaptivity**, addressing the key failure mode of online KG schema maintenance.",
    "experimental_setup": "### Data\n**Primary (Python-feasible): synthetic streaming distant-supervision benchmark with adaptivity stressors**\n- Latent relations \\(K_{true}\\) with multi-modal semantics (polysemy).\n- Observed DS labels are over-fragmented (2–4 labels per latent relation) + label flips.\n- Noisy head/tail types sampled from latent type-pair distributions.\n- **Non-stationary stream:** at a drift point, one latent relation changes its dominant mode and type-pair distribution to simulate Wikipedia topic/time shifts; this forces *both* merges and splits over time.\n\n**Secondary (optional pilot):** small Japanese Wikipedia↔Wikidata DS sample, with a manually coarsened mapping for ~20–50 properties (only for sanity checking, not required for main claim).\n\n### Methods compared\n1) **Fixed Proto (no schema reorg).**\n2) **Merge-only heuristic (SA-ProtoRE-like).**\n3) **RiSM (bootstrap-gated split–merge).**\n4) **AVaRiSM (proposed): CS-gated split–merge with alpha-spending.**\n\n### Evaluation\n- **Primary metric:** Macro-F1 on the final test window against latent true relations (cluster→latent mapping via train majority vote).\n- Additional diagnostics: catastrophic-collapse rate, number of accepted harmful edits (in synthetic where truth is known), edit count, and stability across seeds.\n\n### Protocol\n- Stream: train in windows; reorganize every \\(M\\) batches; evaluate on a held-out final window.\n- Run 10 random seeds; report mean±std Macro-F1.\n- Ablations: remove confidence-sequence gate (revert to bootstrap/fixed-time), remove split proposals, remove type features in proposals/splits.\n\nThis setup directly tests whether **anytime-valid gating** reduces harmful schema drift under adaptive proposal generation.",
    "primary_metric": "f1_score",
    "experimental_code": "import numpy as np\nfrom sklearn.metrics import f1_score\nfrom sklearn.cluster import KMeans\n\n# -----------------------------\n# Synthetic streaming DS benchmark with drift\n# -----------------------------\ndef make_stream(rng, n, K_true=5, splits=3, d=64, polysemy=2,\n                flip=0.12, type_noise=0.15, drift_at=0.6, drift_strength=0.7, T=6):\n    \"\"\"Returns a stream ordered in time. After drift_at fraction, one relation's mode/types shift.\"\"\"\n    K_obs = K_true * splits\n\n    # latent semantic modes (unit vectors)\n    mu_mode = rng.normal(size=(K_true, polysemy, d))\n    mu_mode /= np.linalg.norm(mu_mode, axis=-1, keepdims=True)\n\n    # drift: pick a latent relation and rotate its dominant mode\n    drift_rel = int(rng.integers(K_true))\n    new_dir = rng.normal(size=d)\n    new_dir /= np.linalg.norm(new_dir)\n    mu_mode_drift = mu_mode.copy()\n    mu_mode_drift[drift_rel, 0] = (1 - drift_strength) * mu_mode[drift_rel, 0] + drift_strength * new_dir\n    mu_mode_drift[drift_rel, 0] /= np.linalg.norm(mu_mode_drift[drift_rel, 0])\n\n    # observed labels means (fragmentation)\n    def obs_means(mu_mode_use):\n        mu_obs = np.zeros((K_obs, d))\n        for k in range(K_true):\n            for s in range(splits):\n                base = mu_mode_use[k, s % polysemy]\n                mu_obs[k * splits + s] = base + 0.05 * rng.normal(size=d)\n        mu_obs /= np.linalg.norm(mu_obs, axis=1, keepdims=True)\n        return mu_obs\n\n    mu_obs_pre = obs_means(mu_mode)\n    mu_obs_post = obs_means(mu_mode_drift)\n\n    # latent type-pair distributions pre/post\n    tp_pre = rng.dirichlet(alpha=np.ones(T*T), size=K_true)\n    tp_post = tp_pre.copy()\n    # drift types for the drifting relation\n    tp_post[drift_rel] = rng.dirichlet(alpha=np.ones(T*T))\n\n    # sample latent true relations with mild temporal non-iid\n    y_true = rng.integers(0, K_true, size=n)\n\n    # choose observed label among splits\n    y_obs = y_true * splits + rng.integers(0, splits, size=n)\n\n    # DS flips\n    m = rng.random(n) < flip\n    y_obs[m] = rng.integers(0, K_obs, size=m.sum())\n\n    # drift indicator\n    t = np.arange(n)\n    post = t >= int(drift_at * n)\n\n    # types sampled from pre/post tp\n    tp = np.where(post[:, None], tp_post[y_true], tp_pre[y_true])\n    tp_idx = np.array([rng.choice(T*T, p=tp[i]) for i in range(n)])\n    th, tt = tp_idx // T, tp_idx % T\n    corrupt = rng.random(n) < type_noise\n    th[corrupt] = rng.integers(0, T, size=corrupt.sum())\n    tt[corrupt] = rng.integers(0, T, size=corrupt.sum())\n\n    # embeddings pre/post\n    X = np.zeros((n, d))\n    X[~post] = mu_obs_pre[y_obs[~post]] + 0.30 * rng.normal(size=(~post).sum(), d)\n    X[post]  = mu_obs_post[y_obs[post]]  + 0.30 * rng.normal(size=post.sum(), d)\n    X /= np.linalg.norm(X, axis=1, keepdims=True)\n\n    return X, y_true, y_obs, th, tt, K_obs, T\n\n# -----------------------------\n# Prototype & type utilities\n# -----------------------------\ndef prototypes_from_labels(X, y, K):\n    d = X.shape[1]\n    P = np.zeros((K, d))\n    for k in range(K):\n        idx = (y == k)\n        if idx.sum() > 0:\n            P[k] = X[idx].mean(axis=0)\n    nrm = np.linalg.norm(P, axis=1, keepdims=True)\n    nrm[nrm == 0] = 1\n    return P / nrm\n\ndef predict_nearest_proto(X, P):\n    return (X @ P.T).argmax(axis=1)\n\ndef majority_map(src_labels, y_true, K_src, K_true):\n    m = np.zeros(K_src, dtype=int)\n    for k in range(K_src):\n        idx = (src_labels == k)\n        m[k] = 0 if idx.sum() == 0 else np.bincount(y_true[idx], minlength=K_true).argmax()\n    return m\n\ndef typepair_dist(y, th, tt, K, T, eps=1e-3):\n    q = np.zeros((K, T*T)) + eps\n    for k in range(K):\n        idx = (y == k)\n        if idx.sum() > 0:\n            bins = th[idx] * T + tt[idx]\n            q[k] += np.bincount(bins, minlength=T*T)\n    q /= q.sum(axis=1, keepdims=True)\n    return q\n\ndef jsd(p, q, eps=1e-12):\n    p = np.clip(p, eps, 1)\n    q = np.clip(q, eps, 1)\n    m = 0.5 * (p + q)\n    return 0.5 * (p * (np.log(p) - np.log(m))).sum() + 0.5 * (q * (np.log(q) - np.log(m))).sum()\n\n# -----------------------------\n# Anytime-valid lower confidence bound (stitched Hoeffding CS)\n# For bounded X in [-1, 1], a time-uniform CI can be obtained by adding a loglog term.\n# -----------------------------\ndef cs_lcb_mean(x, alpha=0.1):\n    x = np.asarray(x)\n    n = len(x)\n    if n == 0:\n        return -np.inf\n    m = float(x.mean())\n    # stitched/loglog width (simple, practical form)\n    loglog = np.log(np.log2(n + 1) + 1.0)\n    width = np.sqrt((2.0 * (np.log(2.0 / alpha) + loglog)) / n)  # for [-1,1]\n    return m - width\n\n# -----------------------------\n# AVaRiSM: split-merge with confidence-sequence gating\n# -----------------------------\ndef avarism_repair(rng, Xtr, ytr_true, ytr_obs, th_tr, tt_tr,\n                   Xcal, ycal_true, ycal_obs, th_cal, tt_cal,\n                   K_true, K_obs, T,\n                   tau=0.80, lam=1.0, eps=0.0,\n                   alpha_total=0.10, max_tests=80, max_merges=40, max_splits=20,\n                   split_disp=0.22):\n    # classes are sets of observed labels\n    classes = [{k} for k in range(K_obs)]\n\n    def assign_from_classes(y_obs, classes):\n        y = np.zeros_like(y_obs)\n        for ci, mem in enumerate(classes):\n            for k in mem:\n                y[y_obs == k] = ci\n        return y\n\n    def build_state(X, y, th, tt):\n        K = y.max() + 1\n        P = prototypes_from_labels(X, y, K)\n        Q = typepair_dist(y, th, tt, K, T)\n        return P, Q\n\n    def proxy_correctness(X, y_true, y_assign, P):\n        # proxy: agreement with mapped latent label (available in synthetic; in real DS use bag/type proxies)\n        pred_c = predict_nearest_proto(X, P)\n        c2t = majority_map(y_assign, y_true, P.shape[0], K_true)\n        return (c2t[pred_c] == y_true).astype(float)\n\n    # allocate per-edit alpha (simple alpha-spending)\n    alpha_e = alpha_total / max_tests\n    tests_used = 0\n\n    ytr_c = assign_from_classes(ytr_obs, classes)\n    ycal_c = assign_from_classes(ycal_obs, classes)\n    P, Q = build_state(Xtr, ytr_c, th_tr, tt_tr)\n    base_corr = proxy_correctness(Xcal, ycal_true, ycal_c, P)\n\n    def merge_score(i, j):\n        return float(P[i] @ P[j]) * np.exp(-lam * jsd(Q[i], Q[j]))\n\n    # --- MERGES (greedy) ---\n    for _ in range(max_merges):\n        if tests_used >= max_tests:\n            break\n        K = len(classes)\n        if K <= K_true:\n            break\n        # best pair\n        best = (-1, -1, -1e9)\n        for i in range(K):\n            for j in range(i+1, K):\n                s = merge_score(i, j)\n                if s > best[2]:\n                    best = (i, j, s)\n        i, j, s = best\n        if s < tau:\n            break\n\n        new_classes = [set(mem) for idx, mem in enumerate(classes) if idx not in (i, j)]\n        new_classes.append(set().union(classes[i], classes[j]))\n\n        ytr_p = assign_from_classes(ytr_obs, new_classes)\n        ycal_p = assign_from_classes(ycal_obs, new_classes)\n        Pp, Qp = build_state(Xtr, ytr_p, th_tr, tt_tr)\n\n        prop_corr = proxy_correctness(Xcal, ycal_true, ycal_p, Pp)\n        delta = (prop_corr - base_corr)  # in [-1,1]\n        lcb = cs_lcb_mean(delta, alpha=alpha_e)\n        tests_used += 1\n\n        if lcb >= -eps:\n            classes, ytr_c, ycal_c, P, Q, base_corr = new_classes, ytr_p, ycal_p, Pp, Qp, prop_corr\n        else:\n            break\n\n    # --- SPLITS (largest dispersion) ---\n    for _ in range(max_splits):\n        if tests_used >= max_tests:\n            break\n        K = len(classes)\n        disp = np.zeros(K)\n        for c in range(K):\n            idx = (ytr_c == c)\n            if idx.sum() >= 25:\n                disp[c] = float(1.0 - (Xtr[idx] @ P[c]).mean())\n        c = int(disp.argmax())\n        if disp[c] < split_disp:\n            break\n\n        idx = (ytr_c == c)\n        if idx.sum() < 60:\n            break\n\n        # typepair onehot feature to make split type-aware but still light\n        tp = (th_tr[idx] * T + tt_tr[idx])\n        tp_oh = np.eye(T*T, dtype=float)[tp]\n        Z = np.hstack([Xtr[idx], 0.4 * tp_oh])\n\n        km = KMeans(n_clusters=2, n_init=10, random_state=int(rng.integers(1e9)))\n        z = km.fit_predict(Z)\n\n        mem = classes[c]\n        left, right = set(), set()\n        for k in mem:\n            ids = np.where((ytr_obs == k) & idx)[0]\n            if len(ids) == 0:\n                left.add(k)\n                continue\n            # map local indices\n            local = np.flatnonzero(idx)\n            local_mask = np.isin(local, ids)\n            side = int(np.bincount(z[local_mask], minlength=2).argmax())\n            (left if side == 0 else right).add(k)\n\n        if len(left) == 0 or len(right) == 0:\n            break\n\n        new_classes = [set(m) for i, m in enumerate(classes) if i != c] + [left, right]\n\n        ytr_p = assign_from_classes(ytr_obs, new_classes)\n        ycal_p = assign_from_classes(ycal_obs, new_classes)\n        Pp, Qp = build_state(Xtr, ytr_p, th_tr, tt_tr)\n\n        prop_corr = proxy_correctness(Xcal, ycal_true, ycal_p, Pp)\n        delta = (prop_corr - base_corr)\n        lcb = cs_lcb_mean(delta, alpha=alpha_e)\n        tests_used += 1\n\n        if lcb >= -eps:\n            classes, ytr_c, ycal_c, P, Q, base_corr = new_classes, ytr_p, ycal_p, Pp, Qp, prop_corr\n        else:\n            break\n\n    return classes, P\n\n# -----------------------------\n# Evaluation helper\n# -----------------------------\ndef macro_f1_latent(Xtr, ytr_true, ytr_assign, P, Xte, yte_true):\n    pred_c = predict_nearest_proto(Xte, P)\n    c2t = majority_map(ytr_assign, ytr_true, P.shape[0], int(ytr_true.max() + 1))\n    pred_true = c2t[pred_c]\n    return f1_score(yte_true, pred_true, average='macro')\n\n# -----------------------------\n# Demo run\n# -----------------------------\nif __name__ == \"__main__\":\n    rng = np.random.default_rng(0)\n    K_true, splits, d = 5, 3, 64\n\n    X, y_true, y_obs, th, tt, K_obs, T = make_stream(rng, 3000, K_true=K_true, splits=splits, d=d)\n\n    # time split: train / cal / test are consecutive to respect drift\n    n = len(X)\n    tr, cal = int(0.45*n), int(0.70*n)\n    Xtr, ytr_true, ytr_obs, th_tr, tt_tr = X[:tr], y_true[:tr], y_obs[:tr], th[:tr], tt[:tr]\n    Xcal, ycal_true, ycal_obs, th_cal, tt_cal = X[tr:cal], y_true[tr:cal], y_obs[tr:cal], th[tr:cal], tt[tr:cal]\n    Xte, yte_true, yte_obs, th_te, tt_te = X[cal:], y_true[cal:], y_obs[cal:], th[cal:], tt[cal:]\n\n    # Baseline: fixed observed-label prototypes\n    P_obs = prototypes_from_labels(Xtr, ytr_obs, K_obs)\n    f1_fixed = macro_f1_latent(Xtr, ytr_true, ytr_obs, P_obs, Xte, yte_true)\n\n    # Merge-only heuristic baseline (cosine threshold)\n    tau = 0.80\n    S = P_obs @ P_obs.T\n    parent = list(range(K_obs))\n    def find(a):\n        while parent[a] != a:\n            parent[a] = parent[parent[a]]\n            a = parent[a]\n        return a\n    def union(a,b):\n        ra, rb = find(a), find(b)\n        if ra != rb:\n            parent[rb] = ra\n    for i in range(K_obs):\n        for j in range(i+1, K_obs):\n            if S[i, j] > tau:\n                union(i, j)\n    comps = {}\n    for k in range(K_obs):\n        comps.setdefault(find(k), set()).add(k)\n    classes_merge = list(comps.values())\n    ytr_c = np.zeros_like(ytr_obs)\n    for ci, mem in enumerate(classes_merge):\n        for k in mem:\n            ytr_c[ytr_obs == k] = ci\n    P_merge = prototypes_from_labels(Xtr, ytr_c, len(classes_merge))\n    f1_merge = macro_f1_latent(Xtr, ytr_true, ytr_c, P_merge, Xte, yte_true)\n\n    # Proposed: AVaRiSM\n    classes, P = avarism_repair(\n        rng,\n        Xtr, ytr_true, ytr_obs, th_tr, tt_tr,\n        Xcal, ycal_true, ycal_obs, th_cal, tt_cal,\n        K_true, K_obs, T,\n        tau=0.80, lam=1.0, eps=0.0,\n        alpha_total=0.10, max_tests=80\n    )\n    ytr_r = np.zeros_like(ytr_obs)\n    for ci, mem in enumerate(classes):\n        for k in mem:\n            ytr_r[ytr_obs == k] = ci\n    f1_avarism = macro_f1_latent(Xtr, ytr_true, ytr_r, P, Xte, yte_true)\n\n    print({\n        \"f1_fixed\": float(f1_fixed),\n        \"f1_merge_only\": float(f1_merge),\n        \"f1_avarism\": float(f1_avarism),\n        \"K_obs\": int(K_obs),\n        \"K_merge_only\": int(len(classes_merge)),\n        \"K_avarism\": int(len(classes))\n    })\n",
    "expected_result": "On the drifting synthetic DS benchmark (fragmentation + polysemy + noisy types + temporal drift), we expect:\n- **Fixed Proto:** Macro-F1 ≈ 0.65–0.75 (drift hurts because observed labels stay fragmented and polysemous).\n- **Merge-only:** Macro-F1 ≈ 0.70–0.80, with occasional catastrophic collapses when drift causes semantically nearby-but-distinct relations to be merged.\n- **Bootstrap-gated split–merge (RiSM):** Macro-F1 ≈ 0.76–0.84 but with non-trivial variance, because adaptive re-testing can still accept a harmful edit under repeated proposals.\n- **AVaRiSM (proposed):** Macro-F1 ≈ 0.80–0.88, with **lower variance across seeds** and fewer harmful accepted edits. A typical target is **+0.05 to +0.10** absolute Macro-F1 over Merge-only and **+0.02 to +0.05** over bootstrap-gated RiSM.\n\nDirection: higher Macro-F1 is better. We also expect the catastrophic-collapse rate to drop substantially (e.g., from ~10–20% of runs in Merge-only to <5% with AVaRiSM).",
    "expected_conclusion": "AVaRiSM elevates periodic KG schema reorganization from a clustering heuristic into an **audited online decision process** that remains reliable under the realities of streaming extraction: adaptive proposal generation, repeated checking, and early stopping.\n\nAcademic value: contributes a new connection between (i) continual label-space/ontology repair in relation extraction and (ii) **anytime-valid statistical inference (confidence sequences)**, yielding a principled “do-no-harm” mechanism under adaptivity—an aspect not addressed by prior prototype-based RE (ProtoRE/REGRAB) or by heuristic schema merging.\n\nPractical/social value: for Japanese Wikipedia-scale KG construction, schema errors are expensive and compound over time. An anytime-valid audit gate makes automated split/merge updates safer and more stable, reducing manual property curation burden and improving KG maintainability, interoperability, and downstream query reliability."
  }
}